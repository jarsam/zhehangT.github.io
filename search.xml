<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[VINS-Mono中的回环检测]]></title>
      <url>https://zhehangt.github.io/2018/04/25/SLAM/VINS/VINSRelocalization/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="&#x6458;&#x8981;"><a href="#&#x6458;&#x8981;" class="headerlink" title="&#x6458;&#x8981;"></a>&#x6458;&#x8981;</h1><p>&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x7684;&#x91CD;&#x8981;&#x6027;&#x5BF9;&#x4E8E;&#x4EFB;&#x4F55; SLAM &#x7CFB;&#x7EDF;&#x90FD;&#x662F;&#x4E0D;&#x8A00;&#x800C;&#x55BB;&#x7684;&#x3002;VIO &#x5373;&#x4F7F;&#x7CBE;&#x786E;&#x5EA6;&#x975E;&#x5E38;&#x9AD8;&#xFF0C;&#x4ECD;&#x7136;&#x4F1A;&#x6709;&#x7D2F;&#x79EF;&#x8BEF;&#x5DEE;&#x3002;&#x6B64;&#x65F6;&#x5C31;&#x9700;&#x8981;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x6765;&#x6D88;&#x9664;&#x7D2F;&#x79EF;&#x8BEF;&#x5DEE;&#x3002;&#x6211;&#x4EEC;&#x6765;&#x770B;&#x770B; VINS-Mono &#x4E2D;&#x662F;&#x5982;&#x4F55;&#x8FDB;&#x884C;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x7684;&#x3002;</p>
<h1 id="&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;"><a href="#&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;" class="headerlink" title="&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;"></a>&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;</h1><p>&#x9996;&#x5148;&#x7406;&#x4E00;&#x7406; VINS-Mono &#x8FDB;&#x884C;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x7684;&#x6D41;&#x7A0B;&#x3002;<br>VINS-Mono &#x4E2D;&#x5173;&#x4E8E;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x7684;&#x5165;&#x53E3;&#x5728; pose_graph &#x5305;&#x4E2D;<code>pose_graph_node.cpp</code>&#xFF0C;pose_graph &#x7684;&#x8F93;&#x5165;&#x4E3B;&#x8981;&#x4F9D;&#x8D56;&#x4E09;&#x4E2A;&#x6570;&#x636E;&#xFF0C;&#x7B2C;&#x4E00;&#x662F;&#x539F;&#x59CB;&#x56FE;&#x50CF;&#xFF0C;&#x7B2C;&#x4E8C;&#x662F;&#x5730;&#x56FE;&#x70B9;&#xFF0C;&#x7B2C;&#x4E09;&#x662F;&#x5173;&#x952E;&#x5E27;&#x3002;&#x9664;&#x4E86;&#x539F;&#x59CB;&#x56FE;&#x50CF;&#xFF0C;&#x5176;&#x4ED6;&#x8F93;&#x5165;&#x4E3B;&#x8981;&#x6765;&#x81EA;&#x4E8E; vins_estimator&#x3002;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">queue</span>&lt;sensor_msgs::ImageConstPtr&gt; image_buf;  <span class="comment">//&#x539F;&#x59CB;&#x56FE;&#x50CF;&#x6570;&#x636E;</span></div><div class="line"><span class="built_in">queue</span>&lt;sensor_msgs::PointCloudConstPtr&gt; point_buf; <span class="comment">//&#x4E16;&#x754C;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;&#x5730;&#x56FE;&#x70B9;&#x5750;&#x6807;&#xFF0C;&#x8BE5;&#x5730;&#x56FE;&#x70B9;&#x5728;&#x6700;&#x65B0;&#x56FE;&#x50CF;&#x5E27;&#x4E2D;&#x7684;&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807;&#xFF0C;&#x56FE;&#x50CF;&#x5750;&#x6807;&#x548C;feature id</span></div><div class="line"><span class="built_in">queue</span>&lt;nav_msgs::Odometry::ConstPtr&gt; pose_buf; <span class="comment">//&#x5F53;&#x524D;&#x5E27;&#x7684; pose</span></div></pre></td></tr></table></figure></p>
<p>pose_graph &#x7684;&#x4F9D;&#x9760;&#x8FD9;&#x4E09;&#x4E2A;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x3002;&#x56DE;&#x73AF;&#x5E27;&#x7684;&#x68C0;&#x6D4B;&#x7528;&#x7684;&#x8FD8;&#x662F;&#x8BCD;&#x888B;&#x6A21;&#x578B;&#xFF0C;&#x5982;&#x4F55;&#x5F97;&#x5230;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#xFF0C;&#x6211;&#x4EEC;&#x653E;&#x5230;&#x4E0B;&#x4E00;&#x5C0F;&#x8282;&#x3002;&#x8FD9;&#x91CC;&#x6211;&#x4EEC;&#x5148;&#x5173;&#x6CE8;&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;&#x529F;&#x80FD;&#x3002;<br>&#x8FD9;&#x91CC;vins&#x63D0;&#x4F9B;&#x4E86;&#x4E00;&#x79CD;&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;&#x529F;&#x80FD;&#xFF0C;&#x5F53;&#x68C0;&#x6D4B;&#x5230;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#x65F6;&#xFF0C;&#x76F4;&#x63A5;&#x5C06;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x7684;&#x7EA6;&#x675F;&#x8FD4;&#x56DE;&#x7ED9; vins_estimator &#x8FDB;&#x884C;&#x5FEB;&#x901F;&#x7684;&#x91CD;&#x5B9A;&#x4F4D;&#x3002;&#x76F8;&#x5173;&#x4EE3;&#x7801;&#x5728;&#x51FD;&#x6570;<code>bool KeyFrame::findConnection(KeyFrame* old_kf)</code>&#x3002;</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>(FAST_RELOCALIZATION)</div><div class="line">{</div><div class="line">    <span class="comment">//msg_match_points &#x662F;&#x53D1;&#x9001;&#x7ED9; estimator &#x7684;&#x5339;&#x914D;&#x4FE1;&#x606F;</span></div><div class="line">    <span class="comment">//points &#x91CC;&#x662F;&#x5339;&#x914D;&#x5230;&#x7684;&#x89D2;&#x70B9;&#x7684;&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807;&#x548C;&#x8BE5;&#x5730;&#x56FE;&#x70B9;&#x7684;id</span></div><div class="line">    <span class="comment">//channels &#x662F;&#x56DE;&#x73AF;&#x5E27;&#x7684; pose</span></div><div class="line">    sensor_msgs::PointCloud msg_match_points;</div><div class="line">    msg_match_points.header.stamp = ros::Time(time_stamp);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; (<span class="keyword">int</span>)matched_2d_old_norm.size(); i++)</div><div class="line">    {</div><div class="line">        geometry_msgs::Point32 p;</div><div class="line">        p.x = matched_2d_old_norm[i].x;</div><div class="line">        p.y = matched_2d_old_norm[i].y;</div><div class="line">        p.z = matched_id[i];</div><div class="line">        msg_match_points.points.push_back(p);</div><div class="line">    }</div><div class="line">    Eigen::Vector3d T = old_kf-&gt;T_w_i; </div><div class="line">    Eigen::Matrix3d R = old_kf-&gt;R_w_i;</div><div class="line">    <span class="function">Quaterniond <span class="title">Q</span><span class="params">(R)</span></span>;</div><div class="line">    sensor_msgs::ChannelFloat32 t_q_index;</div><div class="line">    t_q_index.values.push_back(T.x());</div><div class="line">    t_q_index.values.push_back(T.y());</div><div class="line">    t_q_index.values.push_back(T.z());</div><div class="line">    t_q_index.values.push_back(Q.w());</div><div class="line">    t_q_index.values.push_back(Q.x());</div><div class="line">    t_q_index.values.push_back(Q.y());</div><div class="line">    t_q_index.values.push_back(Q.z());</div><div class="line">    t_q_index.values.push_back(index);</div><div class="line">    msg_match_points.channels.push_back(t_q_index);</div><div class="line">    pub_match_points.publish(msg_match_points);</div><div class="line">}</div></pre></td></tr></table></figure>
<p>vins_estimator&#x5229;&#x7528;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x7684;&#x7EA6;&#x675F;&#x5176;&#x5B9E;&#x5E76;&#x975E;&#x76F4;&#x63A5;&#x4F18;&#x5316;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x6240;&#x6709;&#x72B6;&#x6001;&#x91CF;&#xFF0C;&#x4F46;&#x662F;&#x901A;&#x8FC7;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#x8BA1;&#x7B97;&#x51FA;&#x628A;&#x5F53;&#x524D;&#x5E27;&#x8F6C;&#x6362;&#x5230;&#x6B63;&#x786E;&#x4F4D;&#x59FF;&#x6240;&#x9700;&#x8981;&#x7684;&#x4F4D;&#x59FF;&#x4FEE;&#x6B63;&#x91CF;&#xFF0C;&#x5177;&#x4F53;&#x7684;&#x505A;&#x6CD5;&#x5982;&#x4E0B;&#x3002;<br>1.&#x5728;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#x4E2D;&#x5DF2;&#x77E5;&#x56DE;&#x73AF;&#x5E27;&#x5339;&#x914D;&#x5230;&#x4E86;&#x54EA;&#x4E9B;&#x5730;&#x56FE;&#x70B9;&#xFF0C;&#x4EE5;id&#x7684;&#x5F62;&#x5F0F;&#x8BB0;&#x5F55;&#x3002;<br>2.&#x5728;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x641C;&#x7D22;&#x5177;&#x6709;&#x76F8;&#x540C;id&#x7684;&#x5730;&#x56FE;&#x70B9;&#xFF0C;&#x5EFA;&#x7ACB;&#x91CD;&#x6295;&#x5F71;&#x8BEF;&#x5DEE;&#xFF0C;&#x5F53;&#x524D;&#x5E27;&#x7684;&#x4F4D;&#x59FF;&#x4F5C;&#x4E3A;&#x521D;&#x59CB;&#x4F4D;&#x59FF;&#x3002;<br>3.&#x4F18;&#x5316;&#x91CD;&#x6295;&#x5F71;&#x8BEF;&#x5DEE;&#xFF0C;&#x5F97;&#x5230;&#x56DE;&#x73AF;&#x5E27;&#x5728;&#x5F53;&#x524D;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5177;&#x6709;&#x7684;&#x5730;&#x56FE;&#x70B9;&#x4E0B;&#xFF0C;&#x5E94;&#x8BE5;&#x6240;&#x5904;&#x7684;&#x4F4D;&#x59FF;&#x3002;<br>4.&#x6B64;&#x65F6;&#x5F97;&#x5230;&#x7684;&#x56DE;&#x73AF;&#x5E27;&#x4F4D;&#x59FF;&#x548C;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#x4E2D;&#x56DE;&#x73AF;&#x5E27;&#x7684;&#x4F4D;&#x59FF;&#x4EA7;&#x751F;&#x4E86;&#x4E00;&#x4E2A;&#x76F8;&#x5BF9;&#x4F4D;&#x59FF;&#x53D8;&#x6362;&#x91CF;&#xFF0C;&#x5229;&#x7528;&#x8FD9;&#x4E2A;&#x76F8;&#x5BF9;&#x4F4D;&#x59FF;&#x53D8;&#x6362;&#x91CF;&#xFF0C;&#x5C31;&#x53EF;&#x4EE5;&#x628A;&#x5F53;&#x524D;&#x5E27;&#x4FEE;&#x6B63;&#x5230;&#x6BD4;&#x8F83;&#x51C6;&#x786E;&#x7684;&#x4F4D;&#x59FF;&#x3002;</p>
<blockquote>
<p>&#x5176;&#x5B9E;&#x901A;&#x8FC7;&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#x4FEE;&#x6B63;&#x7684;&#x4F4D;&#x59FF;&#xFF0C;&#x53EA;&#x5229;&#x7528;&#x4E86;&#x4E00;&#x5E27;&#x7EA6;&#x675F;&#xFF0C;&#x5E76;&#x4E0D;&#x662F;&#x975E;&#x5E38;&#x51C6;&#x786E;&#xFF0C;&#x6240;&#x4EE5;&#x79F0;&#x4E4B;&#x4E3A;&#x5FEB;&#x901F;&#x91CD;&#x5B9A;&#x4F4D;&#x3002;&#x8FD9;&#x90E8;&#x5206;&#x5185;&#x5BB9;&#x8DDF;&#x8BBA;&#x6587;&#x662F;&#x5BF9;&#x5E94;&#x4E0D;&#x4E0A;&#x7684;&#xFF0C;&#x5E94;&#x8BE5;&#x662F;VINS-Mono&#x65B0;&#x7248;&#x672C;&#x91CC;&#x9762;&#x7684;&#x6539;&#x52A8;&#x3002;</p>
</blockquote>
<h1 id="&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;"><a href="#&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;" class="headerlink" title="&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;"></a>&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;</h1><p>&#x771F;&#x6B63;&#x6267;&#x884C;&#x5168;&#x5C40;&#x56DE;&#x73AF;&#x4F18;&#x5316;&#x7684;&#x662F;&#x51FD;&#x6570;<code>void PoseGraph::optimize4DoF()</code>&#x3002;<br>&#x57FA;&#x672C;&#x7684;&#x601D;&#x8DEF;&#x5F88;&#x7B80;&#x5355;&#xFF0C;&#x901A;&#x8FC7;<code>bool KeyFrame::findConnection(KeyFrame* old_kf)</code>&#x6211;&#x4EEC;&#x5F97;&#x5230;&#x4E86;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#xFF0C;&#x66F4;&#x5177;&#x4F53;&#x7684;&#x8BB2;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x8BA1;&#x7B97;&#x5F53;&#x524D;&#x5E27;&#x89D2;&#x70B9;&#x63CF;&#x8FF0;&#x5B50;&#x548C;&#x56DE;&#x73AF;&#x5E27;&#x7684;&#x89D2;&#x70B9;&#x63CF;&#x8FF0;&#x5B50;&#x4E4B;&#x95F4;&#x7684;&#x6C49;&#x660E;&#x8DDD;&#x79BB;&#xFF0C;&#x5F97;&#x5230;&#x6700;&#x4F73;&#x7684;&#x89D2;&#x70B9;&#x5339;&#x914D;&#x3002;&#x5339;&#x914D;&#x4FE1;&#x606F;&#x5728;&#x4EE5;&#x4E0B;&#x4E09;&#x4E2A;&#x5C40;&#x90E8;&#x53D8;&#x91CF;&#x4E2D;&#x3002;&#x8FD9;&#x4E09;&#x4E2A;&#x53D8;&#x91CF;&#x4E2D;&#x4FDD;&#x5B58;&#x7684;&#x4FE1;&#x606F;&#x662F;&#x4E00;&#x4E00;&#x5BF9;&#x5E94;&#x7684;&#x3002;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;cv::Point2f&gt; matched_2d_cur, matched_2d_old; <span class="comment">//&#x50CF;&#x7D20;&#x5750;&#x6807;</span></div><div class="line"><span class="built_in">vector</span>&lt;cv::Point2f&gt; matched_2d_cur_norm, matched_2d_old_norm; <span class="comment">//&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807; </span></div><div class="line"><span class="built_in">vector</span>&lt;cv::Point3f&gt; matched_3d; <span class="comment">//&#x5730;&#x56FE;&#x70B9;&#x5750;&#x6807;</span></div></pre></td></tr></table></figure></p>
<p>&#x63A5;&#x4E0B;&#x6765;&#x8FDB;&#x884C;&#x4E86;&#x901A;&#x8FC7;pnp&#x5220;&#x9664;&#x4E00;&#x4E9B;&#x8BEF;&#x5339;&#x914D;&#x3002;&#x5982;&#x679C;&#x6B64;&#x65F6;&#x5F97;&#x5230;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x89D2;&#x70B9;&#x5339;&#x914D;&#x6570;&#x76EE;&#x5927;&#x4E8E;<code>MIN_LOOP_NUM</code>&#xFF0C;&#x5219;&#x8868;&#x793A;&#x5F53;&#x524D;&#x56DE;&#x73AF;&#x5E27;&#x68C0;&#x6D4B;&#x662F;&#x6709;&#x6548;&#x7684;&#x3002;&#x6700;&#x540E;&#x628A;&#x5F53;&#x524D;&#x5E27;&#x4E0E;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x4F4D;&#x59FF;&#x8F6C;&#x6362;&#x5173;&#x7CFB;&#x4FDD;&#x5B58;&#x5728;<code>Eigen::Matrix&lt;double, 8, 1 &gt; loop_info</code>&#x4E2D;&#x3002;<br>&#x6709;&#x4E86;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x4F4D;&#x59FF;&#x8F6C;&#x6362;&#x5173;&#x7CFB;&#xFF0C;&#x5C31;&#x5177;&#x5907;&#x4E86;&#x8FDB;&#x884C;&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;&#x7684;&#x6761;&#x4EF6;&#x3002;<br>&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;&#x6709;&#x4E24;&#x79CD;&#x7EA6;&#x675F;&#x5173;&#x7CFB;&#x3002;&#x7B2C;&#x4E00;&#x79CD;&#x662F;&#x901A;&#x8FC7; VIO &#x5F97;&#x5230;&#x7684;&#xFF0C;&#x76F8;&#x90BB;&#x4E24;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x4F4D;&#x59FF;&#x7EA6;&#x675F;&#x3002;&#x7B2C;&#x4E8C;&#x79CD;&#x662F;&#x901A;&#x8FC7;&#x521A;&#x521A;&#x63CF;&#x8FF0;&#x7684;&#x5F53;&#x524D;&#x5E27;&#x4E0E;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x4F4D;&#x59FF;&#x7EA6;&#x675F;&#x3002;&#x6700;&#x540E;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x5BF9; VIO &#x6765;&#x8BF4;&#xFF0C;pitch&#x89D2;&#x548C;rolling&#x89D2;&#x662F;&#x4E0D;&#x4F1A;&#x4EA7;&#x751F;&#x7D2F;&#x79EF;&#x8BEF;&#x5DEE;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x6B64;&#x65F6;&#x7684;&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;&#x53EA;&#x5728;4&#x81EA;&#x7531;&#x5EA6;&#x4E0A;&#x8FDB;&#x884C;&#x3002;<br>&#x57FA;&#x4E8E; VIO &#x4F4D;&#x59FF;&#x7EA6;&#x675F;&#x6784;&#x9020;&#x7684;&#x6B8B;&#x5DEE;&#x9879;&#x5B9A;&#x4E49;&#x5728;<code>struct FourDOFError</code>&#x4E2D;&#x3002;&#x8FD9;&#x91CC;&#x6709;&#x4E24;&#x70B9;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x3002;<strong>&#x9996;&#x5148;</strong>&#xFF0C;&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;&#x53EA;&#x4F1A;&#x4F18;&#x5316;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x540E;&#x7684;&#x5173;&#x952E;&#x5E27;&#xFF0C;&#x56DE;&#x73AF;&#x5E27;&#x81EA;&#x8EAB;&#x4FDD;&#x6301;&#x56FA;&#x5B9A;&#xFF0C;&#x56DE;&#x73AF;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x5E27;&#x4E0D;&#x53C2;&#x4E0E;&#x4F18;&#x5316;&#x3002;<strong>&#x7B2C;&#x4E8C;</strong>&#xFF0C;&#x5728;&#x6784;&#x9020; VIO &#x4F4D;&#x59FF;&#x7EA6;&#x675F;&#x65F6;&#xFF0C;&#x4E0D;&#x4EC5;&#x4EC5;&#x6784;&#x9020;&#x4E86;&#x76F8;&#x90BB;&#x4E24;&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x4F4D;&#x59FF;&#x7EA6;&#x675F;&#xFF0C;&#x800C;&#x662F;&#x591F;&#x9020;&#x4E86;&#x4E0E;&#x524D;5&#x5E27;&#x4E4B;&#x95F4;&#x7684;&#x4F4D;&#x59FF;&#x7EA6;&#x675F;&#x3002;<br>&#x57FA;&#x4E8E;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#x6784;&#x9020;&#x7684;&#x6B8B;&#x5DEE;&#x9879;&#x5B9A;&#x4E49;&#x5728; struct FourDOFWeightError&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;weight&#x53C2;&#x6570;&#x6765;&#x653E;&#x5927;&#x8FD9;&#x4E00;&#x90E8;&#x5206;&#x7684;&#x8BEF;&#x5DEE;&#x503C;&#x3002;&#x4F46; VINS-Mono &#x4EE3;&#x7801;&#x4E2D;&#x5C06; weight&#x8BBE;&#x4E3A;&#x4E86;1&#x3002;<br>&#x7B80;&#x5355;&#x770B;&#x4E00;&#x4E0B;&#x8BEF;&#x5DEE;&#x9879;&#xFF0C;&#x4E0D;&#x96BE;&#x53D1;&#x73B0;&#x8FD9;&#x91CC;&#x7684;&#x8BEF;&#x5DEE;&#x9879;&#x53EA;&#x6709;&#x5E73;&#x79FB;&#x4E09;&#x4E2A;&#x81EA;&#x7531;&#x5EA6;&#xFF0C;yaw&#x89D2;&#x4E00;&#x4E2A;&#x81EA;&#x7531;&#x5EA6;&#xFF0C;&#x4E00;&#x5171;&#x56DB;&#x4E2A;&#x81EA;&#x7531;&#x5EA6;&#x3002;&#x8FD9;&#x90E8;&#x5206;&#x7684;&#x4F18;&#x5316;&#x6C42;&#x89E3;&#x91C7;&#x7528;&#x4E86;&#x81EA;&#x52A8;&#x6C42;&#x5BFC;&#x7684;&#x65B9;&#x5F0F;&#x3002;</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> yaw_i, <span class="keyword">const</span> T* ti, <span class="keyword">const</span> T* yaw_j, <span class="keyword">const</span> T* tj, T* residuals)</span> <span class="keyword">const</span></span></div><div class="line">{</div><div class="line">	T t_w_ij[<span class="number">3</span>]; <span class="comment">//&#x56E0;&#x4E3A;ti&#x548C;tj&#x90FD;&#x662F;&#x4E16;&#x754C;&#x7CFB;&#x4E0B;&#x7684;&#x5E73;&#x79FB;&#xFF0C;&#x6240;&#x4EE5;ti-tj&#x662F;&#x4E16;&#x754C;&#x7CFB;&#x4E0B;&#x7684;&#x5E73;&#x79FB;&#x5DEE;</span></div><div class="line">	t_w_ij[<span class="number">0</span>] = tj[<span class="number">0</span>] - ti[<span class="number">0</span>];</div><div class="line">	t_w_ij[<span class="number">1</span>] = tj[<span class="number">1</span>] - ti[<span class="number">1</span>];</div><div class="line">	t_w_ij[<span class="number">2</span>] = tj[<span class="number">2</span>] - ti[<span class="number">2</span>]; </div><div class="line">	<span class="comment">// &#x5F97;&#x5230;Riw&#xFF0C;Riw(ti-tj)&#x5373;&#x4E3A;i&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;&#x5E73;&#x79FB;</span></div><div class="line">	T w_R_i[<span class="number">9</span>];</div><div class="line">	YawPitchRollToRotationMatrix(yaw_i[<span class="number">0</span>], T(pitch_i), T(roll_i), w_R_i);</div><div class="line">	<span class="comment">// rotation transpose</span></div><div class="line">	T i_R_w[<span class="number">9</span>];</div><div class="line">	RotationMatrixTranspose(w_R_i, i_R_w);</div><div class="line">	<span class="comment">// rotation matrix rotate point</span></div><div class="line">	T t_i_ij[<span class="number">3</span>];</div><div class="line">	RotationMatrixRotatePoint(i_R_w, t_w_ij, t_i_ij);</div><div class="line">	residuals[<span class="number">0</span>] = (t_i_ij[<span class="number">0</span>] - T(t_x));</div><div class="line">	residuals[<span class="number">1</span>] = (t_i_ij[<span class="number">1</span>] - T(t_y));</div><div class="line">	residuals[<span class="number">2</span>] = (t_i_ij[<span class="number">2</span>] - T(t_z));</div><div class="line">	residuals[<span class="number">3</span>] = NormalizeAngle(yaw_j[<span class="number">0</span>] - yaw_i[<span class="number">0</span>] - T(relative_yaw));</div><div class="line">	<span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">}</div></pre></td></tr></table></figure>
<h1 id="&#x5730;&#x56FE;&#x878D;&#x5408;"><a href="#&#x5730;&#x56FE;&#x878D;&#x5408;" class="headerlink" title="&#x5730;&#x56FE;&#x878D;&#x5408;"></a>&#x5730;&#x56FE;&#x878D;&#x5408;</h1><p>&#x5047;&#x8BBE;&#x52A0;&#x8F7D;&#x4E86;&#x4E4B;&#x524D;&#x5EFA;&#x597D;&#x7684;&#x5730;&#x56FE;&#xFF0C;&#x79F0;&#x4E4B;&#x4E3A;&#x5730;&#x56FE;1&#x3002;&#x5982;&#x679C; VINS-Mono &#x5728;&#x542F;&#x52A8;&#x4E4B;&#x540E;&#x5E76;&#x6CA1;&#x6709;&#x68C0;&#x6D4B;&#x5230;&#x4E0E;&#x5730;&#x56FE;1&#x7684;&#x56DE;&#x73AF;&#xFF0C;&#x5373;&#x6CA1;&#x6709;&#x91CD;&#x5B9A;&#x4F4D;&#x6210;&#x529F;&#xFF0C;&#x6B64;&#x65F6;&#x4F1A;&#x65B0;&#x5EFA;&#x4E00;&#x4E2A;&#x5730;&#x56FE;&#xFF0C;&#x79F0;&#x4E3A;&#x5730;&#x56FE;2&#x3002;&#x800C;&#x5728; VINS-Mono &#x8FD0;&#x884C;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x5F53;&#x68C0;&#x6D4B;&#x5230;&#x4E0E;&#x5730;&#x56FE;1&#x4E2D;&#x7684;&#x56DE;&#x73AF;&#x7EA6;&#x675F;&#x4E4B;&#x540E;&#xFF0C;&#x6B64;&#x65F6;&#x7684;&#x5168;&#x5C40;&#x4F4D;&#x59FF;&#x4F18;&#x5316;&#x4F1A;&#x5C06;&#x5730;&#x56FE;1&#x548C;&#x5730;&#x56FE;2&#x5408;&#x5E76;&#x3002;&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;&#x3002;&#x8FD9;&#x90E8;&#x5206;&#x5185;&#x5BB9;&#x5176;&#x5B9E;&#x4E0E;&#x56DE;&#x73AF;&#x68C0;&#x6D4B;&#x6CA1;&#x6709;&#x672C;&#x8D28;&#x4E0A;&#x7684;&#x533A;&#x522B;&#x3002;</p>
<p><img src="/2018/04/25/SLAM/VINS/VINSRelocalization/1.png" alt=""></p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 源码学习 </tag>
            
            <tag> VINS-Mono </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[VINS-Mono中的VIO紧耦合方案]]></title>
      <url>https://zhehangt.github.io/2018/04/24/SLAM/VINS/VINSVIO/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="&#x6458;&#x8981;"><a href="#&#x6458;&#x8981;" class="headerlink" title="&#x6458;&#x8981;"></a>&#x6458;&#x8981;</h1><p>VIO &#x7D27;&#x8026;&#x5408;&#x65B9;&#x6848;&#x662F;&#x6BCF;&#x4E2A;VIO&#x7CFB;&#x7EDF;&#x6700;&#x91CD;&#x8981;&#x7684;&#x7EC4;&#x6210;&#x90E8;&#x5206;&#xFF0C;&#x672C;&#x7BC7;&#x535A;&#x5BA2;&#x5C06;&#x7ED3;&#x5408;&#x8BBA;&#x6587;&#x548C;&#x76F8;&#x5173;&#x4EE3;&#x7801;&#xFF0C;&#x5BF9; VINS-Mono &#x7684;&#x7D27;&#x8026;&#x5408;&#x65B9;&#x6848;&#x4F5C;&#x8BE6;&#x7EC6;&#x5206;&#x6790;&#x3002;</p>
<h1 id="&#x6982;&#x8FF0;"><a href="#&#x6982;&#x8FF0;" class="headerlink" title="&#x6982;&#x8FF0;"></a>&#x6982;&#x8FF0;</h1><p>VIO &#x7D27;&#x8026;&#x5408;&#x65B9;&#x6848;&#x7684;&#x4E3B;&#x8981;&#x601D;&#x8DEF;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x5C06;&#x57FA;&#x4E8E;&#x89C6;&#x89C9;&#x6784;&#x9020;&#x7684;&#x6B8B;&#x5DEE;&#x9879;&#x548C;&#x57FA;&#x4E8E;IMU&#x6784;&#x9020;&#x7684;&#x6B8B;&#x5DEE;&#x9879;&#x653E;&#x5728;&#x4E00;&#x8D77;&#x6784;&#x9020;&#x6210;&#x4E00;&#x4E2A;&#x8054;&#x5408;&#x4F18;&#x5316;&#x7684;&#x95EE;&#x9898;&#xFF0C;&#x6574;&#x4E2A;&#x4F18;&#x5316;&#x95EE;&#x9898;&#x7684;&#x6700;&#x4F18;&#x89E3;&#x5373;&#x53EF;&#x8BA4;&#x4E3A;&#x662F;&#x6BD4;&#x8F83;&#x51C6;&#x786E;&#x7684;&#x72B6;&#x6001;&#x4F30;&#x8BA1;&#x3002;<br>&#x4E3A;&#x4E86;&#x9650;&#x5236;&#x4F18;&#x5316;&#x53D8;&#x91CF;&#x7684;&#x6570;&#x76EE;&#xFF0C;VINS-Mono &#x91C7;&#x7528;&#x4E86;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x7684;&#x5F62;&#x5F0F;&#xFF0C;&#x5F85;&#x4F30;&#x8BA1;&#x91CF;&#x5747;&#x4E3A;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x72B6;&#x6001;&#x53D8;&#x91CF;&#xFF0C;&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;&#x3002;</p>
<p><img src="/2018/04/24/SLAM/VINS/VINSVIO/1.png" width="40%" height="40%"></p>
<p>&#x5176;&#x4E2D;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F; VINS-Mono &#x4E2D;&#x5BF9;&#x4E8E;&#x5730;&#x56FE;&#x70B9;&#x7684;&#x53C2;&#x6570;&#x5316;&#x5F62;&#x5F0F;&#x3002; VINS-Mono &#x7528;&#x7B2C;&#x4E00;&#x6B21;&#x89C2;&#x6D4B;&#x5230;&#x8BE5;&#x5730;&#x56FE;&#x70B9;&#x7684;&#x76F8;&#x673A;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;&#x9006;&#x6DF1;&#x5EA6;&#x6765;&#x8868;&#x793A;&#x4E00;&#x4E2A;&#x5730;&#x56FE;&#x70B9;&#xFF0C;&#x5373;&#x4E0A;&#x56FE;&#x4E2D;&#x5F85;&#x4F30;&#x8BA1;&#x53D8;&#x91CF; &#x3BB;&#x3002;&#x5176;&#x603B;&#x6570; m &#x4E3A;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x80FD;&#x591F;&#x89C2;&#x6D4B;&#x5230;&#x7684;&#x6240;&#x6709;&#x5730;&#x56FE;&#x70B9;&#x7684;&#x6570;&#x76EE;&#x3002;<br>VINS-Mono &#x4E2D;&#x6784;&#x9020;&#x4E86;&#x4E09;&#x4E2A;&#x6B8B;&#x5DEE;&#x9879;&#x3002;&#x7B2C;&#x4E00;&#x9879;&#x4E3A;&#x5148;&#x9A8C;&#x9879;&#xFF0C;&#x7B2C;&#x4E8C;&#x9879;&#x662F;&#x89C6;&#x89C9;&#x6B8B;&#x5DEE;&#x9879;&#xFF0C;&#x7B2C;&#x4E09;&#x9879;&#x662F; IMU &#x6B8B;&#x5DEE;&#x9879;&#xFF0C;&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;&#x3002;</p>
<p><img src="/2018/04/24/SLAM/VINS/VINSVIO/2.png" width="45%" height="45%"></p>
<p>&#x6B8B;&#x5DEE;&#x9879;&#x7684;&#x6784;&#x9020;&#x548C;&#x6C42;&#x89E3;&#x90FD;&#x5728;&#x51FD;&#x6570; <code>void Estimator::optimization()</code> &#x4E2D;&#x3002;<br>&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x6240;&#x6709;&#x5F85;&#x4F18;&#x5316;&#x53D8;&#x91CF;&#x90FD;&#x4FDD;&#x5B58;&#x5728;&#x4E00;&#x4E2A;&#x6570;&#x7EC4;&#x4E2D;&#xFF0C;&#x5982;&#x4E0B;&#x6240;&#x793A;&#x3002;VINS-Mono &#x5728;&#x6574;&#x4E2A;&#x8FD0;&#x884C;&#x8FC7;&#x7A0B;&#x4E2D;&#x7684;&#x6700;&#x7EC8;&#x76EE;&#x7684;&#xFF0C;&#x5C31;&#x662F;&#x8981;&#x901A;&#x8FC7;&#x6784;&#x9020;&#x6B8B;&#x5DEE;&#x9879;&#xFF0C;&#x4F18;&#x5316;&#x5F97;&#x5230;&#x51C6;&#x786E;&#x7684;&#x72B6;&#x6001;&#x4F30;&#x8BA1;&#x91CF;&#x3002;</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Estimator{</div><div class="line">    Vector3d Ps[(WINDOW_SIZE + <span class="number">1</span>)]; <span class="comment">//&#x5E73;&#x79FB;</span></div><div class="line">    Vector3d Vs[(WINDOW_SIZE + <span class="number">1</span>)]; <span class="comment">//&#x901F;&#x5EA6;</span></div><div class="line">    Matrix3d Rs[(WINDOW_SIZE + <span class="number">1</span>)]; <span class="comment">//&#x65CB;&#x8F6C;</span></div><div class="line">    Vector3d Bas[(WINDOW_SIZE + <span class="number">1</span>)]; <span class="comment">//&#x52A0;&#x901F;&#x5EA6;&#x504F;&#x5DEE;</span></div><div class="line">    Vector3d Bgs[(WINDOW_SIZE + <span class="number">1</span>)]; <span class="comment">//&#x9640;&#x87BA;&#x4EEA;&#x504F;&#x5DEE;</span></div><div class="line">}</div></pre></td></tr></table></figure>
<p>&#x63A5;&#x4E0B;&#x6765;&#x6211;&#x4EEC;&#x6765;&#x7740;&#x91CD;&#x5206;&#x6790;&#x4E0B;&#x8FD9;&#x4E09;&#x4E2A;&#x6B8B;&#x5DEE;&#x9879;&#x662F;&#x600E;&#x4E48;&#x6784;&#x9020;&#x7684;&#x3002;</p>
<h1 id="IMU-&#x6B8B;&#x5DEE;&#x9879;"><a href="#IMU-&#x6B8B;&#x5DEE;&#x9879;" class="headerlink" title="IMU &#x6B8B;&#x5DEE;&#x9879;"></a>IMU &#x6B8B;&#x5DEE;&#x9879;</h1><p>&#x9996;&#x5148;&#x6765;&#x770B; IMU &#x7684;&#x6B8B;&#x5DEE;&#x9879;&#xFF0C;&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;&#x3002;</p>
<p><img src="/2018/04/24/SLAM/VINS/VINSVIO/3.png" width="50%" height="50%"></p>
<p>IMU&#x9884;&#x8BA1;&#x5206;&#x7684;&#x5177;&#x4F53;&#x7EC6;&#x8282;&#x548C;&#x516C;&#x5F0F;&#x63A8;&#x5BFC;&#x5C31;&#x4E0D;&#x5C55;&#x5F00;&#x8BB2;&#x4E86;&#xFF0C;&#x4E3B;&#x8981;&#x53EF;&#x4EE5;&#x53C2;&#x8003;&#x6587;&#x732E;[1]&#x548C;&#x6587;&#x732E;[2]&#x3002;&#x6211;&#x4EEC;&#x76F4;&#x63A5;&#x7ED9;&#x51FA;IMU&#x9884;&#x79EF;&#x5206;&#x7684;&#x5904;&#x7406;&#x6D41;&#x7A0B;&#x3002;&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A;IMU&#x6570;&#x636E;&#xFF0C;&#x9700;&#x8981;&#x5904;&#x7406;&#x4E09;&#x4EF6;&#x4E8B;&#x60C5;&#x3002;<strong>&#x7B2C;&#x4E00;</strong>&#xFF0C;&#x66F4;&#x65B0;&#x5F53;&#x524D;&#x7684;&#x9884;&#x79EF;&#x5206;&#x91CF;&#x3002;<strong>&#x7B2C;&#x4E8C;</strong>&#xFF0C;&#x66F4;&#x65B0;IMU&#x6B8B;&#x5DEE;&#x7684;&#x534F;&#x9632;&#x5DEE;&#x77E9;&#x9635;&#x3002;&#x7B2C;&#x4E09;&#xFF0C;&#x66F4;&#x65B0;IMU&#x6B8B;&#x5DEE;&#x5BF9;&#x4E8E;bias&#x7684;&#x96C5;&#x514B;&#x6BD4;&#x77E9;&#x9635;&#x3002;&#x660E;&#x786E;&#x4E86;&#x8FD9;&#x4E09;&#x4EF6;&#x4E8B;&#x60C5;&#x4E4B;&#x540E;&#xFF0C;&#x6211;&#x4EEC;&#x518D;&#x6765;&#x770B;&#x770B; VINS-Mono &#x662F;&#x600E;&#x4E48;&#x505A;&#x8FD9;&#x4E09;&#x4EF6;&#x4E8B;&#x60C5;&#x7684;&#x3002;</p>
<p>VINS-Mono &#x4E2D;&#x5904;&#x7406; IMU &#x6570;&#x636E;&#x7684;&#x5165;&#x53E3;&#x662F;<code>void Estimator::processIMU(double dt, const Vector3d &amp;linear_acceleration, const Vector3d &amp;angular_velocity)</code>&#x3002;&#x5728;<code>void Estimator::processIMU(...)</code>&#x51FD;&#x6570;&#x4E2D;&#xFF0C;&#x6709;&#x4E2A;&#x5F88;&#x5173;&#x952E;&#x7684;&#x51FD;&#x6570;&#x8C03;&#x7528;&#x5C31;&#x662F;<code>pre_integrations[frame_count]-&gt;push_back(dt, linear_acceleration, angular_velocity)</code>&#x3002;&#x5728;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x4E2D;&#xFF0C;VINS-Mono&#x628A;&#x4E0A;&#x9762;&#x63D0;&#x5230;&#x7684;&#x4E09;&#x4EF6;&#x91CD;&#x8981;&#x7684;&#x4E8B;&#x60C5;&#x5168;&#x90E8;&#x90FD;&#x505A;&#x4E86;&#x3002;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x6240;&#x6709;&#x9884;&#x79EF;&#x5206;&#x7684;&#x4FE1;&#x606F;&#x4FDD;&#x5B58;&#x5728;<code>IntegrationBase *pre_integrations[(WINDOW_SIZE + 1)]</code>&#x3002;</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> IntegrationBase{</div><div class="line">    Eigen::Vector3d delta_p; <span class="comment">//&#x5E73;&#x79FB;&#x9884;&#x79EF;&#x5206;</span></div><div class="line">    Eigen::Quaterniond delta_q; <span class="comment">//&#x65CB;&#x8F6C;&#x9884;&#x79EF;&#x5206;</span></div><div class="line">    Eigen::Vector3d delta_v; <span class="comment">//&#x901F;&#x5EA6;&#x9884;&#x79EF;&#x5206;</span></div><div class="line">    Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">15</span>, <span class="number">15</span>&gt; jacobian, covariance; <span class="comment">//&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x548C;&#x534F;&#x65B9;&#x5DEE;&#x77E9;&#x9635;</span></div><div class="line">}</div></pre></td></tr></table></figure>
<p>&#x9996;&#x5148;&#x662F;&#x66F4;&#x65B0;&#x9884;&#x79EF;&#x5206;&#x91CF;&#x3002;<br>&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;VINS-Mono &#x5728;&#x4EE3;&#x7801;&#x4E2D;&#x91C7;&#x7528;&#x4E86;&#x4E2D;&#x503C;&#x79EF;&#x5206;&#x7684;&#x65B9;&#x5F0F;&#xFF0C;&#x4E0E;&#x8BBA;&#x6587;&#x4E2D;&#x7684;&#x6B27;&#x62C9;&#x79EF;&#x5206;&#x4E0D;&#x540C;&#x3002;<br>&#x66F4;&#x65B0;&#x9884;&#x79EF;&#x5206;&#x7684;&#x516C;&#x5F0F;&#x4E3A;&#xFF1A;</p>
<p><img src="/2018/04/24/SLAM/VINS/VINSVIO/4.jpg" width="80%" height="80%"></p>
<p>&#x5176;&#x6B21;&#x662F;&#x66F4;&#x65B0;&#x534F;&#x65B9;&#x5DEE;&#x77E9;&#x9635;&#x548C;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x3002;<br>&#x5728;&#x8BEF;&#x5DEE;&#x4F20;&#x64AD;&#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#x6709;&#x8FD9;&#x6837;&#x4E00;&#x79CD;&#x6027;&#x8D28;&#xFF0C;&#x5982;&#x679C;&#x80FD;&#x627E;&#x5230;&#x72B6;&#x6001;&#x91CF;&#x7684;&#x9012;&#x63A8;&#x516C;&#x5F0F; $\delta z_{k+1} = F \delta z_k + G n_k$<br>&#x5219;&#x6709;&#x534F;&#x65B9;&#x5DEE; $P_{ik+1}$ &#x548C; IMU &#x6B8B;&#x5DEE;&#x5BF9;&#x4E8E;bias&#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635; $J_{ik+1}^{b}$ &#x7684;&#x9012;&#x63A8;&#x66F4;&#x65B0;&#x516C;&#x5F0F;&#xFF1A;<br>$$<br>P_{ik+1} = F_{k}P_{ik}F_{k}^T + G_{k}P_nG_{k}^T<br>\\<br>J_{ik+1}^{b} = F_{k}J_{ik}^{b}<br>$$</p>
<p>&#x5176;&#x4E2D; $P$ &#x8868;&#x793A;&#x534F;&#x65B9;&#x5DEE;&#xFF0C;$J$ &#x8868;&#x793A;IMU&#x9884;&#x79EF;&#x5206;&#x6B8B;&#x5DEE;&#x5BF9;&#x4E8E;bias&#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x3002;<br>&#x5728; VINS-Mono &#x4E2D;&#x91C7;&#x7528;&#x4E86;&#x4E2D;&#x503C;&#x79EF;&#x5206;&#x7684;&#x65B9;&#x5F0F;&#x8BA1;&#x7B97;&#x9884;&#x79EF;&#x5206;&#xFF0C;&#x9012;&#x63A8;&#x516C;&#x5F0F;&#x5982;&#x4E0B;&#xFF1A;<br><img src="/2018/04/24/SLAM/VINS/VINSVIO/5.jpg" width="70%" height="70%"><br><img src="/2018/04/24/SLAM/VINS/VINSVIO/6.jpg" width="70%" height="70%"></p>
<blockquote>
<p>&#x4E4B;&#x524D;&#x4E00;&#x76F4;&#x7EA0;&#x7ED3;&#x4E8E; $F$ &#x548C; $G$ &#x7684;&#x5F62;&#x5F0F;&#x4E3A;&#x4EC0;&#x4E48;&#x5728; VINS-Mono &#x4E2D;&#x548C; Foster &#x7684;&#x9884;&#x79EF;&#x5206;&#x8BBA;&#x6587;&#x4E2D;&#x7684;&#x5F62;&#x5F0F;&#x4E0D;&#x4E00;&#x6837;&#x3002;Foster &#x7684;&#x9884;&#x79EF;&#x5206;&#x8BBA;&#x6587;&#x4E2D;&#x7ED9;&#x51FA;&#x4E86; $F$ &#x548C; $G$ &#x7684;&#x5B8C;&#x6574;&#x63A8;&#x5BFC;&#x8FC7;&#x7A0B;&#x3002;&#x4F46;&#x662F; VINS-Mono &#x76F4;&#x63A5;&#x7ED9;&#x51FA;&#x4E86; $F$ &#x548C; $G$ &#x7684;&#x5177;&#x4F53;&#x5F62;&#x5F0F;&#xFF0C;&#x5E76;&#x6CA1;&#x6709;&#x7ED9;&#x51FA;&#x63A8;&#x5BFC;&#x8FC7;&#x7A0B;&#x3002;&#x4ECE;&#x76F4;&#x89C2;&#x7684;&#x89D2;&#x5EA6;&#x6765;&#x770B;&#xFF0C;&#x5F53;&#x8BEF;&#x5DEE;&#x901A;&#x8FC7; $\delta z_{k+1} = F \delta z_k + G n_k$ &#x8FDB;&#x884C;&#x4F20;&#x64AD;&#x65F6;&#xFF0C;$z_k$ &#x548C; $n_k$ &#x7684;&#x53D8;&#x5316;&#x662F;&#x901A;&#x8FC7;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x6765;&#x4F20;&#x64AD;&#x7684;&#xFF0C;&#x56E0;&#x6B64; $F$ &#x662F;&#x5F53;&#x524D;&#x65F6;&#x523B;&#x7684;&#x72B6;&#x6001;&#x91CF; $z_{k+1}$ &#x5BF9;&#x4E0A;&#x4E00;&#x65F6;&#x523B;&#x72B6;&#x6001;&#x91CF; $z_k$ &#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#xFF0C;$G$ &#x662F;&#x5F53;&#x524D;&#x65F6;&#x523B;&#x7684;&#x72B6;&#x6001;&#x91CF; $z_{k+1}$ &#x5BF9;&#x4E0A;&#x4E00;&#x65F6;&#x523B;&#x8BEF;&#x5DEE;&#x9879; $n_k$ &#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x3002;&#x53C2;&#x8003;&#x77E5;&#x4E4E;&#x4E0A;&#x7684;&#x4E00;&#x4E2A;&#x7B54;&#x6848;&#xFF0C;&#x8FD9;&#x90E8;&#x5206;&#x7684;&#x63A8;&#x5BFC;&#x5E94;&#x8BE5;&#x53EF;&#x4EE5;&#x4ECE;&#x53C2;&#x8003;&#x6587;&#x732E;[2]&#x4E2D;&#x627E;&#x5230;&#x3002;&#x5F85;&#x8FDB;&#x4E00;&#x6B65;&#x5B66;&#x4E60;&#x3002;</p>
</blockquote>
<p>VINS-Mono &#x5173;&#x4E8E; IMU &#x8BEF;&#x5DEE;&#x9879;&#x7684;&#x5B9A;&#x4E49;&#x5728; <code>imu_factor.h</code> &#x4E2D;&#x3002;&#x8FD9;&#x91CC;&#x5728;&#x63D0;&#x4E00;&#x4E0B; Ceres &#x5BF9;&#x4E8E;&#x6B8B;&#x5DEE;&#x7684;&#x8BA1;&#x7B97;&#xFF0C;&#x5982;&#x4E0B;</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">15</span>, <span class="number">15</span>&gt; sqrt_info = Eigen::LLT&lt;Eigen::Matrix&lt;<span class="keyword">double</span>, <span class="number">15</span>, <span class="number">15</span>&gt;&gt;(pre_integration-&gt;covariance.inverse()).matrixL().transpose();</div><div class="line">residual = sqrt_info * residual;</div></pre></td></tr></table></figure>
<p>&#x4E3A;&#x4E86;&#x4FDD;&#x8BC1; IMU &#x548C; &#x89C6;&#x89C9;&#x53C3;&#x5DEE;&#x9879;&#x5728;&#x5C3A;&#x5EA6;&#x4E0A;&#x4FDD;&#x6301;&#x4E00;&#x81F4;&#xFF0C;&#x4E00;&#x822C;&#x4F1A;&#x91C7;&#x7528;&#x4E0E;&#x91CF;&#x7EB2;&#x65E0;&#x5173;&#x7684;&#x9A6C;&#x6C0F;&#x8DDD;&#x79BB;&#xFF0C;&#x5373; $e^TP^{-1}e$ &#xFF0C;&#x4F46;&#x8FD9;&#x660E;&#x663E;&#x4E0E; VINS-Mono &#x4EE3;&#x7801;&#x4E2D;&#x4E0D;&#x4E00;&#x81F4;&#x3002;&#x8FD9;&#x662F;&#x56E0;&#x4E3A;ceres&#x53EA;&#x63A5;&#x53D7;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x4F18;&#x5316;, &#x4E5F;&#x5C31;&#x662F; $e^Te$ &#x3002;&#x4E3A;&#x4E86;&#x5C06;&#x6B27;&#x5F0F;&#x8DDD;&#x79BB;&#x8F6C;&#x6362;&#x4E3A;&#x9A6C;&#x6C0F;&#x8DDD;&#x79BB;&#xFF0C;&#x9996;&#x5148;&#x5F97;&#x628A; $P^{-1}$ &#x505A;LLT&#x5206;&#x89E3;&#xFF0C;&#x5373; $P^{-1}=LL^T$ , &#x56E0;&#x6B64;&#x6709; $e^TP^{-1}e = e^TLL^Te = (L^Te)^T(L^Te)$ , &#x4EE4; $e&#x2032;=L^Te$ &#x4F5C;&#x4E3A;&#x65B0;&#x7684;&#x4F18;&#x5316;&#x8BEF;&#x5DEE;, &#x8FD9;&#x6837;&#x5C31;&#x80FD;&#x7528;ceres&#x6C42;&#x89E3;&#x4E86;&#x3002;<code>Eigen::LLT&lt;Eigen::Matrix&lt;double, 15, 15&gt;&gt;(pre_integration-&gt;covariance.inverse()).matrixL().transpose()</code>&#x8FD9;&#x4E00;&#x884C;&#x4EE3;&#x7801;&#x5176;&#x5B9E;&#x5C31;&#x662F;&#x4EE3;&#x8868;&#x5C06;$P^{&#x2212;1}$&#x4F5C;LLT&#x5206;&#x89E3;&#xFF0C;&#x7136;&#x540E;&#x53D6;$L^T$&#x3002;</p>
<h1 id="&#x89C6;&#x89C9;&#x6B8B;&#x5DEE;&#x9879;"><a href="#&#x89C6;&#x89C9;&#x6B8B;&#x5DEE;&#x9879;" class="headerlink" title="&#x89C6;&#x89C9;&#x6B8B;&#x5DEE;&#x9879;"></a>&#x89C6;&#x89C9;&#x6B8B;&#x5DEE;&#x9879;</h1><p>&#x89C6;&#x89C9;&#x7684;&#x6B8B;&#x5DEE;&#x9879;&#x5728; SLAM &#x7CFB;&#x7EDF;&#x4E2D;&#x90FD;&#x5927;&#x540C;&#x5C0F;&#x5F02;&#xFF0C;&#x4F46;&#x5728; VINS-Mono &#x4E2D;&#x8981;&#x6CE8;&#x610F;&#x4E24;&#x70B9;&#x3002;<strong>&#x7B2C;&#x4E00;</strong>&#x662F; VINS-Mono &#x4E2D;&#x5BF9;&#x4E8E;&#x5730;&#x56FE;&#x70B9;&#x7684;&#x53C2;&#x6570;&#x5316;&#x5F62;&#x5F0F;&#xFF0C;&#x5373;<strong>&#x9006;&#x6DF1;&#x5EA6;</strong>&#x7684;&#x5F62;&#x5F0F;&#x3002;<strong>&#x7B2C;&#x4E8C;</strong>&#x662F;&#x6B8B;&#x5DEE;&#x7684;&#x8BA1;&#x7B97;&#x3002;&#x6B8B;&#x5DEE;&#x7684;&#x8BA1;&#x7B97;&#x7ED9;&#x51FA;&#x4E86;&#x4E24;&#x79CD;&#x5F62;&#x5F0F;&#xFF0C;&#x7B2C;&#x4E00;&#x79CD;&#x662F;&#x5728;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x4E0A;&#x7684;&#x91CD;&#x6295;&#x5F71;&#x8BEF;&#x5DEE;&#xFF0C;&#x8FD9;&#x79CD;&#x8BEF;&#x5DEE;&#x66F4;&#x5E38;&#x89C1;&#xFF0C;&#x4E5F;&#x66F4;&#x597D;&#x7406;&#x89E3;&#x3002;&#x7B2C;&#x4E8C;&#x79CD;&#x662F;&#x628A;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x4E0A;&#x7684;&#x91CD;&#x6295;&#x5F71;&#x8BEF;&#x5DEE;&#x6295;&#x5F71;&#x5230;Unit sphere&#x7684;&#x8BEF;&#x5DEE;&#x3002;&#x8BBA;&#x6587;&#x4E2D;&#x7ED9;&#x51FA;&#x7684;&#x662F;&#x7B2C;&#x4E8C;&#x79CD;&#xFF0C;&#x800C;&#x4EE3;&#x7801;&#x4E2D;&#x4E24;&#x79CD;&#x8BEF;&#x5DEE;&#x90FD;&#x4FDD;&#x7559;&#x4E86;&#xFF0C;&#x5E76;&#x7528;&#x5B8F;<code>UNIT_SPHERE_ERROR</code>&#x8FDB;&#x884C;&#x63A7;&#x5236;&#x3002;&#x8BEF;&#x5DEE;&#x8BA1;&#x7B97;&#x516C;&#x5F0F;&#x662F;&#x5199;&#x6210;&#x8FD9;&#x6837;&#x7684;&#xFF1A;<br><img src="/2018/04/24/SLAM/VINS/VINSVIO/7.png" width="50%" height="50%"></p>
<p>&#x4EE3;&#x7801;&#x662F;&#x5199;&#x6210;&#x8FD9;&#x6837;&#x7684;&#xFF1A;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">    <span class="comment">//i&#x65F6;&#x523B;&#x76F8;&#x673A;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;map point&#x7684;&#x9006;&#x6DF1;&#x5EA6;</span></div><div class="line">    <span class="keyword">double</span> inv_dep_i = parameters[<span class="number">3</span>][<span class="number">0</span>];</div><div class="line">    <span class="comment">//i&#x65F6;&#x523B;&#x76F8;&#x673A;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;map point&#x5750;&#x6807;</span></div><div class="line">    Eigen::Vector3d pts_camera_i = pts_i / inv_dep_i;</div><div class="line">    <span class="comment">//i&#x65F6;&#x523B;IMU&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;map point&#x5750;&#x6807;</span></div><div class="line">    Eigen::Vector3d pts_imu_i = qic * pts_camera_i + tic;</div><div class="line">    <span class="comment">//&#x4E16;&#x754C;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;map point&#x5750;&#x6807;</span></div><div class="line">    Eigen::Vector3d pts_w = Qi * pts_imu_i + Pi;</div><div class="line">    <span class="comment">//&#x5728;j&#x65F6;&#x523B;imu&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;map point&#x5750;&#x6807;</span></div><div class="line">    Eigen::Vector3d pts_imu_j = Qj.inverse() * (pts_w - Pj);</div><div class="line">    <span class="comment">//&#x5728;j&#x65F6;&#x523B;&#x76F8;&#x673A;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;map point&#x5750;&#x6807;</span></div><div class="line">    Eigen::Vector3d pts_camera_j = qic.inverse() * (pts_imu_j - tic);</div><div class="line">    Eigen::Map&lt;Eigen::Vector2d&gt; residual(residuals);</div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> UNIT_SPHERE_ERROR </span></div><div class="line">    residual =  tangent_base * (pts_camera_j.normalized() - pts_j.normalized());</div><div class="line"><span class="meta">#<span class="meta-keyword">else</span></span></div><div class="line">    <span class="keyword">double</span> dep_j = pts_camera_j.z();</div><div class="line">    residual = (pts_camera_j / dep_j).head&lt;<span class="number">2</span>&gt;() - pts_j.head&lt;<span class="number">2</span>&gt;();</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure></p>
<p>&#x628A;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x4E0A;&#x7684;&#x91CD;&#x6295;&#x5F71;&#x8BEF;&#x5DEE;&#x6295;&#x5F71;&#x5230;Unit sphere&#x4E0A;&#x7684;&#x597D;&#x5904;&#x5C31;&#x662F;&#x53EF;&#x4EE5;&#x652F;&#x6301;&#x6240;&#x6709;&#x7C7B;&#x578B;&#x7684;&#x76F8;&#x673A;&#x3002;&#x5BF9;&#x4E8E;&#x6210;&#x50CF;&#x5728;&#x5E73;&#x9762;&#x4E0A;&#x7684;&#x76F8;&#x673A;&#xFF0C;&#x76F4;&#x63A5;&#x8BA1;&#x7B97;&#x6295;&#x5F71;&#x5728;x&#x8F74;&#x548C;y&#x8F74;&#x5750;&#x6807;&#x7684;&#x5DEE;&#x662F;&#x6CA1;&#x6709;&#x95EE;&#x9898;&#x7684;&#xFF0C;&#x4F46;&#x662F;&#x5982;&#x679C;&#x662F;&#x6210;&#x50CF;&#x5728;&#x66F2;&#x9762;&#x4E0A;&#x7684;&#x76F8;&#x673A;&#xFF0C;&#x76F4;&#x63A5;&#x8BA1;&#x7B97;&#x6295;&#x5F71;&#x5728;x&#x8F74;&#x548C;y&#x8F74;&#x5750;&#x6807;&#x7684;&#x5DEE;&#x5F80;&#x5F80;&#x662F;&#x4E0D;&#x591F;&#x51C6;&#x786E;&#x7684;&#xFF0C;&#x6240;&#x4EE5;&#x8981;&#x8F6C;&#x6362;&#x5230;Unit sphere&#x3002;&#x8FD9;&#x90E8;&#x5206;&#x7684;&#x5B9E;&#x9645;&#x6548;&#x679C;&#x8FD8;&#x6709;&#x5F85;&#x8FDB;&#x4E00;&#x6B65;&#x9A8C;&#x8BC1;&#x3002;</p>
<p>&#x5728;&#x6700;&#x65B0;&#x7248;&#x672C;&#x7684; VINS-Mono &#x4E2D;&#xFF0C;&#x8FD8;&#x6DFB;&#x52A0;&#x4E86;&#x5BF9; imu-camera &#x65F6;&#x95F4;&#x6233;&#x4E0D;&#x5B8C;&#x5168;&#x540C;&#x6B65;&#x548C; Rolling shutter &#x76F8;&#x673A;&#x7684;&#x652F;&#x6301;&#x3002;&#x4E3B;&#x8981;&#x7684;&#x601D;&#x8DEF;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x524D;&#x7AEF;&#x5149;&#x6D41;&#x8BA1;&#x7B97;&#x5F97;&#x5230;&#x6BCF;&#x4E2A;&#x89D2;&#x70B9;&#x5728;&#x5F52;&#x4E00;&#x5316;&#x7684;&#x901F;&#x5EA6;&#xFF0C;&#x6839;&#x636E; imu-camera&#x65F6;&#x95F4;&#x6233;&#x7684;&#x65F6;&#x95F4;&#x540C;&#x6B65;&#x8BEF;&#x5DEE;&#x548C;Rolling shutter&#x76F8;&#x673A;&#x505A;&#x4E00;&#x6B21;rolling&#x7684;&#x65F6;&#x95F4;&#xFF0C;&#x5BF9;&#x89D2;&#x70B9;&#x7684;&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807;&#x8FDB;&#x884C;&#x8C03;&#x6574;&#x3002;<br>&#x8FD9;&#x90E8;&#x5206;&#x4EE3;&#x7801;&#x5728;<code>projection_td_factor.h</code>&#x6587;&#x4EF6;&#x4E2D;&#x3002;&#x5176;&#x5B9E;&#x8FD9;&#x91CC;&#x9762;&#x6700;&#x5173;&#x952E;&#x7684;&#x4EE3;&#x7801;&#x5C31;&#x4E24;&#x884C;&#x3002;<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pts_i_td = pts_i - (td - td_i + TR / ROW * row_i) * velocity_i;</div><div class="line">pts_j_td = pts_j - (td - td_j + TR / ROW * row_j) * velocity_j;</div></pre></td></tr></table></figure></p>
<p>&#x5176;&#x4E2D;<code>pts_i</code>&#x662F;&#x89D2;&#x70B9;&#x5728;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x7684;&#x5750;&#x6807;&#xFF0C;<code>td</code>&#x8868;&#x793A;imu-camera&#x65F6;&#x95F4;&#x6233;&#x7684;&#x65F6;&#x95F4;&#x540C;&#x6B65;&#x8BEF;&#x5DEE;&#xFF0C;&#x662F;&#x5F85;&#x4F18;&#x5316;&#x9879;&#x3002;<code>TR</code>&#x8868;&#x793A;Rolling shutter&#x76F8;&#x673A;&#x505A;&#x4E00;&#x6B21;rolling&#x7684;&#x65F6;&#x95F4;&#x3002;&#x56E0;&#x4E3A;&#x5728;&#x5904;&#x7406;imu&#x6570;&#x636E;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x5DF2;&#x7ECF;&#x51CF;&#x8FC7;&#x4E00;&#x6B21;&#x65F6;&#x95F4;&#x540C;&#x6B65;&#x8BEF;&#x5DEE;&#xFF0C;&#x56E0;&#x6B64;&#x4FEE;&#x6B63;&#x540E;&#x7684;&#x65F6;&#x95F4;&#x8BEF;&#x5DEE;&#x662F;<code>td - td_i</code>&#x3002;&#x5176;&#x6B21;<code>row_i</code>&#x662F;&#x89D2;&#x70B9;&#x56FE;&#x50CF;&#x5750;&#x6807;&#x7684;&#x7EB5;&#x5750;&#x6807;&#xFF0C;<code>ROW</code>&#x56FE;&#x50CF;&#x5750;&#x6807;&#x7EB5;&#x5750;&#x6807;&#x7684;&#x6700;&#x5927;&#x503C;&#xFF0C;&#x56E0;&#x6B64;<code>TR / ROW * row_i</code>&#x5C31;&#x662F;&#x76F8;&#x673A; rolling &#x5230;&#x8FD9;&#x4E00;&#x884C;&#x65F6;&#x6240;&#x7528;&#x7684;&#x65F6;&#x95F4;&#x3002;<code>velocity_i</code>&#x662F;&#x8BE5;&#x89D2;&#x70B9;&#x5728;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x7684;&#x8FD0;&#x52A8;&#x901F;&#x5EA6;&#x3002;&#x6240;&#x4EE5;&#x6700;&#x540E;&#x5F97;&#x5230;&#x7684;<code>pts_i_td</code>&#x662F;&#x5904;&#x7406;&#x65F6;&#x95F4;&#x540C;&#x6B65;&#x8BEF;&#x5DEE;&#x548C;Rolling shutter&#x65F6;&#x95F4;&#x540E;&#xFF0C;&#x89D2;&#x70B9;&#x5728;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x7684;&#x5750;&#x6807;&#x3002;</p>
<h1 id="&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;"><a href="#&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;" class="headerlink" title="&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;"></a>&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;</h1><p>&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;&#x662F;&#x6240;&#x6709;&#x6B8B;&#x5DEE;&#x9879;&#x91CC;&#x9762;&#x6700;&#x590D;&#x6742;&#x7684;&#x4E00;&#x90E8;&#x5206;&#xFF0C;&#x4F46;&#x662F;&#x53C8;&#x662F;&#x4E0D;&#x53EF;&#x6216;&#x7F3A;&#x7684;&#x3002;&#x6211;&#x4EEC;&#x5148;&#x4ECE;&#x5B8F;&#x89C2;&#x7684;&#x89D2;&#x5EA6;&#x6765;&#x5206;&#x6790;&#x8FD9;&#x4E2A;&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;&#x5230;&#x5E95;&#x5728;&#x5E72;&#x4EC0;&#x4E48;&#x3002;<br>&#x4E4B;&#x524D;&#x6211;&#x4EEC;&#x63D0;&#x5230;&#xFF0C;&#x4E3A;&#x4E86;&#x9650;&#x5236;&#x4F18;&#x5316;&#x53D8;&#x91CF;&#x7684;&#x6570;&#x76EE;&#xFF0C;VINS-Mono &#x91C7;&#x7528;&#x4E86;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x7684;&#x5F62;&#x5F0F;&#xFF0C;&#x5F85;&#x4F30;&#x8BA1;&#x91CF;&#x5747;&#x4E3A;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x72B6;&#x6001;&#x53D8;&#x91CF;&#x3002;&#x5F53;&#x6709;&#x65B0;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x52A0;&#x5165;&#x5230;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x4E2D;&#xFF0C;&#x9700;&#x8981;&#x53BB;&#x9664;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x4E00;&#x4E2A;&#x65E7;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x4EE5;&#x53CA;&#x76F8;&#x5173;&#x7684;&#x72B6;&#x6001;&#x91CF;&#xFF0C;&#x4EE5;&#x4FDD;&#x8BC1;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x72B6;&#x6001;&#x91CF;&#x6570;&#x76EE;&#x4FDD;&#x6301;&#x7A33;&#x5B9A;&#x3002;VINS-Mono&#x6839;&#x636E;&#x5F53;&#x524D;&#x5E27;&#x662F;&#x5426;&#x4E3A;&#x5173;&#x952E;&#x5E27;&#xFF0C;&#x4F1A;&#x6267;&#x884C;&#x4E24;&#x79CD;&#x4E0D;&#x540C;&#x7684;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x7684;&#x7B56;&#x7565;&#x3002;&#x6ED1;&#x7A97;&#x7B56;&#x7565;&#x7559;&#x5230;&#x4E0B;&#x4E00;&#x7AE0;&#x8282;&#x518D;&#x8BA8;&#x8BBA;&#xFF0C;&#x4F46;&#x4E0D;&#x7BA1;&#x6267;&#x884C;&#x54EA;&#x79CD;&#x6ED1;&#x7A97;&#x7B56;&#x7565;&#xFF0C;&#x90FD;&#x4F1A;&#x4ECE;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x4E2D;&#x5220;&#x9664;&#x67D0;&#x4E2A;&#x56FE;&#x50CF;&#x5E27;&#xFF0C;&#x800C;&#x8FD9;&#x4E2A;&#x56FE;&#x50CF;&#x5E27;&#x901A;&#x8FC7; <strong>IMU &#x9884;&#x79EF;&#x5206;</strong>&#x548C;&#x89C2;&#x6D4B;&#x5230;&#x67D0;&#x4E9B;<strong>&#x5730;&#x56FE;&#x70B9;</strong>&#xFF0C;&#x4F1A;&#x4E0E;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x67D0;&#x4E9B;&#x5176;&#x4ED6;&#x56FE;&#x50CF;&#x5E27;&#x4E4B;&#x95F4;&#x4EA7;&#x751F;&#x7EA6;&#x675F;&#x5173;&#x7CFB;&#xFF0C;&#x5982;&#x679C;&#x76F4;&#x63A5;&#x628A;&#x8BE5;&#x56FE;&#x50CF;&#x5E27;&#x4ECE;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x5220;&#x9664;&#xFF0C;&#x4F1A;&#x767D;&#x767D;&#x4E22;&#x5931;&#x8FD9;&#x4E9B;&#x7EA6;&#x675F;&#x5173;&#x7CFB;&#xFF0C;&#x4ECE;&#x800C;&#x964D;&#x4F4E;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x72B6;&#x6001;&#x91CF;&#x7684;&#x4F18;&#x5316;&#x7ED3;&#x679C;&#x3002;&#x90A3;&#x4E48;&#x600E;&#x4E48;&#x624D;&#x80FD;&#x5373;&#x4E22;&#x6389;&#x67D0;&#x4E2A;&#x56FE;&#x50CF;&#x5E27;&#xFF0C;&#x53EA;&#x4F18;&#x5316;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x72B6;&#x6001;&#x91CF;&#xFF0C;&#x53C8;&#x4FDD;&#x7559;&#x8FD9;&#x4E2A;&#x4E22;&#x6389;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x4E0E;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x56FE;&#x50CF;&#x5E27;&#x72B6;&#x6001;&#x91CF;&#x4E4B;&#x95F4;&#x7684;&#x7EA6;&#x675F;&#x5173;&#x7CFB;&#x5462;&#xFF1F;&#x8FD9;&#x4E2A;&#x65F6;&#x5019;&#x5C31;&#x8981;&#x6D3E;&#x51FA;&#x795E;&#x5668;&#x2014;<strong>&#x8FB9;&#x7F18;&#x5316;</strong> (Marginalization) &#x3002;&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;&#x5C31;&#x662F;&#x901A;&#x8FC7;&#x8FB9;&#x7F18;&#x5316;&#x5F97;&#x5230;&#x7684;&#x3002;&#x8FD9;&#x90E8;&#x5206;&#x7684;&#x77E5;&#x8BC6;&#x4E3B;&#x8981;&#x53C2;&#x8003;&#x4E86;&#x8D3A;&#x4E00;&#x5BB6;&#x5927;&#x795E;&#x7684;&#x535A;&#x5BA2;[4][5]&#x3002;&#x5173;&#x4E8E;&#x6784;&#x9020;&#x8FB9;&#x7F18;&#x5316;&#x6B8B;&#x5DEE;&#x9879;&#x7684;&#x6570;&#x5B66;&#x77E5;&#x8BC6;&#x5F88;&#x591A;&#xFF0C;&#x8FD9;&#x91CC;&#x6211;&#x7B80;&#x5355;&#x5F15;&#x7528;&#x4E00;&#x4E0B;&#x8FB9;&#x7F18;&#x5316;&#x7684;&#x7ED3;&#x8BBA;&#x3002;&#x5047;&#x8BBE;&#x9700;&#x8981;&#x8FB9;&#x7F18;&#x5316;&#x7684;&#x53D8;&#x91CF;&#x4E3A; $x_m$&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x5BF9;&#x5E94;&#x4E0A;&#x9762;&#x63D0;&#x5230;&#x7684;&#x6ED1;&#x7A97;&#x8FC7;&#x7A0B;&#x4E2D;&#x8981;&#x88AB;&#x4E22;&#x6389;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x7684;&#x72B6;&#x6001;&#x91CF;&#x3002;&#x9700;&#x8981;&#x6C42;&#x89E3;&#x7684;&#x53D8;&#x91CF;&#x4E3A;$x_b$&#xFF0C;&#x5373;&#x5BF9;&#x5E94;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x6240;&#x6709;&#x7684;&#x72B6;&#x6001;&#x91CF;&#x3002;&#x5982;&#x679C;&#x4E0D;&#x4E22;&#x5F03;$x_m$&#xFF0C;&#x5728;&#x6C42;&#x89E3;&#x6B8B;&#x5DEE; $\min\limits_{x}e=f(x)$ &#x7684;&#x6700;&#x5C0F;&#x503C;&#x65F6;&#xFF0C;&#x7528;&#x9AD8;&#x65AF;&#x725B;&#x987F;&#x65B9;&#x6CD5;&#x5C06;&#x6784;&#x9020; $H \delta x = b$&#xFF0C;&#x7136;&#x540E;&#x7528; $\delta x$ &#x53BB;&#x66F4;&#x65B0; $x$ &#xFF0C;&#x5F97;&#x5230;&#x4E00;&#x4E2A;&#x66F4;&#x51C6;&#x786E;&#x7684;&#x72B6;&#x6001;&#x4F30;&#x8BA1;&#x3002;&#x7136;&#x800C;&#x73B0;&#x5728;&#x8981;&#x4E22;&#x5F03; $x_m$&#xFF0C;&#x53EA;&#x4F30;&#x8BA1; $x_b$&#x3002; &#x56E0;&#x6B64;&#x53EF;&#x4EE5;&#x628A; $H \delta x = b$ &#x5199;&#x6210;&#x53E6;&#x5916;&#x4E00;&#x79CD;&#x5F62;&#x5F0F;<br>$$<br>\begin{bmatrix} H_{mm} &amp; H_{bm}^T \\ H_{bm} &amp; H_{bb} \end{bmatrix} \begin{bmatrix} x_{m} \\ x_{b} \end{bmatrix} = \begin{bmatrix} b_{m} \\ b_{b} \end{bmatrix}<br>$$<br>&#x901A;&#x8FC7;&#x9AD8;&#x65AF;&#x6D88;&#x5143;&#x53EF;&#x4EE5;&#x53D8;&#x6210;&#x5982;&#x4E0B;&#x5F62;&#x5F0F;&#xFF1A;<br>$$<br>\begin{bmatrix} H_{mm} &amp; H_{bm}^T \\ 0 &amp; H_{bb}-H_{bm}^TH_{mm}^{-1}H_{bm} \end{bmatrix} \begin{bmatrix} x_{m} \\ x_{b} \end{bmatrix} = \begin{bmatrix} b_{m} \\ b_{b}-b_{m}H_{mm}^{-1}H_{bm} \end{bmatrix}<br>$$</p>
<p>&#x4E5F;&#x5C31;&#x662F;&#x8BF4;&#x901A;&#x8FC7;&#x6784;&#x9020; $(H_{bb}-H_{bm}^TH_{mm}^{-1}H_{bm}) \delta x_b = b_{b}-b_{m}H_{mm}^{-1}H_{bm}$ &#x7684;&#x65B9;&#x5F0F;&#x6C42;&#x89E3; $\delta x_b$&#xFF0C;&#x7136;&#x540E;&#x66F4;&#x65B0;$x_b$ &#xFF0C;&#x5B8C;&#x5168;&#x53EF;&#x4EE5;&#x8FBE;&#x5230;&#x5373;&#x4E22;&#x5F03; $x_m$&#xFF0C;&#x53C8;&#x4FDD;&#x7559;&#x5176;&#x4E0E; $x_b$ &#x4E4B;&#x95F4;&#x7684;&#x7EA6;&#x675F;&#x5173;&#x7CFB;&#x3002;<br>&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;&#x5C06;&#x88AB;&#x6784;&#x9020;&#x6210; $ e_{priori} = \dfrac{1}{2}\left |  \hat{x_b} - x_b \right |^2 $&#x3002;&#x56E0;&#x6B64; $\min\limits_{x}e=f(x)$ &#x5C31;&#x53EF;&#x4EE5;&#x7B49;&#x4EF7;&#x4E8E; $\min\limits_{x_b} e = e_{priori} + f(x_b)$&#x3002;&#x4F18;&#x5316;&#x53D8;&#x91CF;&#x4ECE; $x$ &#x53D8;&#x6210;&#x4E86; $x_b$&#x3002;<br>&#x63A5;&#x4E0B;&#x6765;&#x518D;&#x7740;&#x91CD;&#x5206;&#x6790;&#x4E00;&#x4E0B; $H$ &#x77E9;&#x9635;&#x3002;&#x5728;&#x9AD8;&#x65AF;&#x725B;&#x987F;&#x6CD5;&#x4E2D;&#xFF0C;$H$ &#x662F;&#x7528; &#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635; $J$ &#x6765;&#x8FD1;&#x4F3C;&#x7684;&#xFF0C;&#x5373; $H=J^TJ$&#xFF0C;&#x90A3;&#x4E48;&#x5F53;&#x5BF9; $H$ &#x8FDB;&#x884C;&#x5206;&#x89E3;&#x5F97;&#x5230; $H_{mm} ,H_{bm},H_{bb}$ &#x5BF9;&#x5E94;&#x5230; $J$ &#x4E0A;&#x662F;&#x4EC0;&#x4E48;&#x4E1C;&#x897F;&#xFF1F;&#x5176;&#x5B9E; $J$ &#x672C;&#x8EAB;&#x53EF;&#x4EE5;&#x62C6;&#x89E3;&#x4E3A;&#x4E24;&#x4E2A;&#x90E8;&#x5206;&#xFF0C;&#x7B2C;&#x4E00;&#x90E8;&#x5206;&#x662F;&#x6B8B;&#x5DEE;&#x5BF9;&#x4E8E; $x_m$ &#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#xFF0C;&#x7528; $J_m$&#x8868;&#x793A;&#xFF0C;&#x7B2C;&#x4E8C;&#x90E8;&#x5206;&#x662F;&#x6B8B;&#x5DEE;&#x5BF9;&#x4E8E; $x_b$ &#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#xFF0C;&#x7528;$J_b$&#x8868;&#x793A;&#x3002;&#x5219;&#x6709; $H_{mm}= J_m^TJ_m, H_{bm}= J_b^TJ_m, H_{bb}= J_b^TJ_b$&#x3002;<br>&#x7406;&#x8BBA;&#x90E8;&#x5206;&#x5927;&#x81F4;&#x662F;&#x8FD9;&#x6837;&#x7684;&#x539F;&#x7406;&#xFF0C;&#x4F46;&#x5BF9;&#x5E94;&#x5230; VINS-Mono &#x7684;&#x6E90;&#x7801;&#x4E2D;&#xFF0C;&#x8FD9;&#x5C06;&#x662F;&#x4E00;&#x4E2A;&#x975E;&#x5E38;&#x590D;&#x6742;&#x7684;&#x8FC7;&#x7A0B;&#x3002;<br>&#x4F46;&#x603B;&#x4F53;&#x4E0A;&#x5148;&#x9A8C;&#x6B8B;&#x5DEE;&#x9879;&#x7684;&#x6784;&#x9020;&#x53EF;&#x4EE5;&#x5206;&#x4E3A;&#x4EE5;&#x4E0B;&#x51E0;&#x4E2A;&#x6B65;&#x9AA4;&#xFF1A;<br>1.&#x628A;&#x4E0A;&#x4E00;&#x6B21;&#x5148;&#x9A8C;&#x9879;&#x4E2D;&#x7684;&#x6B8B;&#x5DEE;&#x9879;&#x4F20;&#x9012;&#x7ED9;&#x5F53;&#x524D;&#x5148;&#x9A8C;&#x9879;&#xFF0C;&#x5E76;&#x4ECE;&#x4E2D;&#x53BB;&#x9664;&#x9700;&#x8981;&#x4E22;&#x5F03;&#x7684;&#x72B6;&#x6001;&#x91CF;<br>2.&#x6DFB;&#x52A0;&#x4E0E;&#x5F53;&#x524D;&#x9700;&#x8981;&#x4E22;&#x5F03;&#x7684;&#x72B6;&#x6001;&#x91CF;&#x76F8;&#x5173;&#x7684;&#x7EA6;&#x675F;&#x9879;<br>3.&#x901A;&#x8FC7;&#x51FD;&#x6570;<code>void MarginalizationInfo::preMarginalize()</code>&#x5F97;&#x5230;&#x6BCF;&#x4E2A;&#x6B8B;&#x5DEE;&#x9879;&#x5BF9;&#x5E94;&#x7684;&#x53C2;&#x6570;&#x5757;&#xFF0C;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#xFF0C;&#x6B8B;&#x5DEE;&#x503C;&#x3002;<br>4.&#x901A;&#x8FC7;&#x51FD;&#x6570;<code>void MarginalizationInfo::marginalize()</code>&#x5C06;&#x6B65;&#x9AA4;3&#x4E2D;&#x5F97;&#x5230;&#x7684;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x548C;&#x6B8B;&#x5DEE;&#x503C;&#x8FDB;&#x884C;&#x7EC4;&#x5408;&#xFF0C;&#x5F97;&#x5230;&#x6574;&#x4E2A;&#x5148;&#x9A8C;&#x9879;&#x7684;&#x53C2;&#x6570;&#x5757;&#xFF0C;&#x96C5;&#x53EF;&#x6BD4;&#x77E9;&#x9635;&#x548C;&#x6B8B;&#x5DEE;&#x503C;&#x3002;</p>
<p>&#x901A;&#x8FC7;&#x4EE5;&#x4E0A;&#x56DB;&#x6B65;&#x5148;&#x9A8C;&#x9879;&#x5C31;&#x7B97;&#x6784;&#x9020;&#x5B8C;&#x6210;&#x4E86;&#xFF0C;&#x5728;&#x5BF9;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x72B6;&#x6001;&#x91CF;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#x65F6;&#xFF0C;&#x628A;&#x5B83;&#x4E0E; IMU &#x6B8B;&#x5DEE;&#x9879;&#x548C;&#x89C6;&#x89C9;&#x6B8B;&#x5DEE;&#x9879;&#x653E;&#x5728;&#x4E00;&#x8D77;&#x4F18;&#x5316;&#xFF0C;&#x4ECE;&#x800C;&#x5F97;&#x5230;&#x4E0D;&#x4E22;&#x5931;&#x5386;&#x53F2;&#x4FE1;&#x606F;&#x7684;&#x6700;&#x65B0;&#x72B6;&#x6001;&#x4F30;&#x8BA1;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<h1 id="&#x6ED1;&#x7A97;&#x7B56;&#x7565;"><a href="#&#x6ED1;&#x7A97;&#x7B56;&#x7565;" class="headerlink" title="&#x6ED1;&#x7A97;&#x7B56;&#x7565;"></a>&#x6ED1;&#x7A97;&#x7B56;&#x7565;</h1><p>&#x6839;&#x636E;&#x5F53;&#x524D;&#x5E27;&#x662F;&#x5426;&#x4E3A;&#x5173;&#x952E;&#x5E27;&#xFF0C;VINS-Mono &#x4E2D;&#x5C06;&#x4F1A;&#x91C7;&#x7528;&#x4E24;&#x79CD;&#x4E0D;&#x540C;&#x7684;&#x7B56;&#x7565;&#x3002;&#x8FD9;&#x4E24;&#x79CD;&#x7B56;&#x7565;&#x53EF;&#x4EE5;&#x7528;&#x4E0B;&#x56FE;&#x89E3;&#x91CA;&#xFF1A;</p>
<p><img src="/2018/04/24/SLAM/VINS/VINSVIO/8.png" width="60%" height="60%"></p>
<p>&#x5982;&#x679C;&#x5F53;&#x524D;&#x5E27;<strong>&#x662F;&#x5173;&#x952E;&#x5E27;</strong>&#xFF0C;&#x5219;&#x4E22;&#x5F03;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x6700;&#x8001;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#xFF0C;&#x540C;&#x65F6;&#x5BF9;&#x4E0E;&#x8BE5;&#x56FE;&#x50CF;&#x5E27;&#x5173;&#x8054;&#x7684;&#x7EA6;&#x675F;&#x9879;&#x8FDB;&#x884C;&#x8FB9;&#x7F18;&#x5316;&#x5904;&#x7406;&#x3002;&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x5982;&#x679C;&#x8BE5;&#x5173;&#x952E;&#x5E27;&#x662F;&#x89C2;&#x5BDF;&#x5230;&#x67D0;&#x4E2A;&#x5730;&#x56FE;&#x70B9;&#x7684;&#x7B2C;&#x4E00;&#x5E27;&#xFF0C;&#x5219;&#x9700;&#x8981;&#x628A;&#x8BE5;&#x5730;&#x56FE;&#x70B9;&#x7684;&#x6DF1;&#x5EA6;&#x8F6C;&#x79FB;&#x5230;&#x540E;&#x9762;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x4E2D;&#x53BB;&#x3002;<br>&#x5982;&#x679C;&#x5F53;&#x524D;&#x5E27;<strong>&#x4E0D;&#x662F;&#x5173;&#x952E;&#x5E27;</strong>&#xFF0C;&#x5219;&#x4E22;&#x5F03;&#x5F53;&#x524D;&#x5E27;&#x7684;&#x524D;&#x4E00;&#x5E27;&#x3002;&#x56E0;&#x4E3A;&#x5224;&#x5B9A;&#x5F53;&#x524D;&#x5E27;&#x4E0D;&#x662F;&#x5173;&#x952E;&#x5E27;&#x7684;&#x6761;&#x4EF6;&#x5C31;&#x662F;&#x5F53;&#x524D;&#x5E27;&#x4E0E;&#x524D;&#x4E00;&#x5E27;&#x89C6;&#x5DEE;&#x5F88;&#x5C0F;&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x8BF4;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x524D;&#x4E00;&#x5E27;&#x5F88;&#x76F8;&#x4F3C;&#xFF0C;&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#x76F4;&#x63A5;&#x4E22;&#x5F03;&#x524D;&#x4E00;&#x5E27;&#xFF0C;&#x7136;&#x540E;&#x7528;&#x5F53;&#x524D;&#x5E27;&#x4EE3;&#x66FF;&#x524D;&#x4E00;&#x5E27;&#x3002;&#x4E3A;&#x4EC0;&#x4E48;&#x8FD9;&#x91CC;&#x53EF;&#x4EE5;&#x4E0D;&#x5BF9;&#x524D;&#x4E00;&#x5E27;&#x8FDB;&#x884C;&#x8FB9;&#x7F18;&#x5316;&#xFF0C;&#x800C;&#x662F;&#x76F4;&#x63A5;&#x4E22;&#x5F03;&#xFF0C;&#x539F;&#x56E0;&#x5C31;&#x662F;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x524D;&#x4E00;&#x5E27;&#x5F88;&#x76F8;&#x4F3C;&#xFF0C;&#x56E0;&#x6B64;&#x5F53;&#x524D;&#x5E27;&#x4E0E;&#x5730;&#x56FE;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7EA6;&#x675F;&#x548C;&#x524D;&#x4E00;&#x5E27;&#x4E0E;&#x5730;&#x56FE;&#x70B9;&#x4E4B;&#x95F4;&#x7684;&#x7EA6;&#x675F;&#x662F;&#x5F88;&#x63A5;&#x8FD1;&#x7684;&#xFF0C;&#x76F4;&#x63A5;&#x4E22;&#x5F03;&#x5E76;&#x4E0D;&#x4F1A;&#x9020;&#x6210;&#x6574;&#x4E2A;&#x7EA6;&#x675F;&#x5173;&#x7CFB;&#x4E22;&#x5931;&#x4FE1;&#x606F;&#x3002;&#x8FD9;&#x91CC;&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x8981;&#x628A;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x524D;&#x4E00;&#x5E27;&#x4E4B;&#x95F4;&#x7684; IMU &#x9884;&#x79EF;&#x5206;&#x8F6C;&#x6362;&#x4E3A;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x524D;&#x4E8C;&#x5E27;&#x4E4B;&#x95F4;&#x7684; IMU &#x9884;&#x79EF;&#x5206;&#x3002;</p>
<h1 id="&#x53C2;&#x8003;&#x6587;&#x732E;"><a href="#&#x53C2;&#x8003;&#x6587;&#x732E;" class="headerlink" title="&#x53C2;&#x8003;&#x6587;&#x732E;"></a>&#x53C2;&#x8003;&#x6587;&#x732E;</h1><p>[1] On-Manifold Preintegration for Real-Time Visual-Inertial Odometry<br>[2] Quaternion kinematics for the error-state Kalman filter<br>[3] <a href="https://www.zhihu.com/question/64381223" target="_blank" rel="external">https://www.zhihu.com/question/64381223</a><br>[4] <a href="https://blog.csdn.net/heyijia0327/article/details/53707261" target="_blank" rel="external">https://blog.csdn.net/heyijia0327/article/details/53707261</a><br>[5] <a href="https://blog.csdn.net/heyijia0327/article/details/52822104" target="_blank" rel="external">https://blog.csdn.net/heyijia0327/article/details/52822104</a></p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 源码学习 </tag>
            
            <tag> VINS-Mono </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[VINS-Mono中的VIO初始化]]></title>
      <url>https://zhehangt.github.io/2018/04/19/SLAM/VINS/VINSInitialiaztion/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="&#x6458;&#x8981;"><a href="#&#x6458;&#x8981;" class="headerlink" title="&#x6458;&#x8981;"></a>&#x6458;&#x8981;</h1><p>VIO &#x521D;&#x59CB;&#x5316;&#x5BF9;&#x4E8E; VIO &#x7CFB;&#x7EDF;&#x6765;&#x8BB2;&#x5341;&#x5206;&#x91CD;&#x8981;&#xFF0C;VINS-Mono&#x5BF9;&#x521D;&#x59CB;&#x5316;&#x8FD9;&#x5757;&#x6BD4;&#x8F83;&#x91CD;&#x89C6;&#xFF0C;&#x76F8;&#x5173;&#x8BBA;&#x6587;&#x4E5F;&#x53D1;&#x4E86;&#x4E0D;&#x5C11;&#xFF0C;&#x672C;&#x7BC7;&#x535A;&#x5BA2;&#x5C06;&#x4ECE;&#x539F;&#x7406;&#x51FA;&#x53D1;&#xFF0C;&#x5BF9;&#x76F8;&#x5173;&#x4EE3;&#x7801;&#x8FDB;&#x884C;&#x68B3;&#x7406;&#x3002;<br>VINS-Mono&#x4E2D;VIO&#x521D;&#x59CB;&#x5316;&#x7684;&#x5165;&#x53E3;&#x4E3A;<code>bool Estimator::initialStructure()</code>&#x3002;&#x6574;&#x4E2A;&#x521D;&#x59CB;&#x5316;&#x8FC7;&#x7A0B;&#x4E3B;&#x8981;&#x6709;&#x5982;&#x4E0B;&#x51E0;&#x4E2A;&#x6B65;&#x9AA4;&#x3002;</p>
<h1 id="&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;"><a href="#&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;" class="headerlink" title="&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;"></a>&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;</h1><p>VIO &#x521D;&#x59CB;&#x5316;&#x7684;&#x4F9D;&#x8D56;&#x4E8E;&#x89C6;&#x89C9;&#x7684;&#x521D;&#x59CB;&#x5316;&#x3002;VINS-Mono&#x7684;&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;&#x4E0E;&#x4E00;&#x822C;&#x7684;&#x5355;&#x76EE;&#x521D;&#x59CB;&#x5316;&#x5DEE;&#x522B;&#x4E0D;&#x5927;&#x3002;&#x8FD9;&#x91CC;&#x6709;&#x51E0;&#x4E2A;&#x5173;&#x952E;&#x7684;&#x6570;&#x636E;&#x7ED3;&#x6784;&#x3002;&#x5176;&#x4E2D;&#x4E00;&#x4E2A;&#x662F;<code>vector&lt;SFMFeature&gt; sfm_f</code>&#xFF0C;&#x91CC;&#x9762;&#x4FDD;&#x5B58;&#x7684;&#x662F;&#x67D0;&#x4E2A;&#x89D2;&#x70B9;&#x5728;&#x4E00;&#x7CFB;&#x5217;&#x56FE;&#x50CF;&#x4E2D;&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807;&#xFF0C;&#x4FDD;&#x5B58;&#x5728;<code>observation</code>&#x53D8;&#x91CF;&#x5728;&#x4E2D;&#x3002;&#x53E6;&#x4E00;&#x4E2A;&#x662F;<code>map&lt;double, ImageFrame&gt; all_image_frame;</code>&#x3002;&#x8FD8;&#x6709;&#x4E00;&#x4E2A;&#x662F;<code>FeatureManager f_manager</code>&#x3002;<code>FeatureManager f_manager</code>&#x76F8;&#x5BF9;&#x6709;&#x4E9B;&#x590D;&#x6742;&#xFF0C;&#x6211;&#x4EEC;&#x6765;&#x7740;&#x91CD;&#x5206;&#x6790;&#x4E00;&#x4E0B;&#x3002;<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="built_in">map</span>&lt;<span class="keyword">double</span>, ImageFrame&gt; all_image_frame; <span class="comment">// &#x952E;&#x662F;&#x65F6;&#x95F4;&#x6233;&#xFF0C;&#x503C;&#x662F;&#x56FE;&#x50CF;&#x5E27;&#xFF0C;&#x56FE;&#x50CF;&#x5E27;&#x4E2D;&#x4FDD;&#x5B58;&#x4E86;&#x56FE;&#x50CF;&#x5E27;&#x7684;&#x4F4D;&#x59FF;&#xFF0C;&#x9884;&#x79EF;&#x5206;&#x91CF;&#x548C;&#x5173;&#x4E8E;&#x89D2;&#x70B9;&#x7684;&#x4FE1;&#x606F;</span></div><div class="line"><span class="keyword">class</span> ImageFrame</div><div class="line">{</div><div class="line">	<span class="built_in">map</span>&lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, Eigen::Matrix&lt;<span class="keyword">double</span>, 7, 1&gt;&gt; &gt; &gt; points;</div><div class="line">	<span class="keyword">double</span> t;</div><div class="line">	Matrix3d R;</div><div class="line">	Vector3d T;</div><div class="line">	IntegrationBase *pre_integration;</div><div class="line">	<span class="keyword">bool</span> is_key_frame;</div><div class="line">};</div><div class="line"></div><div class="line"><span class="built_in">vector</span>&lt;SFMFeature&gt; sfm_f;</div><div class="line"><span class="keyword">struct</span> SFMFeature</div><div class="line">{</div><div class="line">    <span class="keyword">bool</span> state;</div><div class="line">    <span class="keyword">int</span> id; <span class="comment">// &#x89D2;&#x70B9;&#x7684;id</span></div><div class="line">    <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>,Vector2d&gt;&gt; observation; <span class="comment">// &#x4FDD;&#x5B58;&#x4E86;&#x8FD9;&#x4E2A;&#x89D2;&#x70B9;&#x5728;&#x4E00;&#x4E9B;&#x5217;&#x56FE;&#x50CF;&#x4E2D;&#x7684;&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807;&#xFF0C;&#x4EE5;&#x53CA;&#x90A3;&#x4E9B;&#x56FE;&#x50CF;&#x5E27;&#x7684;id&#x3002;</span></div><div class="line">    <span class="keyword">double</span> position[<span class="number">3</span>]; <span class="comment">// &#x4FDD;&#x5B58;&#x89D2;&#x70B9;&#x7684;&#x4E09;&#x7EF4;&#x5750;&#x6807;</span></div><div class="line">    <span class="keyword">double</span> depth;</div><div class="line">};</div><div class="line"></div><div class="line"><span class="keyword">class</span> FeatureManager{</div><div class="line">	...</div><div class="line">    <span class="built_in">list</span>&lt;FeaturePerId&gt; feature; <span class="comment">// &#x901A;&#x8FC7;FeatureManager&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x6240;&#x6709;&#x7684;&#x89D2;&#x70B9;&#x4FE1;&#x606F;</span></div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">class</span> FeaturePerId{</div><div class="line">    <span class="comment">// &#x4EE5;feature_id&#x4E3A;&#x7D22;&#x5F15;&#xFF0C;&#x5E76;&#x4FDD;&#x5B58;&#x4E86;&#x51FA;&#x73B0;&#x8BE5;&#x89D2;&#x70B9;&#x7684;&#x7B2C;&#x4E00;&#x5E27;&#x7684;id&#xFF0C;</span></div><div class="line">	...</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> feature_id;</div><div class="line">    <span class="keyword">int</span> start_frame;</div><div class="line">    <span class="built_in">vector</span>&lt;FeaturePerFrame&gt; feature_per_frame;	</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">class</span> FeaturePerFrame{</div><div class="line">	<span class="comment">// &#x4FDD;&#x5B58;&#x4E86;&#x5F52;&#x4E00;&#x5316;&#x5750;&#x6807;&#xFF0C;&#x56FE;&#x50CF;&#x5750;&#x6807;&#x4EE5;&#x53CA;&#x6DF1;&#x5EA6;	</span></div><div class="line">	...</div><div class="line">    Vector3d point;</div><div class="line">    Vector2d uv;</div><div class="line">    <span class="keyword">double</span> z;</div><div class="line">}</div></pre></td></tr></table></figure></p>
<p>&#x7B80;&#x5355;&#x5730;&#x8BF4;&#xFF0C;&#x901A;&#x8FC7;<code>FeatureManager f_manager</code>&#xFF0C;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x67E5;&#x8BE2;&#x5F53;&#x524D;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x4E2D;&#x80FD;&#x591F;&#x89C2;&#x6D4B;&#x5230;&#x7684;&#x6240;&#x6709;&#x89D2;&#x70B9;&#xFF0C;&#x4EE5;&#x53CA;&#x8FD9;&#x4E9B;&#x89D2;&#x70B9;&#x88AB;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x54EA;&#x4E9B;&#x5E27;&#x89C2;&#x6D4B;&#x5230;&#x4E86;&#x3002;&#x5229;&#x7528;<code>FeatureManager f_manager</code>&#x4FDD;&#x5B58;&#x7684;&#x4FE1;&#x606F;&#xFF0C;&#x53EF;&#x4EE5;&#x5F97;&#x5230;<code>vector&lt;SFMFeature&gt; sfm_f</code>&#xFF0C;&#x4F5C;&#x4E3A;&#x540E;&#x7EED;&#x5355;&#x76EE;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6570;&#x636E;&#x5173;&#x8054;&#x3002;<br>&#x5728;&#x771F;&#x6B63;&#x8FDB;&#x884C;&#x521D;&#x59CB;&#x5316;&#x4E4B;&#x524D;&#xFF0C;&#x4F1A;&#x5BF9;&#x89C6;&#x5DEE;&#x8FDB;&#x884C;&#x68C0;&#x67E5;&#xFF0C;&#x4F9D;&#x8D56;&#x7684;&#x51FD;&#x6570;&#x7684;&#x662F;<code>bool Estimator::relativePose(Matrix3d &amp;relative_R, Vector3d &amp;relative_T, int &amp;l)</code>&#x3002;&#x5F53;&#x5E73;&#x5747;&#x89C6;&#x5DEE;&#x5927;&#x4E8E;30&#x5E76;&#x4E14;&#x901A;&#x8FC7;&#x57FA;&#x7840;&#x77E9;&#x9635;&#x6C42;&#x89E3;&#x5F97;&#x5230;&#x7684;&#x5185;&#x70B9;&#x6570;&#x76EE;&#x5927;&#x4E8E;12&#xFF0C;&#x53EF;&#x4EE5;&#x8BA4;&#x4E3A;&#x5F53;&#x524D;&#x7684;&#x89D2;&#x70B9;&#x5339;&#x914D;&#x8DB3;&#x591F;&#x652F;&#x6301;&#x5355;&#x76EE;&#x521D;&#x59CB;&#x5316;&#x3002;&#x4ECE;<code>Estimator::relativePose</code>&#x4E2D;&#x8FD8;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x5F53;&#x524D;&#x5E27;&#x548C;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x54EA;&#x4E00;&#x5E27;(&#x4FDD;&#x5B58;&#x5728;&#x53D8;&#x91CF;<code>l</code>&#x4E2D;)&#x662F;&#x89C6;&#x5DEE;&#x8DB3;&#x591F;&#x7684;&#xFF0C;&#x5E76;&#x8BA1;&#x7B97;&#x5F53;&#x524D;&#x5E27;&#x4E0E;&#x7B2C;<code>l</code>&#x5E27;&#x4E4B;&#x95F4;&#x7684;<code>R</code>&#x548C;<code>T</code>&#xFF0C;&#x8BA1;&#x7B97;&#x5F97;&#x5230;<code>R</code>&#x548C;<code>T</code>&#x5176;&#x5B9E;&#x5DF2;&#x7ECF;&#x5B8C;&#x6210;&#x4E86;&#x4E00;&#x90E8;&#x5206;&#x7684;&#x521D;&#x59CB;&#x5316;&#x3002;</p>
<p>&#x771F;&#x6B63;&#x7684;&#x5B8C;&#x6210;&#x5355;&#x76EE;&#x521D;&#x59CB;&#x5316;&#x5728;<code>bool GlobalSFM::construct(...)</code>&#x4E2D;&#xFF0C;&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x4E3B;&#x8981;&#x7ECF;&#x5386;&#x4E86;&#x5982;&#x4E0B;&#x51E0;&#x4E2A;&#x6B65;&#x9AA4;&#x6765;&#x5B8C;&#x6210;&#x521D;&#x59CB;&#x5316;&#x3002;<br>1.&#x901A;&#x8FC7;&#x5F97;&#x5230;&#x6709;&#x8DB3;&#x591F;&#x89C6;&#x5DEE;&#x7684;<code>l</code>&#x5E27;&#x548C;&#x4E0E;&#x5F53;&#x524D;&#x5E27;&#x4E4B;&#x95F4;&#x7684;<code>R</code>&#x548C;<code>T</code>&#x8FDB;&#x884C;&#x4E09;&#x89D2;&#x5316;&#x5F97;&#x5230;&#x5730;&#x56FE;&#x70B9;&#xFF0C;&#x5373;&#x8C03;&#x7528;<code>void GlobalSFM::triangulateTwoFrames(...)</code>&#x3002;<br>2.&#x6709;&#x4E86;&#x4E09;&#x89D2;&#x5316;&#x7684;&#x5730;&#x56FE;&#x70B9;&#x4E4B;&#x540E;&#xFF0C;&#x5BF9;&#x7B2C;<code>l</code>&#x5E27;&#x548C;&#x5F53;&#x524D;&#x5E27;&#x4E4B;&#x95F4;&#x6240;&#x6709;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x901A;&#x8FC7; pnp &#x6C42;&#x89E3;&#x5176;&#x4F4D;&#x59FF;&#xFF0C;&#x5373;&#x8C03;&#x7528;<code>bool GlobalSFM::solveFrameByPnP(...)</code>,&#x5F97;&#x5230;&#x4F4D;&#x59FF;&#x4E4B;&#x540E;&#xFF0C;&#x518D;&#x6B21;&#x8FDB;&#x884C;&#x4E09;&#x89D2;&#x5316;&#x5F97;&#x5230;&#x65B0;&#x7684;&#x5730;&#x56FE;&#x70B9;&#x3002;<br>3.&#x5229;&#x7528;&#x5DF2;&#x6709;&#x7684;&#x5730;&#x56FE;&#x70B9;&#xFF0C;&#x5BF9;<code>l</code>&#x4E4B;&#x524D;&#x7684;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x5E27;&#x8FDB;&#x884C; pnp &#x6C42;&#x89E3;&#x5176;&#x4F4D;&#x59FF;&#xFF0C;&#x7136;&#x540E;&#x518D;&#x6B21;&#x8FDB;&#x884C;&#x4E09;&#x89D2;&#x5316;&#x5F97;&#x5230;&#x65B0;&#x7684;&#x5730;&#x56FE;&#x70B9;&#x3002;<br>4.&#x5BF9;&#x4E8E;&#x5176;&#x4ED6;&#x6CA1;&#x6709;&#x88AB;&#x4E09;&#x89D2;&#x5316;&#x7684;&#x89D2;&#x70B9;&#xFF0C;&#x518D;&#x6B21;&#x8FDB;&#x884C;&#x4E09;&#x89D2;&#x5316;&#x5F97;&#x5230;&#x65B0;&#x7684;&#x5730;&#x56FE;&#x70B9;&#x3002;<br>5.&#x901A;&#x8FC7;&#x4E00;&#x6B21; full BA &#x6765;&#x5BF9;&#x5730;&#x56FE;&#x70B9;&#x548C;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x5173;&#x952E;&#x5E27;&#x4F4D;&#x59FF;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#x3002;&#x8FD9;&#x91CC;&#x7684; full BA &#x53EA;&#x5BF9;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x5173;&#x952E;&#x5E27;&#x4F4D;&#x59FF;&#x4F5C;&#x4F18;&#x5316;&#xFF0C;&#x800C;&#x4E0D;&#x4F18;&#x5316;&#x5730;&#x56FE;&#x70B9;&#x3002;VINS&#x5728;&#x5EFA;&#x7ACB;&#x91CD;&#x6295;&#x5F71;&#x8BEF;&#x5DEE;&#x7684;&#x65F6;&#x5019;&#x5F88;&#x6709;&#x610F;&#x601D;&#xFF0C;&#x5B83;&#x5E76;&#x4E0D;&#x5728;&#x56FE;&#x50CF;&#x5E73;&#x9762;&#x6C42;&#x50CF;&#x7D20;&#x8BEF;&#x5DEE;&#xFF0C;&#x800C;&#x662F;&#x5728;&#x5F52;&#x4E00;&#x5316;&#x5E73;&#x9762;&#x6C42;&#x8BEF;&#x5DEE;&#x3002;<br>6.&#x5728;&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;&#x7684;&#x6700;&#x540E;&#xFF0C;&#x518D;&#x6B21;&#x5BF9;&#x6240;&#x6709;&#x7684;&#x5E27;&#x6C42;&#x89E3;&#x4E00;&#x6B21; pnp&#x3002;&#x56E0;&#x4E3A;&#x524D;5&#x6B65;&#x53EA;&#x5F97;&#x5230;&#x4E86;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x6240;&#x6709;&#x5173;&#x952E;&#x5E27;&#x7684;&#x4F4D;&#x59FF;&#xFF0C;&#x4F46;&#x7531;&#x4E8E;&#x5E76;&#x4E0D;&#x662F;&#x7B2C;&#x4E00;&#x6B21;&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;&#x5C31;&#x80FD;&#x6210;&#x529F;&#xFF0C;&#x6B64;&#x65F6;&#x56FE;&#x50CF;&#x5E27;&#x6570;&#x76EE;&#x6709;&#x53EF;&#x80FD;&#x4F1A;&#x8D85;&#x8FC7;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x7684;&#x5927;&#x5C0F;(&#x6839;&#x636E;&#x56FE;&#x50CF;&#x5E27;&#x7684;&#x89C6;&#x5DEE;&#x5224;&#x65AD;&#x662F;&#x5426;&#x4E3A;&#x5173;&#x952E;&#x5E27;&#xFF0C;&#x7136;&#x540E;&#x9009;&#x62E9;&#x6ED1;&#x7A97;&#x7684;&#x7B56;&#x7565;)&#xFF0C;&#x6B64;&#x65F6;&#x8981;&#x5BF9;&#x90A3;&#x4E9B;&#x4E0D;&#x88AB;&#x5305;&#x62EC;&#x5728;&#x6ED1;&#x52A8;&#x7A97;&#x53E3;&#x5185;&#x7684;&#x56FE;&#x50CF;&#x5E27;&#x4F4D;&#x59FF;&#x8FDB;&#x884C;&#x6C42;&#x89E3;&#x3002;</p>
<p>&#x6B64;&#x65F6;&#x53EF;&#x4EE5;&#x8BF4;&#xFF0C;&#x6574;&#x4E2A;&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;&#x90E8;&#x5206;&#x5C31;&#x5B8C;&#x6210;&#x4E86;&#x3002;</p>
<h1 id="&#x89C6;&#x89C9;-IMU&#x5BF9;&#x9F50;"><a href="#&#x89C6;&#x89C9;-IMU&#x5BF9;&#x9F50;" class="headerlink" title="&#x89C6;&#x89C9;-IMU&#x5BF9;&#x9F50;"></a>&#x89C6;&#x89C9;-IMU&#x5BF9;&#x9F50;</h1><p>VIO&#x521D;&#x59CB;&#x5316;&#x7684;&#x6709;&#x4E24;&#x4E2A;&#x5F88;&#x91CD;&#x8981;&#x7684;&#x76EE;&#x7684;&#x3002;<strong>&#x9996;&#x5148;</strong>&#xFF0C;&#x901A;&#x8FC7; IMU &#x5F97;&#x5230;&#x7684;&#x89C2;&#x6D4B;&#x91CF;&#x662F;&#x5177;&#x5907;&#x7EDD;&#x5BF9;&#x5C3A;&#x5EA6;&#x7684;&#xFF0C;&#x800C;&#x5355;&#x76EE;&#x521D;&#x59CB;&#x5316;&#x7684;&#x7ED3;&#x679C;&#x662F;&#x4E0D;&#x5177;&#x5907;&#x7EDD;&#x5BF9;&#x5C3A;&#x5EA6;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x5C06; IMU &#x7684;&#x89C2;&#x6D4B;&#x503C;&#x4F5C;&#x4E3A;&#x89C2;&#x6D4B;&#x91CF;&#x52A0;&#x5165;&#x5230;&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;&#x7684;&#x7ED3;&#x679C;&#x4E2D;&#xFF0C;&#x53EF;&#x4EE5;&#x6062;&#x590D;&#x51FA;&#x89C6;&#x89C9;&#x521D;&#x59CB;&#x5316;&#x7F3A;&#x5931;&#x7684;&#x5C3A;&#x5EA6;&#x3002;<strong>&#x5176;&#x6B21;</strong>&#xFF0C;IMU&#x7684;&#x89C2;&#x6D4B;&#x7ED3;&#x679C;&#x662F;&#x5426;&#x51C6;&#x786E;&#xFF0C;&#x5728;&#x5F88;&#x5927;&#x7A0B;&#x5EA6;&#x4E0A;&#x4F9D;&#x8D56;&#x4E8E;&#x5BF9; IMU &#x52A0;&#x901F;&#x7D20;&#x548C;&#x89D2;&#x901F;&#x5EA6;&#x7684; bias &#x4F30;&#x8BA1;&#x662F;&#x5426;&#x51C6;&#x786E;&#xFF0C;&#x6B64;&#x65F6;&#x5C06;&#x89C6;&#x89C9;&#x7684;&#x89C2;&#x6D4B;&#x7ED3;&#x679C;&#x4F5C;&#x4E3A;&#x7EA6;&#x675F;&#x9879;&#x52A0;&#x5165;&#x5230; IMU &#x7684;&#x79EF;&#x5206;&#x8BA1;&#x7B97;&#x4E2D;&#xFF0C;&#x53EF;&#x4EE5;&#x5F97;&#x5230; bias &#x7684;&#x521D;&#x59CB;&#x4F30;&#x8BA1;&#x3002; VINS-Mono&#x5728;&#x5C06;&#x89C6;&#x89C9;&#x89C2;&#x6D4B;&#x548C; IMU &#x89C2;&#x6D4B;&#x5BF9;&#x9F50;&#x65F6;&#xFF0C;&#x4E00;&#x5171;&#x7ECF;&#x5386;&#x4E86;4&#x4E2A;&#x6B65;&#x9AA4;&#x3002;</p>
<p><strong>&#x7B2C;&#x4E00;&#x6B65;</strong>&#x662F;&#x89D2;&#x901F;&#x5EA6;(bias) $b_w$ &#x7684;&#x4F30;&#x8BA1;&#x3002;<br>&#x8FD9;&#x90E8;&#x5206;&#x7684;&#x4EE3;&#x7801;&#x5728;<code>void solveGyroscopeBias(map&lt;double, ImageFrame&gt; &amp;all_image_frame, Vector3d* Bgs)</code>&#x4E2D;&#x3002;<br>&#x5BF9;&#x4E8E;&#x8FDE;&#x7EED;&#x7684;&#x4E24;&#x5E27;&#x56FE;&#x50CF;&#xFF0C;&#x901A;&#x8FC7;&#x4E4B;&#x524D;&#x7684;&#x521D;&#x59CB;&#x5316;&#x5F97;&#x5230;&#x4E86;&#x5176;&#x5728;&#x4E16;&#x754C;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;&#x65CB;&#x8F6C; $q_{wi},q_{wi+1}$&#xFF0C;&#x901A;&#x8FC7; IMU &#x9884;&#x79EF;&#x5206;&#x5F97;&#x5230; $\delta q_{ii+1}$&#xFF0C;&#x56E0;&#x6B64;&#x6709;<br>$q_{wi}^T q_{wi+1} = \Delta q_{i,i+1} \otimes \begin{bmatrix}<br>1\\<br>\frac{1}{2} J^{\delta q^{i,i+1}}_{b_{wi}} \delta b_{wi}<br>\end{bmatrix} $&#x3002;&#x8054;&#x7ACB;&#x6240;&#x6709;&#x76F8;&#x90BB;&#x56FE;&#x50CF;&#x5E27;&#x5F97;&#x5230;&#x4E00;&#x7CFB;&#x5217;&#x7684;&#x65B9;&#x7A0B;&#x3002;&#x5047;&#x8BBE;&#x5728;&#x6574;&#x4E2A;&#x521D;&#x59CB;&#x5316;&#x671F;&#x95F4;&#x89D2;&#x901F;&#x5EA6;&#x7684; bias &#x662F;&#x56FA;&#x5B9A;&#x7684;&#xFF0C;&#x5219;&#x8FD9;&#x4E00;&#x7CFB;&#x5217;&#x65B9;&#x7A0B;&#x53EA;&#x6709; $\delta b_{w}$&#x662F;&#x672A;&#x77E5;&#x7684;&#x3002;&#x8054;&#x7ACB;&#x7684;&#x65B9;&#x7A0B;&#x53EF;&#x4EE5;&#x5199;&#x6210;&#x5173;&#x4E8E; $\delta b_{w}$ &#x7684;&#x8D85;&#x5B9A;&#x65B9;&#x7A0B;&#xFF0C;&#x901A;&#x8FC7;&#x77E9;&#x9635;&#x5206;&#x89E3;&#x7684;&#x4E00;&#x4E9B;&#x6280;&#x5DE7;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x6700;&#x5C0F;&#x4E8C;&#x4E58;&#x89E3;&#x3002;&#x5728; VINS-Mono &#x4E2D;&#x91C7;&#x7528;&#x7684;&#x662F;&#x901A;&#x8FC7; LDLT &#x5206;&#x89E3;&#x6C42;&#x89E3;$\delta b_{w}$&#x3002;&#x5F97;&#x5230; $\delta b_{w}$ &#x4E4B;&#x540E;&#xFF0C;&#x901A;&#x8FC7; $\delta b_{w}$ &#x91CD;&#x65B0;&#x8BA1;&#x7B97;&#x6240;&#x6709;&#x7684;&#x9884;&#x79EF;&#x5206;&#x91CF;&#xFF0C;&#x5373;&#x8C03;&#x7528;&#x51FD;&#x6570;<code>void IntegrationBase::repropagate(...)</code>&#x3002;</p>
<p><strong>&#x7B2C;&#x4E8C;&#x6B65;</strong>&#x662F;&#x5BF9;&#x6BCF;&#x4E00;&#x4E2A;&#x56FE;&#x50CF;&#x5E27;&#x7684;&#x901F;&#x5EA6;$V_i$&#xFF0C;&#x91CD;&#x529B;$g$&#x548C;&#x5C3A;&#x5EA6;$s$&#x8FDB;&#x884C;&#x4F30;&#x8BA1;&#x3002;<br>&#x9996;&#x5148;&#x89E3;&#x91CA;&#x4E00;&#x4E0B;&#x4E24;&#x4E2A;&#x516C;&#x5F0F;&#x3002;<br>&#x5728;VIORB&#x7684;&#x8BBA;&#x6587;&#x4E2D;&#x7ED9;&#x51FA;&#x4E86;&#x8FD9;&#x6837;&#x4E00;&#x4E2A;&#x516C;&#x5F0F;: $P_{wb} = s P_{wc} + R_{wc}P_{cb} \qquad (1)$<br>&#x5728;VINS-Mono&#x4E2D;&#x7ED9;&#x51FA;&#x7684;&#x516C;&#x5F0F;&#x662F;&#x8FD9;&#x6837;&#x7684;&#xFF1A;$sP_{wb} = sP_{wc} - R_{wb}P_{bc} \qquad (2)$<br>&#x4E4D;&#x4E00;&#x770B;&#x4E24;&#x4E2A;&#x516C;&#x5F0F;&#x4E0D;&#x4E00;&#x6837;&#xFF0C;&#x5176;&#x5B9E;&#x4ED4;&#x7EC6;&#x5206;&#x6790;&#x4E0B;&#x4E24;&#x4E2A;&#x516C;&#x5F0F;&#x662F;&#x7B49;&#x4EF7;&#x7684;&#x3002;&#x4E24;&#x4E2A;&#x516C;&#x5F0F;&#x7684;&#x5DE6;&#x8FB9;&#x90FD;&#x662F; IMU &#x5750;&#x6807;&#x7CFB;&#x5728;&#x4E16;&#x754C;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x7684;&#x4F4D;&#x59FF;&#xFF0C;&#x53EA;&#x662F;&#x5B9A;&#x4E49;&#x4E0D;&#x4E00;&#x6837;&#x3002;VIORB&#x4E2D;$P_{wb}$&#x662F;&#x5177;&#x5907;&#x5C3A;&#x5EA6;&#x7684;&#x4F4D;&#x59FF;&#xFF0C;&#x800C;&#x5728; VINS-Mono &#x4E2D;$P_{wb}$&#x662F;&#x4E0D;&#x5177;&#x5907;&#x5C3A;&#x5EA6;&#x7684;&#x4F4D;&#x59FF;&#x3002;<br>&#x800C;$P_{cb} = -R_{cb}P_{bc}$&#xFF0C;&#x56E0;&#x6B64;&#x6709;$ R_{wc}P_{cb} = -R_{wc}R_{cb}P_{bc} = -R_{wb}P_{bc}$&#x3002;&#x6240;&#x4EE5;&#x4E24;&#x4E2A;&#x516C;&#x5F0F;&#x5176;&#x5B9E;&#x662F;&#x5B8C;&#x5168;&#x7B49;&#x4EF7;&#x7684;&#x3002;</p>
<p>&#x6839;&#x636E;&#x4E4B;&#x524D;&#x5B66;&#x4E60;&#x7684; IMU &#x9884;&#x79EF;&#x5206;&#x7406;&#x8BBA;&#xFF0C;&#x6709;&#x5982;&#x4E0B;&#x4E24;&#x4E2A;&#x9884;&#x79EF;&#x5206;&#x7EA6;&#x675F;&#xFF1A;<br>$$<br>\delta V_{i,i+1} = R_{wbi}^T ( R_{wbi+1} V_{i+1} - R_{wbi} V_{i} + g\delta t ) \qquad (3)\\<br>\delta P_{i,i+1} = R_{wbi}^T ( P_{wbi+1} - P_{wbi} - R_{wbi} V_{i} \delta t + \frac{1}{2} g\delta t^2 ) \qquad (4)<br>$$</p>
<p>&#x628A;&#x5E26;&#x6709; $s$ &#x53D8;&#x91CF;&#x7684;&#x516C;&#x5F0F;(2)&#x4EE3;&#x5165;&#x516C;&#x5F0F;(4)&#x53EF;&#x5F97;&#xFF1A;<br>$$<br>\delta P_{k,k+1} = R_{wbk}^T ( s(P_{wck+1} - P_{wck}) - (R_{wbk+1}-R_{wbk})P_{bc} - R_{wbk} V_{k} \delta t + \frac{1}{2} g\delta t^2 ) \qquad (5)<br>$$</p>
<p>&#x8054;&#x7ACB;&#x516C;&#x5F0F;(3)&#x548C;&#x516C;&#x5F0F;(5)&#xFF0C;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x4E00;&#x7CFB;&#x5217;&#x7684;&#x65B9;&#x7A0B;&#x3002;<br>&#x5176;&#x4E2D;$R_{wbk} = R_{wck} R_{cb}$&#xFF0C;&#x56E0;&#x6B64;&#x65B9;&#x7A0B;&#x7684;&#x672A;&#x77E5;&#x9879;&#x4E3A;$V_k&#xFF0C;g &#x548C; s$&#x3002;&#x5728; VINS-Mono &#x5C06;&#x5176;&#x5199;&#x6210;&#x77E9;&#x9635;&#x5F62;&#x5F0F;&#xFF0C;&#x5176;&#x4E2D; $XI$ &#x8868;&#x793A;&#x672A;&#x77E5;&#x9879;&#x3002;&#x901A;&#x8FC7;&#x77E9;&#x9635;&#x5206;&#x89E3;&#x7684;&#x65B9;&#x6CD5;&#x6C42;&#x89E3; $H_{b_{k+1}}^{b^{k}} X_I = z_{b_{k+1}}^{b^{k}}$&#x3002;\</p>
<p><img src="/2018/04/19/SLAM/VINS/VINSInitialiaztion/1.png" width="30%" height="30%"><br><img src="/2018/04/19/SLAM/VINS/VINSInitialiaztion/2.png" width="40%" height="40%"><br><img src="/2018/04/19/SLAM/VINS/VINSInitialiaztion/3.png" width="50%" height="50%"></p>
<p><strong>&#x7B2C;&#x4E09;&#x6B65;</strong>&#x662F;&#x5BF9;&#x91CD;&#x529B; $g$ &#x8FDB;&#x884C;&#x4FEE;&#x6B63;&#x3002;</p>
<p>&#x7531;&#x4E8E;&#x7B2C;&#x4E8C;&#x6B65;&#x4E2D;&#x6C42;&#x89E3;&#x7684; $g$ &#x5728;&#x4E00;&#x5B9A;&#x7A0B;&#x5EA6;&#x4E0A;&#x662F;&#x6709;&#x8BEF;&#x5DEE;&#x7684;&#x3002;&#x800C;&#x4E00;&#x822C;&#x6765;&#x8BF4;&#xFF0C;&#x91CD;&#x529B;&#x5927;&#x5C0F;(magnitude)&#x662F;&#x5DF2;&#x77E5;&#xFF0C;&#x56E0;&#x6B64;&#x53EF;&#x4EE5;&#x6839;&#x636E;&#x8FD9;&#x4E2A;&#x5148;&#x9A8C;&#xFF0C;&#x5BF9; $g$ &#x8FDB;&#x884C;&#x8FDB;&#x4E00;&#x6B65;&#x7684;&#x4FEE;&#x6B63;&#x3002;<br>&#x5047;&#x8BBE;&#x91CD;&#x529B;&#x5927;&#x5C0F;&#x4E3A;G&#xFF0C;&#x5219;&#x6709; $g = G \cdot  \hat{g} + w_1b_1 + w_2b_2$&#xFF0C;&#x5176;&#x4E2D; $\hat{g}$  &#x662F;&#x6211;&#x4EEC;&#x5728;&#x7B2C;&#x4E09;&#x6B65;&#x4E2D;&#x6C42;&#x89E3;&#x5F97;&#x5230; $g$ &#x7684;&#x65B9;&#x5411;(&#x5355;&#x4F4D;&#x5411;&#x91CF;)&#xFF0C;$b_1$, $b_2$ &#x5728; $\hat{g}$ &#x7684;&#x6B63;&#x5207;&#x5E73;&#x9762;&#x4E0A;&#xFF0C;&#x5E76;&#x4E14;&#x6B63;&#x4EA4;&#x3002;&#x4E5F;&#x5C31;&#x662F;&#x8BF4;$b_1,b_2,\hat{g}$ &#x6784;&#x6210;&#x6807;&#x51C6;&#x6B63;&#x4EA4;&#x57FA;&#x3002;$b_1,b_2$ &#x7684;&#x8BBE;&#x7F6E;&#x5982;&#x4E0B;&#x56FE;&#x6240;&#x793A;<br><img src="/2018/04/19/SLAM/VINS/VINSInitialiaztion/4.png" width="60%" height="60%"></p>
<p>&#x628A; $g = G \cdot  \hat{g} + w_1b_1 + w_2b_2$ &#x4EE3;&#x5165;&#x5230;&#x516C;&#x5F0F;(2)&#x548C;&#x516C;&#x5F0F;(4)&#x4E2D;&#xFF0C;&#x6B64;&#x65F6;&#x4E0D;&#x518D;&#x6C42;&#x89E3; $g$&#xFF0C;&#x800C;$b_1,b_2$&#x662F;&#x5DF2;&#x77E5;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x53EA;&#x9700;&#x8981;&#x6C42;&#x89E3;$w_1,w_2$&#x3002;&#x5373;$X_I = [V_0,&#x2026;,V_n,w_1,w_2,s]$&#x3002;&#x4E5F;&#x5C31;&#x662F;&#x8BF4;&#x5BF9;&#x91CD;&#x529B; $g$ &#x8FDB;&#x884C;&#x4FEE;&#x6B63;&#x7684;&#x540C;&#x65F6;&#xFF0C;&#x4E5F;&#x4F1A;&#x5BF9; $V_i$ &#x548C; $s$ &#x8FDB;&#x884C;&#x8C03;&#x6574;&#x3002;<br>&#x6B64;&#x65F6; $z_{b_{k+1}}^{b_k}$ &#x7684;&#x7B2C;&#x4E00;&#x9879;&#x8981;&#x51CF;&#x53BB; $\frac{1}{2} R_{c_0}^{b_k} G \cdot  \hat{g} \delta t^2$&#xFF0C;&#x7B2C;&#x4E8C;&#x9879;&#x8981;&#x51CF;&#x53BB; $R_{c_0}^{b_k} G \cdot  \hat{g} \delta t$&#xFF0C;&#x5176;&#x4ED6;&#x7684;&#x548C;&#x7B2C;&#x4E8C;&#x6B65;&#x4FDD;&#x6301;&#x4E00;&#x81F4;&#x3002;<br>&#x8FD9;&#x90E8;&#x5206;&#x7684;&#x4EE3;&#x7801;&#x5728;<code>void RefineGravity(...)</code>&#x3002;&#x4E3A;&#x4E86;&#x4F7F;&#x5F97; g &#x7684;&#x503C;&#x53EF;&#x4EE5;&#x6536;&#x655B;&#xFF0C;<code>void RefineGravity(...)</code>&#x4E2D;&#x5BF9; $H_{b_{k+1}}^{b^{k}} X_I = z_{b_{k+1}}^{b^{k}}$ &#x65B9;&#x7A0B;&#x8FED;&#x4EE3;&#x6C42;&#x89E3;4&#x6B21;&#x3002;</p>
<blockquote>
<p>&#x5176;&#x5B9E;&#x5BF9;&#x8FD9;&#x4E00;&#x6B65;&#x4FEE;&#x6B63; g &#x7684;&#x539F;&#x7406;&#x6709;&#x4E00;&#x70B9;&#x8FF7;&#x3002;&#x7B2C;&#x4E8C;&#x6B65;&#x6C42;&#x89E3;&#x7684; $g$ &#x7684;&#x5927;&#x5C0F;(magnitude)&#x4E0E;&#x5B9E;&#x9645;&#x7684;&#x91CD;&#x529B;&#x5927;&#x5C0F;&#x4E0D;&#x4E00;&#x81F4;&#xFF0C;&#x9664;&#x4E86;&#x6709;&#x566A;&#x58F0;&#x7684;&#x5F71;&#x54CD;&#x5916;&#xFF0C;&#x52A0;&#x901F;&#x5EA6;&#x8BA1;&#x7684; bias &#x7684;&#x5F71;&#x54CD;&#x5E94;&#x8BE5;&#x4E5F;&#x662F;&#x5B58;&#x5728;&#x7684;&#x3002;&#x8FD9;&#x91CC;&#x7684;&#x4FEE;&#x6B63;&#x611F;&#x89C9;&#x4E0A;&#x662F;&#x901A;&#x8FC7;&#x627E;&#x51FA;&#x4FEE;&#x6B63;&#x91CD;&#x529B;&#x65B9;&#x5411;&#x7684; $w_1b_1$ &#x548C; $w_2b_2$&#x3002;$G \cdot  \hat{g}$  &#x5F3A;&#x884C;&#x5C06; $g$ &#x7684;&#x5927;&#x5C0F;&#x62C9;&#x626F;&#x5230;&#x5B9E;&#x9645;&#x7684;&#x91CD;&#x529B;&#x5927;&#x5C0F;&#xFF0C;&#x7136;&#x540E;&#x53E6; $g = G \cdot  \hat{g} + w_1b_1 + w_2b_2$&#xFF0C;&#x4E5F;&#x5C31;&#x662F;&#x8BF4;&#x7528; $w_1b_1+w_2b_2$ &#x4EE3;&#x66FF;&#x566A;&#x58F0;&#x548C;&#x52A0;&#x901F;&#x5EA6;&#x8BA1;&#x7684; bias &#x5BF9; $g$ &#x7684;&#x5F71;&#x54CD;&#x3002;&#x6C42;&#x89E3;&#x51FA; $w_1,w_2$ &#x4E4B;&#x540E;&#xFF0C;&#x6700;&#x540E;&#x5F97;&#x5230;&#x7684;&#x91CD;&#x529B;&#x7684;&#x65B9;&#x5411;&#x4E3A; $g = G \cdot  \hat{g} + w_1b_1 + w_2b_2$ &#x7684;&#x5355;&#x4F4D;&#x65B9;&#x5411;&#x5411;&#x91CF;&#x3002;&#x5F88;&#x8FF7;&#x5F88;&#x8FF7;&#x2026;</p>
</blockquote>
<p><strong>&#x7B2C;&#x56DB;&#x6B65;</strong>&#x5B8C;&#x6210;&#x521D;&#x59CB;&#x5316;<br>&#x5F97;&#x5230;&#x91CD;&#x529B; $g$ &#x4E4B;&#x540E;&#xFF0C;&#x5373;&#x53EF;&#x6839;&#x636E;&#x5176;&#x4E0E;&#x65B9;&#x5411;&#x5411;&#x91CF;[0,0,1]&#x4E4B;&#x95F4;&#x7684;&#x65CB;&#x8F6C;&#xFF0C;&#x5F97;&#x5230;&#x4E16;&#x754C;&#x5750;&#x6807;&#x7CFB;&#x4E0E;&#x7B2C;&#x4E00;&#x5E27;&#x76F8;&#x673A;&#x5750;&#x6807;&#x7CFB;&#x4E4B;&#x95F4;&#x65CB;&#x8F6C;&#xFF0C;&#x4ECE;&#x800C;&#x53EF;&#x4EE5;&#x5C06;&#x6240;&#x6709;&#x56FE;&#x50CF;&#x5E27;&#x7684;&#x65CB;&#x8F6C;&#x8F6C;&#x6362;&#x5230;&#x4E16;&#x754C;&#x5750;&#x6807;&#x7CFB;&#x4E0B;&#x3002;&#x5176;&#x6B21;&#x6839;&#x636E;&#x5F97;&#x5230;&#x7684;&#x5C3A;&#x5EA6;$s$&#xFF0C;&#x5BF9;&#x5730;&#x56FE;&#x70B9;&#x5750;&#x6807;&#x548C;&#x5E73;&#x79FB;&#x8FDB;&#x884C;&#x7F29;&#x653E;&#x3002;&#x8FD9;&#x90E8;&#x5206;&#x4EE3;&#x7801;&#x5728;<code>void visualInitialAlign()</code>&#x51FD;&#x6570;&#x7684;&#x540E;&#x534A;&#x90E8;&#x5206;&#x3002;</p>
<h1 id="&#x603B;&#x7ED3;"><a href="#&#x603B;&#x7ED3;" class="headerlink" title="&#x603B;&#x7ED3;"></a>&#x603B;&#x7ED3;</h1><p>&#x8C1C;&#x4E00;&#x6837;&#x7684;&#x91CD;&#x529B;&#x4FEE;&#x6B63;&#x6C42;&#x89E3;&#x7B54;&#x3002;&gt; - &lt;</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 源码学习 </tag>
            
            <tag> VINS-Mono </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[VINS-Mono中的Feature Tracker]]></title>
      <url>https://zhehangt.github.io/2018/04/18/SLAM/VINS/VINSFeatureTracker/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>VINS-Mono中采用光流法作为视觉前端，从实验效果上来看，追踪的成功率非常的高，虽然牺牲了一定的精确度，但不会像ORB-SLAM那样容易丢失。本篇博客将对VINS-Mono的前端代码做简单的梳理。</p>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>VINS-Mono以 ROS 作为代码开发的基础，与前端相关的代码都在feature_tracker目录下。最主要的流程就是通过订阅图像的Topic，获取图像后进行处理。处理得到了图像与图像之间的匹配关系，然后通过消息发布。这里涉及到一个很重要的数据结构就是<code>sensor_msgs::PointCloud</code>。其定义如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">std_msgs/Header header</div><div class="line">geometry_msgs/Point32[] points</div><div class="line">sensor_msgs/ChannelFloat32[] channels</div></pre></td></tr></table></figure></p>
<p>在points中存储了所有当前图像中追踪到的角点的图像归一化坐标。<br>在channels中存储了关于该角点的相关信息，这里共存储了5种信息。<br>1.角点的id<br>2.角点像素坐标的横坐标<br>3.角点像素坐标的纵坐标<br>4.角点在x方向的速度<br>5.角点在y方向的速度<br><code>sensor_msgs::PointCloud</code>中所有的信息都是在<code>void img_callback(const sensor_msgs::ImageConstPtr &amp;img_msg)</code>中处理得到的，实际依赖的是<code>FeatureTracker</code>类，处理图像的入口是<code>void FeatureTracker::readImage(const cv::Mat &amp;_img, double _cur_time)</code>。<code>FeatureTracker</code>类中几个比较重要的数据结构，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">camodocal::CameraPtr m_camera; //相机模型，保存了相机的内参和相机投影方程</div><div class="line"></div><div class="line">cv::Mat prev_img, cur_img, forw_img; //原始图像数据。prev_img好像没啥用，cur_img保存了上一帧图像，forw_img保存了当前帧。</div><div class="line">vector&lt;cv::Point2f&gt; prev_pts, cur_pts, forw_pts; //图像中的角点坐标</div><div class="line">vector&lt;int&gt; track_cnt; //保存了当前追踪到的角点一共被多少帧图像追踪到</div><div class="line">vector&lt;int&gt; ids; //保存了当前追踪到的角点的ID，这个ID非常关键，保存了帧与帧之间角点的匹配关系。</div></pre></td></tr></table></figure></p>
<h1 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h1><p>整个视觉前端最关键的代码就在<code>void FeatureTracker::readImage(const cv::Mat &amp;_img, double _cur_time)</code>中。<br>上一帧的图像和图像中的角点分别保存在<code>cur_img</code>和<code>cur_pts</code>中，利用<code>cv::calcOpticalFlowPyrLK(cur_img, forw_img, cur_pts, forw_pts, status, err, cv::Size(21, 21), 3);</code>函数可以得到当前图像中能通过光流追踪到的角点的坐标，保存在<code>forw_pts</code>中。<br>由于前一帧保存的角点并不一定全部能够被当前帧追踪到，追踪的成功与否保存在<code>status</code>中。因此利用<code>status</code>和<code>void reduceVector(vector&lt;cv::Point2f&gt; &amp;v, vector&lt;uchar&gt; status)</code>函数对<code>track_cnt，ids</code>等进行更新。<br>经过更新之后<code>track_cnt</code>里面保存都是能够追踪到的角点的追踪次数，执行+1操作。</p>
<p>由于光流匹配会导致追踪到的角点越来与少，因此需要根据<code>MAX_CNT</code>和在当前帧中追踪到的角点数目差，进行新的角点的提取。依赖的函数为<code>cv::goodFeaturesToTrack(forw_img, n_pts, MAX_CNT - forw_pts.size(), 0.01, MIN_DIST, mask);</code>新提取的角点坐标保存在<code>n_pts</code>中。值得注意的是这里有个参数<code>MIN_DIST</code>，这个参数保证2个相邻角点之间的最小距离，通过这个参数可以在一定程度上保证角点的均匀分布。<br>这里<code>mask</code>的设置也是有讲究的，因为在<code>forw_img</code>中已经通过光流追踪到了一部分角点，当再次从<code>forw_img</code>中提取角点的时候，通过设置相应的<code>mask</code>,来保证角点的提取不会重复。设置<code>mask</code>是通过<code>void FeatureTracker::setMask()</code>实现的。<code>void FeatureTracker::setMask()</code>里面有个很有趣的操作就是会根据角点被追踪的次数进行排序，即<code>track_cnt, ids, forw_pts</code>都是按照角点被追踪的次数排序的。<br>之后会把新追踪到的角点<code>n_pts</code>加入到<code>forw_pts</code>和<code>ids</code>中去。<br>最后调用<code>void FeatureTracker::undistortedPoints()</code>对角点图像坐标做去畸变处理，并计算每个角点的速度。</p>
<p>至此，所有<code>sensor_msgs::PointCloud</code>里面的数据都已经计算得到了。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>Feature Tracker其实仅仅做了视觉前端的数据关联，只是视觉前端的第一步。与ORB通过描述子进行关联匹配不同的是，VINS采用了KLT光流的方式对角点进行关联匹配，并通过id的方式记录关联的结果。总的来说，速度快，抗干扰强，但牺牲了一定的精确度。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 源码学习 </tag>
            
            <tag> VINS-Mono </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ORB_SLAM2中的回环检测]]></title>
      <url>https://zhehangt.github.io/2018/04/11/SLAM/ORBSLAM/ORBSLAM2LoopClosing/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>ORB_SLAM2 中的回环检测是相对独立的模块，主要采用了词袋模型来进行检测回环，通过建立当前帧和回环帧之间的匹配关系，来修正视觉里程计的局部误差。</p>
<h1 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h1><p>词袋是通过离线的方式建立的。ORB_SLAM2 中的词袋是基于ORB特征来生成的，主要通过以下几个步骤。<br>1、采集与目标场景相类似，并且图像具有多样性的图像数据集，并提取所有图像的ORB特征描述子。<br>2、将抽取的特征描述子用 k-means++ 算法聚类，将描述子划分成 k 类。<br>3、将划分得到的k类特征描述子，继续利用 k-means++ 算法做聚类<br>4、按照上述循环n，最终可以得到 $k^n$ 种不同的ORB描述子，即 $k^n$个word。<br>自此词袋就建立完成。不难发现，通过这种方式建立的词袋是k叉树的结构 ，当进行描述子和word之间的匹配时，可以通过分层的方式进行搜索匹配，可以极大的提升搜索速度。<br>word本身是一个描述子，这个描述子是通过前面所说的聚类的方式得到的，因此可以认为是某一类相似的描述子的平均表达。word除了描述子之外，还会计算WordValue值。直观的讲，这个WordValue描述了这个word到底重不重要，WordValue值越大，表示这个word对于图像的区分越明显。WordValue最常用的计算方式为IDF-TF，这部分内容在十四讲里面有提及，不再详述。</p>
<h1 id="图像帧的词袋表达"><a href="#图像帧的词袋表达" class="headerlink" title="图像帧的词袋表达"></a>图像帧的词袋表达</h1><p>图像帧的词袋表达依赖于两个数据结构：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">DBoW2::BowVector mBowVec;</div><div class="line">DBoW2::FeatureVector mFeatVec;</div></pre></td></tr></table></figure></p>
<p><code>DBoW2::BowVector mBowVec</code>是从<code>std::map&lt;WordId, WordValue&gt;</code>继承的，这里的<code>WordId</code>的取值范围即为生成词袋模型时word的数目($k^n$)。这里存储的<code>WordValue</code>是 word 的 WordValue的累加值，当图像中的多个特征都与WordId对应的word相似，这里存储的<code>WordValue</code>是对 word 中的 WordValue的多次累加。<br><code>DBoW2::FeatureVector mFeatVec</code>是被称为 direct index 的东西，用来支持图像与图像之间特征匹配的速度。其从<code>std::map&lt;NodeId, std::vector&lt;unsigned int&gt; &gt;</code>继承，<code>NodeId</code>是词袋树中某一层的节点的id，在ORB_SLAM2中是倒数第四层，<code>std::vector&lt;unsigned int&gt;</code>保存的是与该节点下的 word 匹配的特征点的id。</p>
<h1 id="DetectLoop"><a href="#DetectLoop" class="headerlink" title="DetectLoop()"></a>DetectLoop()</h1><p>ORB_SLAM2中回环检测的相关代码主要在<code>LoopClosing.cc</code>文件中，在ORB_SLAM2的运行过程中，<code>LoopClosing</code>运行在单独的一个线程中。<br>对于每一个关键帧，通过<code>DetectLoop()</code>函数进行是不是存在回环的判断。主要经过了如下几个步骤。<br>1、计算当前帧与其关联视图中的关键帧之间图像词袋相似度，取其中的最小值，检测回环帧时，其与当前帧的相似度要大于这个最小值。<br>2、通过<code>KeyFrameDatabase::DetectLoopCandidates()</code>函数获取回环候选帧。为了加快候选帧的检索，这里采用了一种 inverse index 的技巧，依赖于<code>std::vector&lt;list&lt;KeyFrame*&gt;&gt; mvInvertedFile</code>。这个vector的大小为word的数目($k^n$)，下标为i的地方存储的是一系列关键帧，这些关键帧中包含 WordId=i 的 word。这期间还会统计候选帧与当前帧之间有多少个相同的word，存储于KeyFrame类的mnLoopWords成员变量中。候选帧保存在list<keyframe*> lKFsSharingWords中。之后会对lKFsSharingWords作两次剔除。首先是保留mnLoopWords大于其最大值的80%的候选帧。其次是计算候选帧所有关联视图中相邻帧与当前帧mnLoopWords的累加和，保留累加和大于最大值75%的候选帧。<br>3、对从<code>KeyFrameDatabase::DetectLoopCandidates()</code>得到的候选帧<code>vpCandidateKFs</code>进一步进行一致性检验。对于<code>vpCandidateKFs</code>里面的每一个关键帧，作为当前关键帧。我们找出其关联视图中的关键帧们组成一个当前整体<code>spCandidateGroup</code>。如果当前关键帧是第一次检测到回环，直接把这个<code>spCandidateGroup</code>整体，以分数0直接放到<code>mvConsistentGroups</code>中。如果不是第一次检测到回环，就从<code>mvConsistentGroups</code>中依次取出里面的元素pair<set<keyframe*>,int&gt;的first(sPreviousGroup)，即为之前的<code>spCandidateGroup</code>。只要是当前整体中的任意一个关键帧能在以前整体里面找到，就要将当前整体的得分加1，并把当前整体放到mvConsistentGroups里面。如果当前整体的得分大于3（mnCovisibilityConsistencyTh）了的话，当前帧就通过了一致性检测，把当前帧放到<code>mvpEnoughConsistentCandidates</code>。如果<code>mvpEnoughConsistentCandidates</code>不为空，则检测到回环。</set<keyframe*></keyframe*></p>
<h1 id="ComputeSim3"><a href="#ComputeSim3" class="headerlink" title="ComputeSim3()"></a>ComputeSim3()</h1><p>检测到回环帧之后，开始调用<code>ComputeSim3()</code>函数计算当前帧和回环帧之间的平移和旋转。主要经历了如下几个步骤。<br>1、基于回环帧和当前帧的词袋，通过<code>matcher.SearchByBoW()</code>寻找回环帧和当前帧之间的ORB特征匹配。注意因为这里可能有不止一个候选帧，对于每个候选帧都会与当前帧进行特征匹配。<br>2、对于步骤1中的每一对特征匹配，都会构建一个Sim3问题求解，计算出当前帧和回环帧之间的平移和旋转，这里会通过RANSACS去剔除一些异常的回环候选帧。<br>3、当通过Sim得到一个初始的平移和旋转之后，会通过<code>matcher.SearchBySim3()</code>来寻找更多的特征匹配。主要的思路就是，对于<code>matcher.SearchByBoW()</code>中没有被匹配的地图点，分别投影到回环帧和当前帧中，去搜索地图点和特征点之间的匹配，最后对匹配进行验证。<br>4、有了前三步的异常点剔除和特征点匹配，利用重投影误差构造sim3优化问题，通过<code>Optimizer::OptimizeSim3()</code>优化，得到更准确的平移和旋转。<br>5、最后把回环帧和其关联视图中的关键帧们中的所有地图点投影到当前帧搜索特征匹配，如果匹配的数目大于40，则回环被接受。</p>
<h1 id="CorrectLoop"><a href="#CorrectLoop" class="headerlink" title="CorrectLoop()"></a>CorrectLoop()</h1><p>1、通过上一步的<code>ComputeSim3()</code>，可以对当前帧的位姿进行调整。利用之前已知的两帧之间的位姿关系，就可以对所有的位姿进行调整。<br>2、之后利用调整过的位姿更新这些相连关键帧对应的地图点。<br>3、对回环帧和当前帧中的地图点进行融合，并将融合后的地图点重新投影到回环帧和当前帧以及关联视图中的其他帧中，建立新的匹配关系。<br>4、根据新的匹配关系，通过<code>Optimizer::OptimizeEssentialGraph</code>进行全局位姿图的优化。最后利用<code>RunGlobalBundleAdjustment</code>进行全局的BA优化。</p>
<p>至此，整个回环优化完成。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>回环优化这一块从代码上看还是比较复杂，这部分代码还需要进一步深入阅读。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ORB-SLAM </tag>
            
            <tag> 源码学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ORB_SLAM2中的ORB特征提取与匹配]]></title>
      <url>https://zhehangt.github.io/2018/01/11/SLAM/ORBSLAM/ORBSLAM2ORBExtractor/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>最近在尝试将 VINS 的光流匹配替换为 ORBSLAM 中的 ORB 描述子匹配，看看能不能减少 VINS 在没有回环的情况下的定位精确度。以前只大致知道ORB特征提取和特征匹配理论过程，现在将 ORBSLAM 中的相关代码学习了一遍，本篇博客将作简单记录。</p>
<h1 id="ORB特征"><a href="#ORB特征" class="headerlink" title="ORB特征"></a>ORB特征</h1><p>首先简单回顾一下什么是 ORB 特征。ORB 特征主要由两部分组成，即<strong>关键点</strong>和<strong>描述子</strong>。<strong>关键点</strong>为 ORB 特征在图像中出现的图像坐标，ORB 特征中的关键点称为“Oriented Fast”，是一种改进的角点。<strong>描述子</strong>则为该角点的二进制描述，称为BRIEF特征描述。对于三维空间中的某个点，即使出现在多张图片中且关键点的像素坐标互不相同，其描述子应该是相似的。有了描述子就可以进行图像之间的特征匹配，其方法就是计算描述子之间的汉明距离，汉明距离越小，两个 ORB 特征就越相似。<br>判断<em>关键点</em>的条件以及描述子的计算可以参考《视觉SLAM十四讲》的第七讲。之前的博文<a href="http://zhehangt.win/2017/03/03/SLAM/FeatureMatching/" target="_blank" rel="external">《图像特征匹配 》</a>中也有简单提及。<br>有了这些基础理论，接下来可以进一步学习ORB_SLAM中是如何进行 ORB 特征提取和图像之间的特征匹配的。</p>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>ORB_SLAM2 中与 ORB 特征提取和特征匹配相关的数据结构主要就是 Frame类 、 KeyFrame类 和 MapPoint类。Frame类 和 KeyFrame类  ORB_SLAM 中基本上代替了原始图像，当通过原始图像构造了一个 Frame 或者 KeyFrame 之后， 原始图像就被丢弃了。之后所有的 Tracking，Mapping 和 Loop Closing 都是通过 Frame 或者 KeyFrame 来完成的。对于每个原始图像，首先是将其构造成 Frame类，而构造成 Frame类 的过程中，最重要的一步就是提取 ORB 特征。<br>Frame类 中与提取 ORB 特征最紧密相关的几个成员变量如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//ORB特征提取器</span></div><div class="line">ORBextractor*  mpORBextractorLeft</div><div class="line"></div><div class="line"><span class="comment">//原始关键点图像坐标</span></div><div class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::KeyPoint&gt;  mvKeys;</div><div class="line"><span class="comment">//经过矫正模型矫正的关键点图像坐标</span></div><div class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::KeyPoint&gt;  mvKeysUn;</div><div class="line"></div><div class="line"><span class="comment">//存储ORB描述子，在矩阵的第i行存储的是上面vector中第i个关键点的描述子</span></div><div class="line">cv::Mat mDescriptors;</div></pre></td></tr></table></figure>
<p>KeyFrame类 中与 ORB 特征相关的成员变量与 Frame类 中类似。</p>
<p>而每一个MapPoint中同样会存储一个ORB 描述子。之前我们都是直接在帧与帧之间建立特征匹配，而通过MapPoint中的 ORB 描述子，我们还可以直接建立 MapPoint 和 Frame 之间的关联关系。<br>这个描述子被称为该 MapPoint 的最具有代表性的 ORB 描述子。其计算的基本思路是，在所有能观测到该 MapPoint 的关键帧的描述子中，选择其中一个描述子，使其与剩余的描述子之间的汉明距离之和最小。<br>这部分的实现代码为 MapPoint类 的 <code>void ComputeDistinctiveDescriptors();</code> 函数。</p>
<h1 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h1><p>ORB_SLAM 中关于特征提取的相关实现在 ORBextractor类 中。 当每一帧的图像数据被送到 Tracking类 进行追踪时，都会将图像数据构造成 Frame类，然后通过 Tracking类 中构造的 <code>ORBextractor* mpORBextractorLeft;</code>完成特征提取，特征提取的结果被保存在 Frame类中的成员变量<code>std::vector&lt;cv::KeyPoint&gt; mvKeys;</code> 和 <code>cv::Mat mDescriptors;</code> 中。</p>
<p>首先来看一看 ORBextractor类 的构造函数。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//nfeatures表示要在当前图像中提取的ORB特征点的数目</span></div><div class="line"></div><div class="line"><span class="comment">//为了使ORB特征具备尺度一致性，通过采样多个层级的图像金字塔进行ORB特征提取，</span></div><div class="line"><span class="comment">//scaleFactor表示相邻两层之间ORB特征点数目的倍数，nlevels表示图像金子塔的层数</span></div><div class="line"><span class="comment">//参考TUM1.yaml文件中的参数，每一帧图像共提取1000个特征点，分布在金字塔8层中，层间尺度比例1.2</span></div><div class="line"><span class="comment">//计算下来金字塔0层大约有217个特征点，7层大约有50个特征点</span></div><div class="line"></div><div class="line"><span class="comment">//在提取FAST角点时，会把中心像素和周围像素之间的亮度差作为判断标准。</span></div><div class="line"><span class="comment">//iniThFAST表示这个 threshold 的初始值，当提取的角点数目不够时，则会采用 minThFAST 作为 threshold。</span></div><div class="line">ORBextractor(<span class="keyword">int</span> nfeatures, <span class="keyword">float</span> scaleFactor, <span class="keyword">int</span> nlevels,  <span class="keyword">int</span> iniThFAST, <span class="keyword">int</span> minThFAST);</div></pre></td></tr></table></figure></p>
<p>在构造函数中主要完成了以下几个事情。<br>首先根据 scaleFactor 和 nlevels 计算每一层图像金字塔应该提取的 ORB特征 数目。保存在 <code>std::vector&lt;int&gt; mnFeaturesPerLevel;</code><br>其次是构造了一个<code>std::vector&lt;cv::Point&gt; pattern;</code>成员变量。这个成员变量将用于描述子的计算。<br>最后是构造了一个<code>std::vector&lt;int&gt; umax;</code>成员变量。这个成员变量将用于 ORB特征 的方向向量。</p>
<p>在 ORBextractor类 中扮演的最重要的角色就是重载了( )操作符 <code>void operator()( cv::InputArray image, cv::InputArray mask, std::vector&lt;cv::KeyPoint&gt;&amp; keypoints, cv::OutputArray descriptors);</code>。在将图像数据构造成 Frame类时，即在Frame类的构造函数内调用了 ORBextractor类 的( )操作符 <code>(*mpORBextractorLeft)(im, cv::Mat(), mvKeys, mDescriptors);</code>，从而完成对当前图像的 ORB特征 的提取。<br>下面我们将着重分析这个操作符函数。</p>
<p>在这个操作符函数中，首先进行的是对图像进行金字塔降采样，其调用了函数<code>void ComputePyramid(cv::Mat image);</code>。其降采样过程主要是根据之前 scaleFactor，计算每一层金字塔图像的分辨率大小，然后通过<code>void resize( InputArray src, OutputArray dst, Size dsize, double fx = 0, double fy = 0, int interpolation = INTER_LINEAR );</code>得到新的图像，存储在成员变量<code>std::vector&lt;cv::Mat&gt; mvImagePyramid;</code>。</p>
<p>完成图像金字塔降采样之后，将调用函数<code>void ComputeKeyPointsOctTree(std::vector&lt;std::vector&lt;cv::KeyPoint&gt; &gt;&amp; allKeypoints);</code>计算每一层金字塔图像的关键点坐标，存储在局部变量 <code>vector &lt; vector&lt;KeyPoint&gt; &gt; allKeypoints;</code>中。allKeypoints 的第一维表示金字塔的层数，第二维表示在该层上提取的各个关键点坐标。得到 allKeypoints 的计算过程如下：<br>1）将图像划分为像素大小为 W=30 的网格。在每个网格上调用函数 <code>void FAST( InputArray image, CV_OUT std::vector&lt;KeyPoint&gt;&amp; keypoints, int threshold, bool nonmaxSuppression=true );</code>进行关键点坐标的提取。这些关键点保存在局部变量<code>vector&lt;cv::KeyPoint&gt; vToDistributeKeys;</code><br>2）由于在网格中提取的关键点很大程度上也是’扎堆‘出现的，因此在一定区域内仅保留响应极大值的角点，避免角点集中的问题。这个过程是通过将所有当前提取的关键点分配到平面四叉树中去实现的，即函数<code>std::vector&lt;cv::KeyPoint&gt; DistributeOctTree( ... );</code>。将所有的角点根据空间关系分配到一定数目的四叉树节点中去，然后取每个节点上的最大响应点。<br>3）最后将计算每个角点的方向信息。该过程将对每个角点调用<code>float IC_Angle(const Mat&amp; image, Point2f pt,  const vector&lt;int&gt; &amp; u_max)</code>函数。</p>
<p>有了关键点坐标，最后就是计算描述子。对于每个关键点，都调用了<code>void computeOrbDescriptor(const KeyPoint&amp; kpt, const Mat&amp; img, const Point* pattern, uchar* desc)</code>函数，每个描述子都是一个32位的 unchar 类型的字符串。</p>
<p>至此，整个特征提取过程就完成。</p>
<h1 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h1><p>ORB_SLAM 的特征匹配是通过 ORBmatcher类 完成的。在 ORBmatcher类 中，提供了多种寻找特征匹配方式。简单看一下 ORBmatcher类 的定义。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//寻找当前帧和地图点之间的匹配，用于TrackLocalMap()</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchByProjection</span><span class="params">(Frame &amp;F, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMapPoints, <span class="keyword">const</span> <span class="keyword">float</span> th=<span class="number">3</span>)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">//寻找当前帧和前一帧之间的匹配，用于TrackWithMotionModel()</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchByProjection</span><span class="params">(Frame &amp;CurrentFrame, <span class="keyword">const</span> Frame &amp;LastFrame, <span class="keyword">const</span> <span class="keyword">float</span> th, <span class="keyword">const</span> <span class="keyword">bool</span> bMono)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">//寻找当前帧和关键帧之间的匹配，用于检测到回环时，与候选的回环帧之间建立匹配</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchByProjection</span><span class="params">(Frame &amp;CurrentFrame, KeyFrame* pKF, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">set</span>&lt;MapPoint*&gt; &amp;sAlreadyFound, <span class="keyword">const</span> <span class="keyword">float</span> th, <span class="keyword">const</span> <span class="keyword">int</span> ORBdist)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">//当计算出当前帧和回环帧之间的sim(3)变换之后，建立当前帧和地图点之间的匹配</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchByProjection</span><span class="params">(KeyFrame* pKF, cv::Mat Scw, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpPoints, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMatched, <span class="keyword">int</span> th)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">//用于回环检测时当前帧</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchByBoW</span><span class="params">(KeyFrame *pKF, Frame &amp;F, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;MapPoint*&gt; &amp;vpMapPointMatches)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">//用于初始化时，寻找当前帧和前一帧之间的匹配</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchForInitialization</span><span class="params">(Frame &amp;F1, Frame &amp;F2, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cv::Point2f&gt; &amp;vbPrevMatched, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vnMatches12, <span class="keyword">int</span> windowSize=<span class="number">10</span>)</span></span>;</div><div class="line"></div><div class="line"><span class="comment">//用于进行三角时，寻找当前帧和其他帧之间的匹配</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchForTriangulation</span><span class="params">(KeyFrame *pKF1, KeyFrame* pKF2, cv::Mat F12, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">size_t</span>, <span class="keyword">size_t</span>&gt; &gt; &amp;vMatchedPairs, <span class="keyword">const</span> <span class="keyword">bool</span> bOnlyStereo)</span></span>;</div></pre></td></tr></table></figure></p>
<p>其实搞了这么多种匹配方式，无非就是为了限制待匹配特征的集合大小，而不是对整张图上的每一个特征点都进行汉明距离的计算，从而减小计算开销，加快匹配速度。<br>比如在<code>TrackWithMotionModel()</code>中调用<code>int SearchByProjection(Frame &amp;CurrentFrame, const Frame &amp;LastFrame, const float th, const bool bMono);</code>寻找当前帧和前一帧之间的匹配时，将通过如下方式加速匹配速度。</p>
<p>1) 对于前一帧能观测到所有地图点<code>cv::Mat x3Dw</code>，根据当前帧在世界坐标系下的旋转<code>const cv::Mat Rcw</code>和平移<code>const cv::Mat tcw</code>计算该地图点在当前帧相机坐标系下的坐标<code>cv::Mat x3Dc = Rcw*x3Dw+tcw;</code><br>2）利用相机的内参矩阵将<code>x3Dc</code>进行投影，得到该地图点在当前帧的像素坐标（u，v）。以（u，v）为搜索关联关系的中心，radius为半径，通过<code>vector&lt;size_t&gt; Frame::GetFeaturesInArea(const float &amp;x, const float  &amp;y, const float  &amp;r, const int minLevel, const int maxLevel)</code>方法返回候选的 ORB特征。选择其中与当前地图点汉明距离最小的 ORB特征，即建立起了关联关系。这个关联关系存储于<code>std::vector&lt;MapPoint*&gt; mvpMapPoints;</code>。<code>mvpMapPoints</code>的索引即为在该图像帧中的ORB特征的id值。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>博客写得再详细，依旧会忽略很多代码中的细节。越来越能理解一项工作，论文只能描述20%，剩下的80%全在代码里。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ORB-SLAM </tag>
            
            <tag> 源码学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《C++ Primer》 读书笔记 7]]></title>
      <url>https://zhehangt.github.io/2017/11/20/C++/Primer-07/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Momenta实习生二面，竟然被一道初始化列表和构造函数的问题给问崩了，唉，天下武功，为什么偏偏要选c++。<br>本篇博客将记录在恶补《C++ Primer》第七章时的重点内容。</p>
<h1 id="常量成员函数"><a href="#常量成员函数" class="headerlink" title="常量成员函数"></a>常量成员函数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">class ScaleData&#123;</div><div class="line"></div><div class="line">private:</div><div class="line">    string bookNO;</div><div class="line"></div><div class="line">public:</div><div class="line">    ScaleData(string str):bookNO(str)&#123;&#125;</div><div class="line">    string isbn() const &#123; return &quot;const_method &quot; + bookNO; &#125;</div><div class="line">    string isbn() &#123; return &quot;no_const_method &quot; + bookNO; &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>在 ScaleData 类中，有两个 isbn() 方法，第一个 isbn() 方法后面跟了一个 const 修饰符，这个 isbn() 因此被称为常量成员函数。那么这个函数的“常量性”体现在哪呢？首先得从普通的成员函数说起，即第二个 isbn() 。在调用成员函数时，成员函数会通过一个名为 this 的额外的隐式参数，来访问调用它的那个对象。也就是说 isbn() 对 bookNO 变量的访问实际上是通过 this-&gt;bookNO 的方式得到的。this总是指向调用函数的对象本身，无法指向其他对象，因此是常量指针。但是 this 不是常量类型的，即可以修改 this 对象内部的变量，比如 bookNO 变量。如果要使 this 变成常量类型的常量指针，即不能修改 this 对象内部的变量，要该怎么办？常量成员函数就是用来解决这个问题的。也就是说 c++ 中通过常量成员函数，来保证 this 指针是常量类型的，从而无法更改其对象内部的变量。总结起来有以下几点规则：</p>
<ul>
<li>常量对象只能调用常量成员函数，无法调用普通成员函数。</li>
<li>普通对象即可以调用常量成员函数，也可以调用普通成员函数（优先）。</li>
<li>常量成员函数可以访问常量成员变量和普通成员变量，但不允许修改普通成员变量。</li>
<li>普通成员函数只能访问普通成员变量，不允许访问常量成员变量。</li>
</ul>
<h1 id="构造函数与初始化列表"><a href="#构造函数与初始化列表" class="headerlink" title="构造函数与初始化列表"></a>构造函数与初始化列表</h1><p>构造函数用于对类的成员变量进行初始化。只要类的对象被创建，就会执行构造函数。<br>构造函数不能被声明为 const 的，当创建类的一个const对象时，直到构造函数完成初始化过程，对象才能真正取得其“常量”属性。<br>当一个类不存在构造函数时，编译器会为其生成默认的构造函数。默认构造函数一般通过以下步骤进行初始化：</p>
<ul>
<li>如果存在类内的初始值，用它来初始化成员变量。</li>
<li>否则，默认初始化。对于内置类型或复合类型，其初始化方式未定义。对于类对象，则利用默认构造函数进行初始化(其实是在默认初始化列表里完成的)。</li>
</ul>
<p>初始化列表可以认为是构造函数的一部分。因此构造函数的执行可以分为两个阶段：初始化阶段和计算阶段。初始化阶段即执行初始化列表，计算阶段即执行构造函数的函数体。初始化阶段先于计算阶段。<br>初始化阶段将会对所有非静态类类型的成员变量进行初始化，如果该成员变量没有出现在构造函数的初始化列表中（此时调用该成员的默认构造函数）。如果该成员变量出现在初始化列表中，则会调用对应参数的构造函数或者是拷贝构造函数。<br>计算阶段则只能进行赋值操作进行初始化。</p>
<p><strong>由于初始化阶段先于计算阶段执行，分别利用这两者进行初始化会存在性能上的差别。</strong><br>对于内置类型，使用初始化列表和构造函数体进行初始化差别不大。<br>但是对于类类型，最好使用初始化列表。采用初始化列表进行初始化只会调用一次对应参数的构造函数或者是拷贝构造函数，而采用构造函数体进行初始化将会调用一次默认构造函数和一次赋值构造函数。</p>
<p><strong>由于某些类类型对象的特殊性，对于某些成员变量必须采用初始化列表进行初始化。</strong></p>
<ol>
<li>初始化一个reference成员变量。</li>
<li>初始化一个const成员变量。</li>
<li>成员变量对应的类不存在默认构造函数。</li>
<li>基类不存在默认构造函数。</li>
</ol>
<h1 id="类的静态成员"><a href="#类的静态成员" class="headerlink" title="类的静态成员"></a>类的静态成员</h1><p>类的静态成员包含静态成员变量和静态成员函数。<br>类的静态成员通过 static 关键字使得其与类关联在一起，也就是说类的静态成员在于任何对象之外。因此无法通过 this 指针对静态成员变量和静态成员函数进行访问。静态成员函数也不能被声明成const的，也不能在静态成员函数内部使用 this 指针。</p>
<p>对于静态成员，可以直接使用类名加作用域运算符（::）进行访问。<br>虽然静态成员不属于类的某个对象，但是我们仍然可以使用类的对象、引用或者指针来访问静态成员。</p>
<blockquote>
<p>其实这一点还是有点奇怪，毕竟在调用成员函数时，会通过 this 指针。对于静态成员函数，应该有另一套实现机制。</p>
</blockquote>
<p>与普通成员函数一样，静态成员函数可以在类的内部定义，也可以在类的外部定义。但要注意的是，当在类的外部定义静态成员函数时，不能重复 static 关键字，该关键字只出现在类的声明语句。</p>
<p>而对于静态成员变量，因为其不属于类的任何一个对象，因此不能在构造函数中对静态成员变量进行初始化。而且一般来说也不能在类的内部进行静态成员变量的初始化（const static 变量是例外）。对于静态成员变量的初始化，必须在类的外部定义和初始化。<br>即使一个常量静态变量在类的内部被初始化了，通常情况下也应该在类的外部定义一下该成员。</p>
<p><strong>静态成员与普通成员</strong><br>静态成员变量的第一个特性是，静态成员变量可以是不完全类型（声明之后，定义之前）。特别地，静态成员的类型可以就是它所属的类类型。而非静态数据则受限制，只能声明成它所属类的指针或引用。</p>
<p>静态成员变量的第二个特性是，静态成员变量可以作为默认参数，而非静态成员函数不能作为默认参数。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>《C++ Primer 第5版》</p>
]]></content>
      
        <categories>
            
            <category> C++ </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[A Robust and Versatile Monocular Visual-Inertial State Estimator 论文笔记]]></title>
      <url>https://zhehangt.github.io/2017/11/11/SLAM/VINS/VINSPaper/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>VINS-Mono是最新的基于单目与IMU融合的SLAM系统，整篇文章详细描述了VINS系统的各个部分，与ORB-SLAM-Inertial进行对比学习，进一步加深IMU预积分如何应用到一个单目SLAM系统中去。</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-01.png">
<p>VINS主要分为4个部分：<br>（1）观测数据的预处理<br>负责图像特征提取，IMU测量值的预积分<br>（2）初始化<br>提供所有初始信息，包括位姿、速度、重力向量、陀螺仪偏差和三维地图点<br>（3）VIO<br>将IMU预积分信息与视觉特征融合，形成紧耦合的VIO，并且带有重定位和回环检测功能<br>（4）位姿优化<br>验证重定位结果，执行全局优化来消除累积误差</p>
<h1 id="观测数据预处理"><a href="#观测数据预处理" class="headerlink" title="观测数据预处理"></a>观测数据预处理</h1><p>1）图像数据预处理<br>VINS中，单目图像的追踪是通过 KLT sparse optical flow 算法进行的，提取的图像特征为角点特征，检测器（detector）通过设定像两个角点特征之间的最小像素间隔，来保证所提取的角点特征在图像中的分布。<br>在这一阶段还会执行关键帧的选择，需要满足两个条件。第一，当前帧和上一个关键帧之间要有足够的视差。第二，如果当前帧所追踪的特帧数目低于某个值，当前帧会被作为关键帧，以避免特征点追踪丢失的情况。</p>
<p>2）IMU预积分<br>这篇论文的IMU预积分在速度和平移上的与之前的两篇类似，但是在旋转上采用了四元数的方式。四元数的形式感觉上不如李代数来的简洁直观，采用四元数的方式的原因大概是跟作者之前的工作保持一致。</p>
<p>这篇论文还简单地给出了如何推导出预积分模型，主要思路就是将参考系从世界坐标系转换到预积分开始时刻的IMU坐标系下，从而可以将IMU的观测值分离出来。<br>公式（3）是世界坐标系下的计算公式。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-02.png"><br>公式（5）是以积分开始时刻IMU坐标系下的计算公式。$\alpha,\beta,\gamma$为只与imu测量值相关的部分，如公式（6）<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-03.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-04.png"><br>公式（7）为公式（6）的差分形式，要计算两个关键帧之间的预积分，即对每个观测值进行累加。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-05.png"></p>
<p>公式（9）给出了欧拉积分公式下系统状态的误差方程。得到协方差矩阵$P_{b_{k+1}}^{b_k}$。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-06.png"></p>
<p>公式（10）给出了协方差矩阵的迭代求解方式。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-07.png"></p>
<p>同样对于系统状态的雅可比矩阵也可以用迭代的方式求解（11）。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-08.png"></p>
<p>当得到新的IMU偏差量之后，为了减少计算量，可以通过公式（12）的方式更新IMU预积分量。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-09.png"></p>
<p>至此，得到了IMU的观测模型（13），即可建立基于IMU预积分的残差项。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-10.png"></p>
<blockquote>
<p>总得来说，IMU预积分模型都大同小异，对于雅可比矩阵和协方差矩阵的计算，还需要进一步的分析，但是感觉应该和前两篇论文是一样的。</p>
</blockquote>
<h1 id="估计初始化"><a href="#估计初始化" class="headerlink" title="估计初始化"></a>估计初始化</h1><p>与ORB-SLAM-Inertial不同的是，在本篇文章的初始化中，作者选择忽视加速度偏差，其理由是加速度偏差与重力在量级上相差比较大，加速度偏差很难被观测。</p>
<p>1）视觉初始化<br>与ORB-SLAM-Inertial类似，初始化过程将从纯视觉SLAM开始。当帧之间有足够的视差，就利用5点法恢复出旋转和平移，并对特征点进行三角化。根据当前的最后一帧和三角化的地图点，利用PnP方法对其他帧进行位姿估计。最后利用full-BA进行优化。另外，将第一帧图像作为世界坐标系的原点。</p>
<p>2）陀螺仪偏差初始化<br>陀螺仪的偏差估计原理上与ORB-SLAM-Inertial是相同的，只不过这里使用了四元数。如式（15）所示。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-11.png"></p>
<p>基于新得到的陀螺仪偏差，利用式（12）对IMU预积分项进行更新。</p>
<p>3）速度，重力向量和尺度初始化<br>把速度，重力向量和尺度作为估计量：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-12.png"></p>
<p>这部分的求解与ORB-SLAM-Inertial初始化的第二步是类似的，即将待估计的变量构造成一个超定方程，然后求其最小二乘解。但是在ORB-SLAM-Inertial作者曾指出，如果直接引入速度变量，会增加超定方程是病态的可能性。这篇文章中，并没有这样的考虑。</p>
<p>4）重力修正<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-16.png"><br>其基本原理就是利用上图所示的切线空间中的正交基$b_1$和$b_2$，计算其修正量$w1$和$w2$，修正后的重力向量为$g\cdot \bar{\hat{g}} + w_1b_1 + w_2b_2$。将其代入式（17），将$w_1$和$w_2$与（16）中的变量一起求解。正交基$b_1$和$b_2$可以通过如下的方式得到。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-17.png"><br>通过不断的迭代上述过程，直到$\hat{g}$收敛。</p>
<p>通过修正重力向量的方向，可以得到世界坐标系$(\cdot)^w$与参考坐标系$(\cdot)^{c_0}$之间的旋转$q_{c_0}^w$。利用$q_{c_0}^w$即可将之前得到的所有的速度修正到世界坐标系下。</p>
<blockquote>
<p>对比ORB-SLAM-Inertial中的重力修正部分，此处的重力修正部分虽然不如ORB-SLAM-Inertial中来得简洁明了，但是其利用了重力向量只有两个自由度需要估计（pitch，roll）的优点，在计算速度和准确性上可能会存在优势。</p>
</blockquote>
<h1 id="紧耦合VIO"><a href="#紧耦合VIO" class="headerlink" title="紧耦合VIO"></a>紧耦合VIO</h1><p>对于VIO，本文采用了基于滑动窗口的紧耦合的形式，在滑动窗口内所有要估计的系统状态如下：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-13.png"><br>其中$x_k$表示IMU在第$k$张图片时的状态，$\lambda_l$表示第$l$个特征点的反向深度。<br>我们采取 visual-inertial bundle adjustment 的形式，来对系统状态进行优化：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-14.png"><br>分别包含了先验残差项，IMU残差项和视觉残差项。</p>
<p>1）IMU残差项<br>基于式（13）的可以建立式（24）的IMU残差项，<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-15.png"><br>很奇怪的是，式（24）中的IMU预积分并没有包含偏差IMU偏差。</p>
<p>2）视觉残差项<br>与之前两篇论文中不同的是，VINS中所用的视觉残差项是基于球面的，而不是针孔的。这样做的好处是视觉模型可以兼容几乎所有类型的相机，包括广角相机和鱼眼相机。式（25）概括了整个视觉残差项的构造过程。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-18.png"><br>需要注意的是，这里的反投影是利用图像中的像素点坐标$[u,v]$得到三维空间中的特征点坐标。首先利用第$i$帧图像，该计算特征点坐标在当前相机坐标系下的三维坐标。然后利用第$j$帧图像对相同特帧点的观测，以及第$i$帧图像和第$j$帧图像之间的外参，同样计算出该特征点在第$i$帧图像相机坐标系下的三维坐标，在将两者的误差投影到正切空间，即可得到一个二维的残差值。其中$b_1$和$b_2$的计算与重力修正时用到的算法相同。</p>
<p>3）边缘化<br>为了保证滑动窗口内的关键帧数目保持在一个可以快速求解的数目，VINS中使用了 marginalization 方法，关键帧的选择方法如图所示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-19.png"></p>
<p>边缘化方法使用了 Schur complement 方法，并将边缘化的观测数据与要被去除的状态，转化为先验信息，加入到整个优化问题中。</p>
<p>4）Motion-only Bundle Adjustment<br>为了进一步减小计算量，引入另一种BA的形式。<br>此处依旧沿用式（22），但不再优化像（21）中所示的所有变量，只优化位姿、速度和固定数目的IMU状态，而把特征点深度、外参、偏差和旧的IMU状态作为固定量。使用依靠所有视觉和IMU观测量的motion-only BA，而不是依靠单张图像帧的PnP方法，可以使得状态估计的结果更加平滑。<br>这里有点像ORB-SLAM中的Tracking线程干的事。</p>
<p>文中还提到了IMU-Rate的状态估计、失败检测及恢复。</p>
<h1 id="重定位"><a href="#重定位" class="headerlink" title="重定位"></a>重定位</h1><p>重定位首先从回环检测开始。与ORB-SLAM类似，回环检测使用了词袋模型DBoW2，依赖于corner角点和BRIEF描述子。文中提到由于VIO可以观测roll和pitch，因此不需要依赖像ORB这种具备旋转不变性的特征。</p>
<p>检测到回环之后，要建立当前帧和回环帧之间的特征关联，通过BRIEF得到的特征关联会有很多的异常匹配，因此文中采用了基于对极几何和PnP的RANSAC来剔除异常匹配。</p>
<p>建立匹配之后，将利用匹配关系建立约束，如式（26）所示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-20.png"><br>与式（22）相比，式（26）多了一个回环误差项。</p>
<h1 id="全局位姿优化"><a href="#全局位姿优化" class="headerlink" title="全局位姿优化"></a>全局位姿优化</h1><p>重定位只是将当前滑动窗口与之前的位姿信息进行了对齐，为了获得全局一致的位姿信息，还需要进行全局位姿优化。之前也提到过，对于VIO来说，roll和pitch是客观测的，因此全局位姿的优化是一个四自由度的优化问题。</p>
<p>当关键帧被边缘化时，它会被加入到全局位姿图中。全局位姿图中的关联关系有两种，第一种称为 Sequential Edges, 是当前关键帧$i$在滑动窗口中与之前关键帧$j$之间的相对位置关系。如式（27）。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-21.png"></p>
<p>如果被边缘化的关键帧被检测到是回环，则会存在第二种关联关系，称为Loop Closure Edge，即关键帧与回环帧之间相对位置关系。其约束关系与式（27）相同。</p>
<p>有了关联关系之后，即可定义进行位姿图优化的残差项，如式（28）。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-22.png"><br>$\hat{\theta_i},\hat{\phi_i}$ 是可以从VIO中直接得到的 roll 和 pitch 的角度。</p>
<p>全局位姿优化问题即可构造为：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/VINS/VINS-23.png"><br>其中第一项是Sequential Edges，第二项为Loop Closure Edge。</p>
<p>为了保证VINS可以长时间的运行，防止位姿图过于庞大，在运行过程中采用了一种降采样的方式，来限制位姿图的尺寸。包含 Loop Closure Edge 的关键帧将会被保留，而某些与相邻关键帧很接近或者关联关系很类似的关键帧，就会被去除。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实验部分暂时先跳过了。<br>初次学习VIO，通读整篇文章还是有些吃力，很多地方理解的还有些羞涩。希望能够结合VINS的代码，进一步加深理解。<br>下一步，好好撸代码吧！</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator<br><a href="http://www.cnblogs.com/buxiaoyi/p/7353353.html" target="_blank" rel="external">http://www.cnblogs.com/buxiaoyi/p/7353353.html</a></p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> VIO </tag>
            
            <tag> VINS-Mono </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Visual-Inertial Monocular SLAM with Map Reuse 论文笔记]]></title>
      <url>https://zhehangt.github.io/2017/11/09/SLAM/ORBSLAM/ORBIMU/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>On-Manifold Preintegration for Real-Time Visual-Inertial Odometry提出了IMU预积分技术并且给出了详细的数学推导过程。ORB-SLAM的作者在IMU预积分的基础之上，在ORB-SLAM中实现了VIO。因此这篇论文在一定程度上可以看作是IMU预积分技术的工程实践。</p>
<h1 id="IMU预积分模型"><a href="#IMU预积分模型" class="headerlink" title="IMU预积分模型"></a>IMU预积分模型</h1><p>这篇论文中的IMU预积分模型，与上一篇论文中的预积分模型基本保持一致。</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-08.png">
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-09.png">
<h1 id="Visual-Inertial-ORB-SLAM"><a href="#Visual-Inertial-ORB-SLAM" class="headerlink" title="Visual-Inertial ORB-SLAM"></a>Visual-Inertial ORB-SLAM</h1><p>原始版本的ORB-SLAM中，系统拥有三个线程 Tracking，Local Mapping 和 Loop Closing。Visual-Inertial ORB-SLAM 将分别对这三个线程作修改，用以融合IMU信息。</p>
<p><strong>Tracking</strong><br>有了IMU之后，Tracking线程可以估计位姿，速度和IMU偏差，因此Tracking将会变得更加准确。<br>基于重投影误差和IMU预积分，建立帧与帧之间的约束关系，构造优化问题，从而得到当前帧位姿的最佳估计。<br>论文中根据当前是否更新了地图点，采用不同的优化方式。<br>1）地图点被 Local Mapping 和 Loop Closing 线程更新<br>此时IMU预积分误差项是建立在当前帧$j$和最近的一个关键帧$i$之间。状态估计问题建模为：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-01.png"></p>
<p>视觉误差项<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-02.png"></p>
<p>IMU误差项<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-03.png"></p>
<p>要注意的是，这里的残差项使用了Huber损失函数和马式距离。<br>利用g2o对公式（4）进行求解，其状态估计和Hessian矩阵将作为下一次更新的先验信息。</p>
<p>2) 地图点没有发生更新<br>此时IMU预积分误差项是建立在当前帧$j+1$和上一帧$j$之间，并利用之前已经计算得到的关于帧$j$的状态估计和Hessian矩阵建立额外的约束条件：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-04.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-05.png"></p>
<p><strong>Local Mapping</strong><br>对于Local Mapping线程的改动可以从下图中看出来。有了新的关键帧之后，将会对前N个关键帧进行优化，当前的关键帧（N+1）将固定不变，提供IMU预积分约束。将利用公式（5）和公式（6）建立优化问题的约束条件。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-06.png"></p>
<p>Local Mapping的另外一个功能是管理关键帧。对与local BA，如果连续关键帧之间相差小于0.5s，则进行剔除。对于full BA，如果连续关键帧之间相差小于3s,则进行剔除。</p>
<p><strong>Loop Closing</strong><br>由于IMU提供了尺度信息，因此全局位姿优化将从7个自由度下降到6个自由度。全局位姿优化将忽略IMU信息，因此不再优化速度和偏差，当完成全局位姿优化后，再根据矫正后的位姿对速度进行矫正。完成位姿优化后，将执行一次full BA，优化所有的系统状态，包括速度和偏差。</p>
<h1 id="IMU初始化"><a href="#IMU初始化" class="headerlink" title="IMU初始化"></a>IMU初始化</h1><p>IMU初始化对尺度，重力方向，速度和IMU偏差给出初始的估计量。这个估计量是从一系列被单目SLAM算法处理后的关键帧中估计出来的。初始化将会分为1）陀螺仪偏差估计，2）尺度和重力估计（忽略加速度偏差），3）加速度估计，并对尺度和重力方向进行修正，4）速度估计</p>
<p>1）陀螺仪偏差估计<br>陀螺仪的初始偏差估计比较简单，只需要根据ORB-SLAM求得的关键帧之间的旋转，对比利用IMU预积分模型求得的旋转，以偏差为变量，最小化两者的差值，如下图所示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-07.png"></p>
<p>式（9）的求解将基于高斯牛顿法进行。</p>
<p>2）尺度和重力估计（忽略加速度偏差）<br>首先关注尺度如何恢复。相机位姿的坐标$p_c$与真实世界相差一个尺度$s$，可以用如下的公式表示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-10.png"><br>将公式（10）代入公式（3）可得：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-11.png"></p>
<p>为了从等式（11）中求解尺度和重力，作者又开始了数学表演：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-12.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-13.png"></p>
<p>式（11）到式（12）的目的就是将速度变量$v$消除。主要思路是通过连续的三个关键帧列出两个式（11）的等式，然后利用式（3）中的速度预积分方程，将其消除。</p>
<p>对于$N$个连续关键帧，可以列出$N-2$个类似于式（12）的等式，其矩阵形式为$A_{3(N-2)\times 4} X_{4 \times 1 } = B_{3(N-2)\times 1}$，要求解的未知数是一个四维向量（尺度一维，重力三维），因此最少需要4个连续关键帧。<br>$A_{3(N-2)\times 4} X_{4 \times 1 } = B_{3(N-2)\times 1}$是一个超定方程，因此可以利用SVD来求解方程的最小二乘解。</p>
<p>3）加速度估计，并对尺度和重力方向进行修正</p>
<p>在求解式（12）时，我们忽略了加速度偏差，作者给出了忽略加速度偏差的理由。由于重力和加速度偏差比较难区分，如果直接在式（12）中加入加速度偏差，会导致其称为一个病态系统（ill-conditioned）。</p>
<p>而加速度偏差导致我们已经求得的重力向量是不准确的，为了求出这个加速度偏差，作者又要开始表演了。<br>在这里作者将重力G作为额外信息。在惯性参考坐标系下$I$中，重力的方向$\hat{g_I}=\{ 0,0,-1 \}$，而通过我们已经计算得到世界坐标系下的重力向量$g_w^* $，可以计算该重力向量的方向$\hat{g_w} = g_w^*/ \left \| g_w^*   \right \| $<br>（刚开始一直以为世界坐标系就是惯性参考坐标系，其实并不是。最理想的世界坐标系当然是惯性参考坐标系，但是在实际中，世界坐标系往往是以第一帧的位姿作为世界坐标系的原点建立的，这与惯性参考坐标系有明显的区别）</p>
<p>因此就可以计算惯性参考坐标系和世界坐标系之间的旋转矩阵$R_{WI}$以及修正：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-14.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-15.png"></p>
<p>由于计算$g_w^* $时未去除加速度偏差的影响，因此要得到理想的$g_w$，需要对式（15）进行优化。假设其优化量为$\delta \theta$，则有：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-16.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-17.png"></p>
<p>将（17）式代入（11）式，就会得到新的方程，这个方程包含了修正后的尺度$s$，重力方向的微调量$\delta \theta$和加速度偏差量$b_a$：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-18.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-19.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBIMU-20.png"></p>
<p>方程（19）的求解和方程（12）的求解方式类似，是一个$A_{3(N-2)\times 6} X_{6 \times 1 } = B_{3(N-2)\times 1}$的线性系统，需要至少4个连续关键帧，通过SVD求解。</p>
<p>4）速度估计</p>
<p>求得$s$, $\delta \theta$和$b_a$之后，利用式（18）和式（3），即可求解每个关键帧的速度$v$。</p>
<p>5）偏差重估计</p>
<p>当重定位模块完成了重定位之后，会利用式（9）对陀螺仪偏差重新估计。<br>加速度偏差用式（19）重新估计，只不过此时不需要再估计尺度和重力。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>Visual-Inertial Monocular SLAM with Map Reuse</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> VIO </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[On-Manifold Preintegration for Real-Time Visual-Inertial Odometry 论文笔记]]></title>
      <url>https://zhehangt.github.io/2017/11/07/SLAM/PreintegratedIMU/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><blockquote>
<p>考完雅思，写完硕士论文，各种乱七八糟的事在瞎忙，总算有时间好好看看文章了。</p>
</blockquote>
<p>IMU预积分在IMU与单目视觉SLAM融合中起到了非常关键的作用，很多VIO系统都是基于IMU预积分理论进行实现的。这篇论文算上附录总共85个公式，又一次让我感慨，数学是由多重要。</p>
<p>借助IMU，可以有效地解决单目SLAM系统尺度不确定问题和尺度漂移问题，极大地提升了单目SLAM系统的实用性。<br>在很多公式看不大懂的前提下，本文将基于论文严密的数学推导，用简单的思想进行概括，希望能够理清楚思路。</p>
<p>ps：公式编号遵循论文。</p>
<h1 id="状态估计"><a href="#状态估计" class="headerlink" title="状态估计"></a>状态估计</h1><p>所有的SLAM问题都可以建模成状态估计问题，通过相机或者激光的约束方程，来对需要估计的状态进行约束，从而得到状态估计的最优估计。在VIO中，可以建立如下的状态估计问题：<br>$$ x_i \doteq [R_i, p_i, v_i, b_i] \quad  （22）$$<br>其中下标$i$表示某个时刻，$(R_i, p_i)$表示机器人的位姿，即旋转矩阵和平移向量。$v_i \in \mathbb{R^3} $表示速度。$b_i=[b_i^g, b_i^a] \in \mathbb{R^6}$表示IMU中加速度计和陀螺仪的偏差。<strong>$X_k \doteq \{x_i\}_{i\in K_k}$都是在系统运行过程中未知的，需要我们通过观测数据来进行估计的。</strong></p>
<p>OK，那我们有哪些<strong>观测数据</strong>呢？当然就是相机数据和IMU数据了。用如下变量表示：<br>$$ Z_k \doteq \{C_i, I_{ij}\}_{(i,j) \in K_k} $$<br>其中$C_i$表示图像关键帧，$I_{ij}$表示两个连续关键帧之间的IMU数据。<br>因此整个状态估计问题就可以建模成基于观测数据$Z_k$，求$X_k$的最大后验<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-01.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-02.png"></p>
<p>其中$r_{I_{i,j}}$是基于IMU数据的残差值，$r_{C_{i,l}}$是基于图像数据的残差值。因此，要求解这个状态估计问题，理清楚$r_{I_{i,j}}$和$r_{C_{i,l}}$的表达方式非常重要。</p>
<h1 id="IMU模型"><a href="#IMU模型" class="headerlink" title="IMU模型"></a>IMU模型</h1><p>IMU数据包含两个部分，三轴加速度数据和三轴角速度数据。我们观测得到的加速度数据和角速度数据与实际值之间还相差了一个偏差$b$和噪声$\eta$。如下图所示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-03.png"></p>
<p>其中坐标为观测到的IMU数据，这些数据包含了噪声和偏差。B表示为IMU坐标系，W表示为世界坐标系。世界坐标系下，IMU的位姿用$ \{ \mathrm{R_{WB}}, \mathrm{_Wp} \}$ 表示。 $\mathrm{_w g}$表示世界坐标系下的重力向量。</p>
<p>从加速度推算速度和位移，以及从角速度推算旋转有如下的积分关系：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-04.png"></p>
<p>由于从IMU中获取的数据是离散的，因此需要将积分形式转为差分形式：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-05.png"></p>
<p>式（30）中的$\mathrm{_wa(t)}$和$\mathrm{_w \omega_{WB}(t)}$均为理论值，与实际的观测值之间相差一个偏差和噪声，用式（27）和式（28）对式（30）进行替换，得到：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-06.png"></p>
<h1 id="IMU误差项"><a href="#IMU误差项" class="headerlink" title="IMU误差项"></a>IMU误差项</h1><p>式（31）从理论上是可以作为$r_{I_{i,j}}$的约束条件的，但是由于IMU获取观测数据的频率非常高，这会导致约束条件过多，从计算速度上来说是不可行的。预积分就是要解决这个问题。基本思路就是将两个关键帧（i和j）之间的所有IMU数据进行一个计算，使其称为关键帧i和关键帧j的一个运动约束。如下图：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-07.png"></p>
<p>但是式（32）存在一个致命的问题，就是当$t_i$时刻的状态（如$R_{i}$）发生变化时，所有的运动约束关系都需要重新计算，因此引入一个增量的概念，使得$i$时刻和$j$时刻之间的约束关系只与IMU的观测数据有关，而与当前的状态无关，如下图所示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-08.png"></p>
<p>式（33）是预积分中的关键，它可以直接利用IMU的观测数据计算得到，作为$r_{I_{i,j}}$的约束条件。后续所有的公式都是围绕怎么计算这个公式，以及如何在非线性优化中计算状态估计的更新量。<br>式（33）应该包含了一次坐标转换，将世界坐标系下的物理量转换到了i时刻IMU的坐标系下的物理量。（待商榷）</p>
<p>式（33）已经建立了关键帧之间的系统状态与IMU数据之间的关联关系。利用式（33）构造式（26）的IMU残差项时，还会存在一个会头疼的问题，协方差矩阵怎么搞？</p>
<p>为了求这个协方差矩阵，又要开始堆公式了…<br>首先是将噪声数据从观测数据中分离出来：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-09.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-10.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-11.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-12.png"></p>
<p>主要利用的是右乘BCH近似雅可比和对数映射的一些运算公式。<br>公式（38）想要表达的意思就是通过观测值得到预积分，跟通过实际值得到的预积分，就是相差一个误差项而已，通过分析这个误差项，我们就可以得到关于IMU误差项的协方差矩阵。又要开始作者的表演。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-13.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-14.png"></p>
<p>总算证明出误差项在一阶近似的情况下，是对每一时刻的高斯噪声的线性组合，因此还是高斯噪声。这下可以通过一种线性传播的方式来求解每一个IMU预积分项的协方差矩阵了。在论文的附录A中给出了协方差矩阵的迭代求解方式。</p>
<p>讲完了误差项，再讲一讲偏差项。在上述的所有推导中，我们都是假设偏差项$b_i$是不变的，但在实际中，$b_i$是会随时间变化的一些列数值，通常可以认为是一种布朗运动。因此当我们在优化IMU残差项的过程中，会执行$b_i=\bar{b} + \delta b$更新，如果每得到一个新的$b_i$就重新计算一次预积分，计算量是不可接受的。因此利用一阶展开，可以对预积分进行增量更新。</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-15.png">
<p>公式中出现N多的雅可比都是可以在预积分过程中预先计算的。公式（44）的证明以及雅可比的计算方式在论文的附录B中给出了。</p>
<p>有了上面这么多的公式，总算可以构造我们的IMU误差项了：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-16.png"></p>
<p>在对误差项利用高斯牛顿法求解时，需要求解待优化变量的雅可比矩阵，其求解方式在论文附录C中给出。</p>
<p>另外，为了使得IMU的偏差符合布朗运动，另外添加一项约束到IMU误差项中：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-17.png"></p>
<h1 id="视觉误差项"><a href="#视觉误差项" class="headerlink" title="视觉误差项"></a>视觉误差项</h1><p>回顾一下式（26）中的视觉残差项：</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-18.png">
<p>一个标准的视觉残差模型如下所示：</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-19.png">
<p>式（50）中如果对所有的地图点$\rho_l$进行优化，会带来不小的计算开销，因此论文采用了一种structureless approach的方法，来避免对地图点进行优化。</p>
<p>在高斯牛顿法的迭代过程中，将求解如下的最小二乘问题，从而得到系统状态估计的更新量$\delta \phi _i, \delta p _i, \delta \rho _l$。</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-20.png">
<p>式（51）中$\check{\pi}(\cdot)$使用了一次李代数上的扰动模型。</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-21.png">
<p>求解式（51）的常用办法就是作线性展开，得到：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-22.png"><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-23.png"></p>
<p>其中若将$\delta T _{x(l)}$作为已知项，求式（53）的最小二乘则有：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-24.png"></p>
<p>将式（54）代入式（53），整个求解问题就转变为了：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PreintegratedIMU-25.png"></p>
<blockquote>
<p>论文这一部分的动机是将地图点从整个优化问题中去除出去，运用了一种Schur complement trick的BA方法，但是为什么不像ORB-SLAM那样固定地图点，而只优化位姿呢？这部分内容还需要结合相关材料在深入理解。</p>
</blockquote>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.On-Manifold Preintegration for Real-Time Visual-Inertial Odometry</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> VIO </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[雅思考试小记]]></title>
      <url>https://zhehangt.github.io/2017/10/20/Ielts/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>从8月开始准备到10月考试，学雅思，学到生活不能自理，学到怀疑人生。<br>科研停滞，博客停滞。这段时间觉得能看论文、能写代码是多么幸福的一件事。</p>
<p>大概高考过后再也没有系统学过英语了，简直就是吃老本吃到负债。四六级直接裸考过还一度对自己的英语很自信，真是迷之自信。。。</p>
<p>简单总结一下这两个月学雅思的一些体会。</p>
<p><strong>阅读</strong>：<br>曾经一天3篇论文的阅读量好得还管点用。<br>阅读最重要的应该就是单词和关键信息的定位了。<br>所以，背单词背单词背单词吧。单词量破万！</p>
<blockquote>
<p>参考资料：<br>新东方雅思词汇（乱序版）<br>真题4-12阅读生词</p>
</blockquote>
<p><strong>听力</strong>：<br>听力虐我一千遍，我待听力如初恋。<br>自幼阅片无数，阅剧无数，按道理应该听力无敌。可惜现实真无奈。Section4几乎全军覆没，实在是实在是听不懂啊。大概和我及其不标准的口语发音有关。<br>要保证一个基本的分数（6分），语料库是王道。这两个月没用对语料库很遗憾。一定要每天听写每天听写，不能只对着错误背背背，一点用都没有。所以要保证一周能把3、4、5章轮一遍。每天大概3到5个wordlist。拿下语料库6.5分有望。<br>再接下来大概就要靠天赋了。嗯，再想想。</p>
<blockquote>
<p>参考资料：<br>雅思王听力真题语料库<br>真题4-12听力<br>朗易思听</p>
</blockquote>
<p><strong>口语</strong>：<br>每次一张嘴，必定遭到周围小伙伴的群嘲，真是很受伤。<br>这次考试全靠背，越背到后面越没有信心。<br>以后决心每天跟读1小时，纠正发音，说不定还能把听力练好。</p>
<blockquote>
<p>参考资料：<br>手把手教雅思词伙<br>10天突破雅思口语<br>雅思哥</p>
</blockquote>
<p><strong>写作</strong>：<br>写得出论文，写不出作文。顾家北看了不知道多少遍，感觉还是写不出来。<br>下次准备的策略大概会是：<br>1.反复看范文积累各个话题的词伙<br>2.反复读背范文，说不定能打通筋骨。</p>
<blockquote>
<p>参考资料：<br>手把手教雅思写作<br>小美粑粑公共号</p>
</blockquote>
<p>这次学雅思的内心煎熬大概比得上3年前膝盖第二次手术。从刚开始的满怀希望，到最后几近绝望。<br>其实最怕的不是失败吧，是害怕自己失去了那份坚持的动力。</p>
<p>所以，不管这次能不能过，接下来的日子，希望会有一天，遇到一个说着地道英语的自己。<br>一索哥！加油吧！</p>
]]></content>
      
        <categories>
            
            <category> 不知道在说什么 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[GPU升级小记]]></title>
      <url>https://zhehangt.github.io/2017/07/15/UpdateGPU/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在实验室这台4年前的3代i5的机子上撸深度学习真是件令人崩溃的事。能有一块Titan X成了心中的小梦想，1080ti也凑合吧。某个夜晚辗转反侧思索良久，梦想是用来实现的，钱不花出去都是纸。所以，下单1050ti。毕竟梦想很丰满，现实很骨感。</p>
<h1 id="选购"><a href="#选购" class="headerlink" title="选购"></a>选购</h1><p>简单总结下已有的硬件，H61的骨灰级主板，3代i5，5400转500G超大硬盘，2条4G内存，280W电源，办公用小机箱以及步骤AMD不知道啥型号的显卡。我在想，搞做深度学习方面研究的童鞋们还有比我更渣的机器吗。多说都是类。<br>鉴于手头的资金有限，考虑升级1050ti或者1060。万恶的矿工们，搞得显卡价格蹭蹭涨，唉。<br>简单调研了下，1050ti建议电源300w，尺寸中等，网上有成功在H61上装1050ti的案例。1060建议电源500w，尺寸大，网上有说H61需要升级bios才能点亮1060。鉴于条件艰苦，还是保守1050ti吧。具体型号：微星红龙1050ti。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>刷掉一个月生活费，1050ti终于出现在我的手上。<br>拆机箱，拆显卡，上1050ti，打开电源。嗯，果然显卡没有任何反映。我…难道是主板太旧了…我的生活费…内心是崩溃的…<br>其实其实就是外接电源没接上，不是说好的1050ti不需要外接电源也能用吗。新购4pin转6pin电源线，插上成功点亮。可是，插上新显卡后进不了ubuntu系统…生活真是多磨难…<br>猜测肯定是驱动的原因，于是禁用独显，用集显输出，成功进入系统，用ubuntu自带的附加驱动程序安装1050ti的显卡驱动…搞定！<br>接下来就是搭环境了。</p>
<ul>
<li>安装cuda<br>从官网下载相应版本的安装程序。大家都推荐本地运行的run文件。<br>注意，安装过程中在询问是否安装驱动时选择否，否则后果很严重，大家都是这么说的。</li>
<li>安装cuddn<br>从官网下载相应版本的库文件，将库文件拷贝到相应的cuda安装目录中，并重新建立软连接。</li>
<li>重新编译caffe<br>之前安装了cpu-only 的caffe，所以在编译前通过 make clean 删除之前编译生成的文件，否则make test会出错。</li>
</ul>
<p>大功告成！</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>简单测试下1050ti的性能。<br>运行caffe自带的example中的用caffenet做分类的例子。</p>
<p>在CPU模式下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">%timeit</div><div class="line">%timeit net.forward()</div><div class="line"><span class="comment"># 1 loop, best of 3: 4.97 s per loop</span></div></pre></td></tr></table></figure></p>
<p>在GPU模式下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">caffe.set_device(<span class="number">0</span>)  <span class="comment"># if we have multiple GPUs, pick the first one</span></div><div class="line">caffe.set_mode_gpu()</div><div class="line">net.forward()  <span class="comment"># run once before timing to set up memory</span></div><div class="line">%timeit net.forward()</div><div class="line"><span class="comment"># 10 loops, best of 3: 71.9 ms per loop</span></div><div class="line">`</div></pre></td></tr></table></figure></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>我想要Titan X，我不管我就是要Titan X。</p>
]]></content>
      
        <categories>
            
            <category> 折腾日记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[TensorFlow Get Started 学习笔记]]></title>
      <url>https://zhehangt.github.io/2017/06/26/DeepLearning/Tensorflow_GetStarted/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>关于 <a href="https://www.tensorflow.org/get_started/" target="_blank" rel="external">Tensorflow Get Started</a> 的学习笔记。</p>
<h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><h3 id="Getting-Started-With-TensorFlow"><a href="#Getting-Started-With-TensorFlow" class="headerlink" title="Getting Started With TensorFlow"></a><a href="https://www.tensorflow.org/get_started/get_started" target="_blank" rel="external">Getting Started With TensorFlow</a></h3><p>TensorFlow的基本数据单元被称为tensor。每一个tensor实质上是具有特定维度，特定类型的多维数组。<br>TensorFlow对tensor的每一次操作都是一个节点，对tensor的多个操作就形成了整个的计算任务，整个计算任务被称为图。</p>
<p>首先可以对tensor进行三种不同类型的操作，赋予其一定的属性，从而得到最原始的数据节点。<br><strong>tf.constant</strong>：赋予tensor常量属性<br><strong>tf.placeholder</strong>：表示tensor在用来进行计算之前才赋予相应的数据，通常用来表示数据集<br><strong>tf.Variable</strong>：表示tensor在计算时数值会变化，通常用来表示模型参数</p>
<p>对单个数据节点或者数据节点之间可以进行计算操作，得到计算节点。比如<br><strong>tf.square</strong>：表示平方<br><strong>tf.add</strong>：表示乘法<br><strong>tf.matmul</strong>：表示加法<br><strong>…</strong></p>
<p>对于每个计算任务，首先将tensor构造成数据节点，然后对数据节点进行多次的计算操作，从而得到最终的computational graph。有了computational graph之后，需要通过session的run方法来执行计算，从而得到最终的计算结果。</p>
<p>TensorFlow中最特殊的计算操作为optimizers。</p>
<p>除了通过最简单的节点操作构造计算任务，TensorFlow还提供了更高层的tf.contrib.learn包，来快速的构造计算任务。</p>
<h3 id="MNIST-For-ML-Beginners"><a href="#MNIST-For-ML-Beginners" class="headerlink" title="MNIST For ML Beginners"></a><a href="https://www.tensorflow.org/get_started/mnist/beginners" target="_blank" rel="external">MNIST For ML Beginners</a></h3><p>这一章节，针对 MNIST 分类问题，构造了一个softmax分类器，交叉熵作为损失函数，利用梯度下降进行训练。<br>代码整体流程如下：</p>
<ol>
<li>定义模型。</li>
<li>基于模型定义交叉熵损失函数</li>
<li>构造梯度训练的优化器，损失函数和训练集作为优化器的参数对模型进行训练，训练共进行了1000次迭代</li>
<li>基于模型定义正确率函数，在测试集上进行测试，得到正确率</li>
</ol>
<h3 id="Deep-MNIST-for-Experts"><a href="#Deep-MNIST-for-Experts" class="headerlink" title="Deep MNIST for Experts"></a><a href="Deep MNIST for Experts">Deep MNIST for Experts</a></h3><p>这一章节，针对 MNIST 分类问题，构造了一个多层的卷积神经网络，交叉熵作为损失函数，利用梯度下降进行训练。<br>代码整体流程如下：</p>
<ol>
<li>定义模型，通过deepnn()函数完成。模型为卷积层，池化层，卷积层，池化层，全连接层，dropout层，全连接层</li>
<li>基于模型定义交叉熵损失函数</li>
<li>构造Adam优化器</li>
<li>定义正确率函数</li>
<li>训练共进行20000次迭代，每100次计算一次正确率</li>
</ol>
<h3 id="TensorFlow-Mechanics-101"><a href="#TensorFlow-Mechanics-101" class="headerlink" title="TensorFlow Mechanics 101"></a><a href="https://www.tensorflow.org/get_started/mnist/mechanics" target="_blank" rel="external">TensorFlow Mechanics 101</a></h3><p>这一章节，针对 MNIST 分类问题，构造了一个全连接的神经网络，交叉熵作为损失函数，利用梯度下降进行训练。<br>其中mnist.py文件中定义了全连接神经网络模型，定义了梯度下降优化器。<br>可以用TensorBoard显示损失函数的值变化和computational graph。</p>
<h3 id="tf-contrib-learn-Quickstart"><a href="#tf-contrib-learn-Quickstart" class="headerlink" title="tf.contrib.learn Quickstart"></a><a href="https://www.tensorflow.org/get_started/tflearn" target="_blank" rel="external">tf.contrib.learn Quickstart</a></h3><p>tf.contrib.learn是TensorFlow中的高层API，可以快速地进行机器学习模型的构建与训练。这一章节，针对 Iris 分类问题。<br>通过tf.contrib.learn.DNNClassifier方法，可以快速构造一个多层的神经网络。<br>通过fit方法可以进行自动化的训练。<br>通过evaluate方法可以进行模型评估。<br>通过predict方法可以利用模型进行预测。</p>
<h3 id="Building-Input-Functions-with-tf-contrib-learn"><a href="#Building-Input-Functions-with-tf-contrib-learn" class="headerlink" title="Building Input Functions with tf.contrib.learn"></a><a href="https://www.tensorflow.org/get_started/input_fn" target="_blank" rel="external">Building Input Functions with tf.contrib.learn</a></h3><p>在使用tf.contrib.learn进行机器学习模型的构建与训练时，支持对原始数据进行预处理。</p>
<h3 id="Logging-and-Monitoring-Basics-with-tf-contrib-learn"><a href="#Logging-and-Monitoring-Basics-with-tf-contrib-learn" class="headerlink" title="Logging and Monitoring Basics with tf.contrib.learn"></a><a href="https://www.tensorflow.org/get_started/monitors" target="_blank" rel="external">Logging and Monitoring Basics with tf.contrib.learn</a></h3><p>在训练机器学习模型时，通常需要实时的跟踪和评估模型，这时可以借助 Monitor 包。</p>
<h3 id="TensorBoard-Visualizing-Learning"><a href="#TensorBoard-Visualizing-Learning" class="headerlink" title="TensorBoard: Visualizing Learning"></a><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard: Visualizing Learning</a></h3><p>TensorBoard 涉及到的运算，通常是在训练庞大的深度神经网络中出现的复杂而又难以理解的运算。</p>
<p>为了更方便 TensorFlow 程序的理解、调试与优化，可以用 TensorBoard 来展现 TensorFlow 图像，绘制图像生成的定量指标图以及附加数据。</p>
<p>TensorBoard 通过读取 TensorFlow 的事件文件来运行。TensorFlow 的事件文件包括了你会在 TensorFlow 运行中涉及到的主要数据。下面是 TensorBoard 中汇总数据（Summary data）的大体生命周期。</p>
<ol>
<li>首先，创建你想汇总数据的 TensorFlow 图，然后再选择你想在哪个节点进行汇总(summary)操作。<br>比如，假设你正在训练一个卷积神经网络，用于识别 MNISt 标签。你可能希望记录学习速度(learning rate)的如何变化，以及目标函数如何变化。通过向节点附加scalar_summary操作来分别输出学习速度和期望误差。然后你可以给每个 scalary_summary 分配一个有意义的 标签，比如 ‘learning rate’ 和 ‘loss function’。<br>或者你还希望显示一个特殊层中激活的分布，或者梯度权重的分布。可以通过分别附加 histogram_summary 运算来收集权重变量和梯度输出。</li>
<li>在TensorFlow中，所有的操作只有调用执行方法，方法才会真正执行。而summary操作是相互独立的，如果对每个summary操作调用执行操作会相当繁琐，因此，需要通过tf.summary.merge_all汇总所有这些被summary的节点。</li>
<li>汇总之后即可调用执行方法完成汇总操作，会将所有数据生成一个序列化的Summary protobuf对象。</li>
<li>最后，为了将汇总数据写入磁盘，需要将汇总的protobuf对象传递给tf.summary.FileWriter。</li>
</ol>
<h3 id="TensorBoard-Embedding-Visualization"><a href="#TensorBoard-Embedding-Visualization" class="headerlink" title="TensorBoard: Embedding Visualization"></a><a href="https://www.tensorflow.org/get_started/embedding_viz" target="_blank" rel="external">TensorBoard: Embedding Visualization</a></h3><p>Embeddings 在机器学习模型中非常常见，特别是在推荐系统，NLP等等。TensorFlow提供了Embedding Projector可视化工具，对embeddings数据进行可视化显示。</p>
<h3 id="TensorBoard-Graph-Visualization"><a href="#TensorBoard-Graph-Visualization" class="headerlink" title="TensorBoard: Graph Visualization"></a><a href="https://www.tensorflow.org/get_started/graph_viz" target="_blank" rel="external">TensorBoard: Graph Visualization</a></h3><p>利用 TensorFlow 构造的 computational graph 非常强大但往往又非常复杂，图表可视化在理解和调试时显得非常有帮助。</p>
<p>对于深度学习模型来说，一个computational graph往往包含上千个节点，同时显示上千个节点并不是一种可取的方式。TensorBoard 可以利用 tf.name_scope 来生成节点的命名空间，借助于命名空间，TensorBoard可以对节点实现层级显示。因此命名空间的设计是可视化的关键所在。</p>
<p>TensorBoard 的计算图可视化有两种形式。节点之间如果是数据相关的，用实线相连。节点之间如果是控制相关的，用虚线相连。</p>
<p>TensorBoard 还有很多技巧来优化计算图的显示，包括改变形状，改变颜色等等。</p>
<h3 id="TensorBoard-Histogram-Dashboard"><a href="#TensorBoard-Histogram-Dashboard" class="headerlink" title="TensorBoard Histogram Dashboard"></a><a href="https://www.tensorflow.org/get_started/tensorboard_histograms" target="_blank" rel="external">TensorBoard Histogram Dashboard</a></h3><p>可以显示数据直方图</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.tensorflow.org/get_started/" target="_blank" rel="external">Tensorflow Get Started</a></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
            <category> TensorFlow </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《深度学习》 读书笔记 09]]></title>
      <url>https://zhehangt.github.io/2017/06/15/DeepLearning/DeepLearning-09/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>《深度学习》第九章介绍了卷积神经网络。</p>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>首先回答什么是卷积神经网络。卷积神经网络是指<strong>至少在网络的一层中使用卷积运算来代替一般矩阵乘法运算</strong>的神经网络。<br>卷积神经网络是神经科学的原理影响深度学习的典型代表。</p>
<blockquote>
<p>此前在《数字图像处理》的第三章已经简单了解过卷积的概念。</p>
</blockquote>
<p>其次回答为什么要用卷积。卷积运算通过三个重要的思想来帮助改进机器学习系统：<br><strong>稀疏交互</strong>(sparse interactions)：传统的神经网络使用全连接来建立输入与输出的关系，这意味着如果输入的维度很大时，会导致参数规模也很大。稀疏交互(稀疏权重)可以显著的降低参数规模。<br><strong>参数共享</strong>(parameter sharing)：在卷积神经网络中，卷积核会作用在输入的每个位置上，也就是说卷积核的参数是共享的。<br><strong>等变表示</strong>(equivariant representations)：令$g$是输入的任意平移函数，那么卷积函数对于g具有等变性。</p>
<p>卷积神经网络的卷积层基本结构如下图所示，大致有三级：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/DeepLearning-09-01.png"></p>
<p>在Convolution Stage，并行地进行多个卷积运算来产生一组线性激活函数。<br>在Detector Stage，非线性的激活函数如(ReLU)作用在每一个线性输出上。<br>在pooling stage，通过池化函数来更进一步地调整卷积级的输出。</p>
<p>卷积函数可以看作为一种特征学习器。<br>探测函数可以看作为一种特征激活器。<br>池化函数使用某一位置的相邻输出的总体特征来代替网络在该位置的输出。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合，还能起到保证少量平移不变性，处理不同大小的输入等。</p>
<p>从概率学的角度来讲，卷积与池化作为一种无限强的先验，这个先验要求卷积层包含局部连接关系并且对平移具有等变性，只有当先验的假设合理且正确时才有用。当一项任务涉及到要对输入中相隔较远的信息进行合并时，那么卷积所需要的先验可能就不正确的。</p>
<h1 id="卷积核"><a href="#卷积核" class="headerlink" title="卷积核"></a>卷积核</h1><p>每个卷积核涉及到空间尺寸问题，卷积核只与输入数据的局部区域连接，该连接的空间大小叫做感受野(receptive field)。其次卷积核第三个维度的大小始终与输入图像的通道数相同。<br>每个卷积层涉还及到3个超参数：<br><strong>深度(depth)</strong>：深度指的是卷积核的数目。单个卷积核只能提取一种类型的特征，多个卷积核能够提取多种类型的特征。有时候也称为滤波器数目。输入数据与卷积核进行卷积得到的输出数据称为深度切片。深度切片数与卷积核数目相同。<br><strong>步长(stride)</strong>：步长指在输出的每个方向上进行s个像素的下采样(downsampling)。<br><strong>零填充(zero padding)</strong>：零填充涉及三种不同方式。<br>第一种是不使用零填充，并且卷积核只允许访问那些图像中能够完全包含整个核的位置，称为有效(valid)卷积，这使得输出像素的表示非常有效，但是会导致输出的大小在每一层都会缩减。如果输入的图像宽度是$m$，核的宽度是$k$，那么输出的宽度就会变成$(m-k+1)$。<br>第二种是使用零填充来保持输入和输出具有相同的大小，称为相同(same)卷积，此时零填充的数目是$\dfrac{k-1}{2}$。相同卷积可能会导致边界像素存在一定程度的欠表示。<br>第三种是进行足够多的零填充，使得每个像素在每个方向上恰好被方位了$k^3$，此时零填充的数目是$(k-1)$，最终输出的图像的宽度为$(m+k-1)$，称为全(full)卷积。</p>
<p>输入数据的尺寸为：$W_1 \times H_1 \times D_1$<br>卷积层4个超参数：卷积核数目$K$，感受野尺寸为$F$，步长为$S$，零填充为$P$<br>输出数据体的空间尺寸为：$W_2 \times H_2 \times D_2$<br>$W_2 = (\dfrac{W_1-F+2P}{S}+1)$<br>$H_2 = (\dfrac{H_1-F+2P}{S}+1)$<br>$D_2 = K$</p>
<h1 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h1><p>通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值<br>输入数据的尺寸为：$W_1 \times H_1 \times D_1$<br>池化层2个超参数：空间尺寸为$F$，步长为$S$<br>输出数据体的空间尺寸为：$W_2 \times H_2 \times D_2$<br>$W_2 = (\dfrac{W_1-F}{S}+1)$<br>$H_2 = (\dfrac{H_1-F}{S}+1)$<br>$D_2 = D_1$</p>
<h1 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h1><p><strong>输入层</strong>应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。<br><strong>卷积层</strong>应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长$S=1$。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当$F=3$，那就使用$P=1$来保持输入尺寸。当$F=5$,就用$P=2$，一般对于任意$F$，当$P=(F-1)/2$的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。<br><strong>池化层</strong>负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即F=2）的最大值汇聚，步长为2（S=2）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。</p>
<h1 id="案例学习"><a href="#案例学习" class="headerlink" title="案例学习"></a>案例学习</h1><p>下面是卷积神经网络领域中比较有名的几种结构：</p>
<p><strong>LeNet</strong>： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的LeNet结构。</p>
<p><strong>AlexNet</strong>：AlexNet卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的ImageNet ILSVRC 竞赛中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。</p>
<p><strong>ZF Net</strong>：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为 ZFNet（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。</p>
<p><strong>GoogLeNet</strong>：ILSVRC 2014的胜利者是谷歌的Szeged等实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是Inception-v4。</p>
<p><strong>VGGNet</strong>：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为VGGNet。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的预训练模型是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。</p>
<p><strong>ResNet</strong>：残差网络（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了批量归一化（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（视频，PPT），以及一些使用Torch重现网络的实验。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文Identity Mappings in Deep Residual Networks，2016年3月发表。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>《深度学习》第九章<br><a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：卷积神经网络笔记</a></p>
]]></content>
      
        <categories>
            
            <category> 深度学习 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《数字图像处理》 读书笔记 03]]></title>
      <url>https://zhehangt.github.io/2017/06/05/ComputerVision/DigitalImageProcessing-03/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>《数字图像处理》第三章介绍了最常用的图像处理方式—空间滤波。</p>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>空间滤波是图像处理领域应用最广泛的主要工具之一。利用空间滤波，可以实现图像降噪，图像模糊，图像增强等功能。<br>线性空间滤波实际上与频率域滤波之间存在一一对应的关系。而对于非线性空间滤波的处理效果，频率域滤波是无法做到的。<br>空间滤波的基本概念如下图所示：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/DigitalImageProcessing-03-01.jpg"></p>
<p>其中滤波器模板又通常被称为空间滤波器，空间掩模，核，模板，窗口等。<br>计算公式如下：<br>$<br>w(x,y)☆f(x,y)=\sum \limits _{s=-a}^{a} \sum \limits_{t=-b}^{b} w(s,t)f(x+s, y+t)<br>$</p>
<p>在执行线性空间滤波时，有两个相近的概念。一个是相关(Correlation)，一个是卷积(Convolution)。<br>相关是滤波器模板移过图像并计算每个位置乘积之和的处理，即上述的空间滤波公式。卷积的机理类似，但滤波器模板需要先旋转180度。</p>
<blockquote>
<p>卷积的基本特性是某个函数与某个单位冲激卷积，得到一个在该冲激出的这个函数的拷贝。</p>
</blockquote>
<p>计算公式如下：<br>$<br>w(x,y)\bigstar f(x,y)=\sum \limits _{s=-a}^{a} \sum \limits_{t=-b}^{b} w(s,t)f(x-s, y-t)<br>$</p>
<p>其实空间滤波使用的是相关操作，但为什么要提到卷积？因为对于线性空间滤波，实际上与频率域滤波之间存在一一对应的关系，即空间域的相关操作就是在频率域进行卷积操作。但是无论是相关操作还是卷积操作，最重要的还是如何设计滤波器模板，从而使图像通过滤波能够呈现出我们所希望的结果。</p>
<p>图像滤波的目的有两个：一个是抽出对象的特征作为图像识别的特征模式；另一个是为适应图像处理的要求，消除图像数字化时所混入的噪声。<br>图像滤波的要求也有两个：一是不能损坏图像的轮廓及边缘等重要信息，二是使图像清晰视觉效果好</p>
<h1 id="线性滤波"><a href="#线性滤波" class="headerlink" title="线性滤波"></a>线性滤波</h1><p><strong>方框滤波(均值滤波)</strong><br>最简单的线性滤波器。将窗口内的像素求和或求均值。求均值即为<strong>均值滤波</strong>。<br>方框滤波计算简单，计算速度快。但方框滤波的缺点就是不能很好地保护细节，从而使图像变得模糊。方框滤波无法去掉噪声，只能微弱的减弱它。。</p>
<p><strong>高斯滤波</strong><br>高斯滤波器是一类根据高斯函数的形状来选择权值的线性平滑滤波器。高斯滤波器对于抑制服从正态分布的噪声非常有效。均值滤波器的模板系数都是相同的为1；而高斯滤波器的模板系数，则随着距离模板中心的增大而系数减小。所以，高斯滤波器相比于均值滤波器对图像个模糊程度较小。高斯滤波器模板就是对二维高斯函数进行离散化采样，得到模板系数。<br>高斯滤波被用作为平滑滤波器的本质原因是因为它是一个低通滤波器。</p>
<h1 id="非线性滤波"><a href="#非线性滤波" class="headerlink" title="非线性滤波"></a>非线性滤波</h1><p><strong>中值滤波</strong><br>中值滤波是一种典型的非线性滤波技术，基本思想是用像素点邻域灰度值的中值来代替该像素点的灰度值，该方法在去除脉冲噪声和椒盐噪声的同时又能保留图像的边缘细节。但是中值滤波花费的时间是均值滤波的5倍以上。</p>
<p><strong>双边滤波</strong><br>双边滤波器可以做边缘保存的同时去除噪声。之所以可以达到此去噪效果，是因为滤波器是由两个函数构成。一个函数是由几何空间距离决定滤波器系数，称为定义域核。另一个由像素差值决定滤波器系数，称为值域核。<br>具体公式定义见《OpenCV3 编程入门》 第六章。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>《数字图像处理》第三章<br>《OpenCV3 编程入门》 第六章</p>
]]></content>
      
        <categories>
            
            <category> 计算机视觉 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数字图像处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《数字图像处理》 读书笔记 04]]></title>
      <url>https://zhehangt.github.io/2017/05/25/ComputerVision/DigitalImageProcessing-04/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>《数字图像处理》第四章从傅立叶变换的原理出发,介绍了二维傅立叶变换在数字图像处理中的应用.有点难懂,希望通过读书笔记加深理解.</p>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>任何周期函数都可以表示为不同频率的正弦与余弦之和的形式,每个正弦项和余弦项都乘以不同的系数,这个系数被称为傅立叶级数.无论函数多么复杂,只要它是周期的,并且满足某些数学条件,就可以用这样的和来表示.傅立叶级数有三角函数展开式和复指数展开式两种.其实两者本质上是一样的,可以通过欧拉公式相互转换.<br><strong>傅立叶级数有三角函数展开式</strong>:<br>$$f(t)= \dfrac{a_0}{2} + \sum_{n=1}^{+\infty} a_n  \cos(\omega nt) + b_n  \sin(\omega nt) \quad$$<br><strong>欧拉公式</strong>:<br>$$ e^{j\theta} = \cos \theta + jsin\theta$$<br><strong>傅立叶级的复数展开式</strong>:<br>$$f(t)=\sum_{n=-\infty}^{\infty}c_ne^{j\omega nt}$$</p>
<p>其中$\omega = \dfrac{2\pi}{T}$</p>
<p>而对于非周期函数,只要曲线下的面积是有限的,就可以用正弦与余弦乘以加权函数的积分来表示.这个变换公式被称为傅立叶变换.<br><strong>傅立叶变换</strong><br>$$F(\omega) = \Im\{f(t)\} = \int_{-\infty}^{\infty} f(t) e^{-j\omega t}dt$$</p>
<p>可以看到函数$f(t)$经过傅立叶变换得到$F(\omega)$,变量从时间域$t$变为了频率域$\omega$.对于在难以分析的时域信号,通过傅立叶变换就可以将其转换为易于分析的频域信号.</p>
<p>无论是傅立叶级数还是傅立叶变换,针对的都是连续信号,即$f(t)$是一个连续函数.但是计算机是无法处理连续值的,只能处理离散值,因此引出了另外两种傅立叶变换形式.</p>
<p>对于离散周期性信号$\{f(x)\}_{0\leq x&lt;M}$,通过<strong>离散傅立叶变换</strong>,得到离散周期频域信号:<br>$$F(u) = \sum _{x=0}^{M-1} f(x) e^{-j \frac{2\pi}{M} u x} \quad, u=0,1,2,…,M-1$$</p>
<p>对于离散非周期信号$\{f_n\}_{0\leq n&lt;M}$,通过<strong>离散时间傅里叶变换</strong>,得到连续周期频域信号:<br>$$F(\omega)=\sum_{n=-\infty}^{\infty} f_n e^{-j\omega n}$$</p>
<p>由于<strong>离散时间傅里叶变换</strong>得到的频域信号是连续的,计算机还是无法处理,因此通常对<strong>离散时间傅里叶变换</strong>连续谱进行等间隔采样,从而转换成<strong>离散傅立叶变换</strong>的离散谱.</p>
<p>计算机无法处理连续值,因此对于一个连续信号,首先通过<strong>冲激函数</strong>进行采样,从而将连续信号离散化.采样会涉及到<strong>采样定理</strong>,即采样频率要大于信号频率的两倍.否则会发生信号<strong>混淆</strong>.取样后的离散信号往往也是非周期的,也就是说得到频域信号是连续的,因此需要通过对连续谱进行等间隔采样,最终得到了计算机可以处理的离散傅立叶变换(DTF).<br>《数字图像处理》第四章的4.1到4.4节就描述了整个过程,具体公式推导参见原书.</p>
<h1 id="图像处理中的傅立叶变换"><a href="#图像处理中的傅立叶变换" class="headerlink" title="图像处理中的傅立叶变换"></a>图像处理中的傅立叶变换</h1><p>对于图像处理,对应的傅立叶变换往往是<strong>二维离散傅立叶变换</strong>:<br>$$F(u,v)=\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} f(x,y) e^{-j2\pi (\frac{ux}{m}+\frac{vy}{N})}$$<br>其中$f(x,y)$表示大小为$M \times N$的数字图像.</p>
<p>未完待续…</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>《数字图像处理》第四章<br><a href="https://zhuanlan.zhihu.com/p/19763358" target="_blank" rel="external">傅里叶分析之掐死教程</a><br><a href="https://zhuanlan.zhihu.com/p/23739221" target="_blank" rel="external">我理解的傅里叶变换</a><br><a href="https://zhuanlan.zhihu.com/p/23607336#!" target="_blank" rel="external">离散傅立叶变换在图像处理中的推导和意义</a></p>
]]></content>
      
        <categories>
            
            <category> 计算机视觉 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数字图像处理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器人领域期刊会议]]></title>
      <url>https://zhehangt.github.io/2017/05/18/RoboticsJournal&amp;Conference/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>最近简单整理了机器人领域的顶级期刊和会议，基本上是对网上其他资料的一个小汇总。未经查实，仅供参考。</p>
<h1 id="期刊"><a href="#期刊" class="headerlink" title="期刊"></a>期刊</h1><p>IEEE Transactions on Robotics (TRO)</p>
<p>International Journal of Robotics Research (IJRR)</p>
<p>Journal of Field Robotics (JFR)</p>
<p>IEEE Robotics &amp; Automation Magazine (RAM)</p>
<blockquote>
<p>以上机器人顶级期刊</p>
</blockquote>
<p>Transactions on Cybernetics (TCY-B)</p>
<p>Transactions on Mechatronics (T-MECH)</p>
<p>Transactions on Automation Science and Engineering (T-ASE)</p>
<blockquote>
<p>以上机器人顶级期刊（偏控制）</p>
</blockquote>
<p>Robotics and Autonomous Systems (RAS)</p>
<p>Autonomous Robots (AURO)</p>
<p>Robotics and Computer-Integrated Manufacturing (RCIM)</p>
<blockquote>
<p>以上机器人次顶级期刊</p>
</blockquote>
<p>International Journal of Advanced Robotic Systems (IJARS)</p>
<p>International Journal of Robotics &amp; Automation</p>
<p>International Journal of Humanoid Robotics</p>
<p>Industrial Robot: An International Journal</p>
<p>Journal of Intelligent and Robotic Systems</p>
<p>Advanced Robotics</p>
<p>Robotica</p>
<blockquote>
<p>以上机器人一般期刊</p>
</blockquote>
<p>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>International Journal of Computer Vision (IJCV)<br>IEEE Transactionson Image Processing (TIP)</p>
<blockquote>
<p>以上计算机视觉顶级期刊</p>
</blockquote>
<h1 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h1><p>Robotics: Science and Systems (RSS)</p>
<p>IEEE The International Conference on Robotics and Automation (ICRA)</p>
<p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS）</p>
<blockquote>
<p>以上机器人顶级会议</p>
</blockquote>
<p>IEEE International Conference on Robotics and Biomimetics (ROBIO)</p>
<p>IEEE International Conference on Real-time Computing and Robotics (RCAR)</p>
<p>International Conference on Advanced Robotics (ICAR)</p>
<p>IEEE International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (IEEE-CYBER)</p>
<p>IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)</p>
<blockquote>
<p>以上机器人次顶级会议</p>
</blockquote>
<p>IEEE International Conference on Advanced Intelligent Mechatronics (AIM)</p>
<p>IEEE International Conference on Humanoid Robots (Humanoids)</p>
<p>International Conference on Climbing and Walking Robots (CLAWAR)</p>
<p>IEEE International Conference on Information and Automation (ICIA)</p>
<blockquote>
<p>以上机器人一般会议</p>
</blockquote>
<p>IEEE International Conference on Computer Vision (ICCV)</p>
<p>Europeon Conference on Computer Vision (ECCV)</p>
<p>Internaltional Conference on Computer Vision and Pattern Recogintion (CVPR)</p>
<blockquote>
<p>以上计算机视觉三大顶级会议</p>
</blockquote>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://blog.sina.com.cn/s/blog_628dd2bc0101eanx.html" target="_blank" rel="external">机器人领域 期刊与会议</a><br><a href="https://www.zhihu.com/question/20224890" target="_blank" rel="external">机器学习领域有哪些著名的期刊和会议? - 知乎</a><br><a href="https://www.zhihu.com/question/37687006" target="_blank" rel="external">计算机视觉顶尖期刊和会议有哪些？ - 知乎</a></p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 杂谈 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Real-Time Loop Closure in 2D LIDAR SLAM 论文笔记]]></title>
      <url>https://zhehangt.github.io/2017/05/01/SLAM/CartographerPaper/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这是关于Google开源的Cartographer SLAM系统的论文。一直在学习视觉SLAM的相关知识，突然上手读这篇基于激光的SLAM论文，有些措手不及。希望能通过论文笔记的方式去数理整篇论文的思路。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>论文的贡献在于：提出了一种新颖的基于激光数据的回环检测方式，这种方式可以减少计算量，满足实时的大场景地图构建以及大规模的实时优化的性能需求。<br>激光数据往往被认为信息量少，难以用于回环检测，因此这个工作还是非常nice的。</p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>对于scan matching方面了解较少，所以将这方面的论文都列出来，方面以后的进一步学习。<br>相关工作中首先介绍了scan matching的几种方法：<br>1.scan-to-scan matching 是基于激光的SLAM中最常用来估计相关位姿的方法。但是非常容易造成累积误差。[1,2,3,4]<br>2.scan-to-map matching 可以减少累积误差，因为scan-to-map每次都通过高斯-牛顿法获得局部最优的位姿，前提是要有一个比较好的初始化位姿估计。[5]<br>3.pixel-accurate scan matching 可以进一步减少局部误差，但是计算量比较大。这个方法同样可以用来检测回环。<br>4.从laser scan中提取特征，从而减少计算量[4]。histogram-based matching用于回环检测[6]。用机器学习做laser scan的特征检测[7]。</p>
<p>之后介绍了处理累积误差的两种方式。<br>1.基于粒子滤波的优化。粒子滤波在处理大场景地图时，由于粒子数的极具增长造成资源密集。粒子滤波这一块我了解不多，论文中给出了给了几个相关论文。[8,9,10]<br>2.基于位姿图的优化。与视觉SLAM的位姿图优化大同小异，主要是在观测方程上的区别。论文中给的相关论文[2,11,12,13]。</p>
<blockquote>
<p>感觉作者写相关工作写的好敷衍。其实我觉得，整篇论文都好敷衍，公式多解释少。</p>
</blockquote>
<h1 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a>System Overview</h1><p>Cartographer 能产生一个精确度为5cm的2D栅格地图。精确度为5cm是因为组成submap是一系列5cm×5cm的栅格。每次laser scan会通过当前的submap进行scan matching，从而将laser scan插入到submap中的最优估计位置。然后再用laser scan去更新submap。submap是通过与当前laser scan通过邻近的多个laser scan更新产生了，因此scan matching本质上也是在当前laser scan和多个邻近laser scan之间进行的。<br>通过scan matching得到的位姿估计在短时间内是可靠的，但是长时间会有累积误差，为了优化这个误差会有规律的执行pose optimization。当不再有新的scan插入到一个submap，就认为这个submap已经创建完成了。所有创建完成的submap以及scans都会用作回环检测的scan matching。如果有scans和submap在距离上足够的近，则scan matcher会尝试寻找回环can matching。为了减少计算量，Cartographer设置了特殊的策略来找到回环scan matching。这个策略就是根据当前的位姿估计附近设置搜索窗口，在这个搜索窗口内执行branch-and-bound方法来寻找回环scan matching，如果找到了一个足够好的match，则会将该匹配的闭环约束加入到位姿优化问题中。</p>
<p>论文中说回环优化的速度非常快，快到可以在加入新的submap之前就完成优化，从而保证了一种软实时。优化这么快是因为用了 branch-and-bound approach 以及submap中栅格的预计算。</p>
<h1 id="Local-2D-SLAM"><a href="#Local-2D-SLAM" class="headerlink" title="Local 2D SLAM"></a>Local 2D SLAM</h1><p>Cartographer通过 local 和 global 两种方式进行SLAM。<br>local方式就是通过submap进行scan matching在这一章。<br>global方式就是回环检测在下一章。</p>
<p><strong>A.Scans</strong><br>一个scans包含一个起点和很多个终点的。起点称为origin，终点称为scan points，用$H= \{ h_k \}_ {k=1,…,k}$。在scan坐标系下，origin就是坐标原点，scan points就是在scan坐标系下的坐标。当把一个scans插入到一个submap中时，假设submap坐标系到scan坐标系的坐标转换为$T_{\xi}$，即激光传感器在submap坐标系下的位姿。则每个$h$在submap坐标系下的坐标为：$T_{\xi}h$</p>
<p><strong>B.Submaps</strong><br>一个submap是通过几个连续scans创建，是由5cm×5cm大小的概率栅格$[p_{min},p_{max}]$(submap创建完成时的栅格概率如果小于$p_{min}$表示未被占用，在$p_{min}$和$p_{max}$之间表示未知，大于$p_{max}$表示占用)。每个submap的中心称为grid point，submap中的其他点称为corresponding pixel。<br>对于每个scan，都会产生一组称为hits的grid point和一组称为misses的grid point。如下图所示。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/CartographerPaper-01.png"><br>其中阴影带叉的是hits，阴影不带叉的是misses。<br>每个hits中的grid point被赋予初始值$p_{hit}$，每个misses中的grid point被赋予初始值$p_{miss}$<br>如果grid point已经有$p$值,则用下述公式更新hit。</p>
<p>$odds(p)=\dfrac{p}{1-p}$<br>$M_{new}(x)= clamp(odds^{-1}(odds(M_{old}(x)) \cdot odds(p_{hit}) ))$</p>
<p>miss也是类似的。</p>
<p><strong>C.Ceres scan matching</strong><br>把scan插入到submap之前，需要通过scan matching对$T_{\varepsilon}$进行优化。这里的优化问题为非线性最小二乘问题，通过Ceres库进行求解，非线性最小二乘问题构造形式如下式：<br>$$<br>\underset {\xi}{argmin} \sum_{k=1}^K(1-M_{smooth}(T_{\xi}h_k))^2 \qquad  (\mathrm{cs})<br>$$<br>其中$M_{smooth}$函数完成了从2D到1D的平滑函数。这里用的是 bicubic interpolation 方法。<br>对于优化问题，一个相对精确的初始估计非常重要。因此如果通过IMU提供角度信息，可以提高优化的准确性。在缺少IMU的情况下，高频率的matching或者pixel-accurate scan matching也可以提高准确性，但会增加时间复杂度。</p>
<blockquote>
<p>有几个问题感觉论文中没说清楚，一个submap的大小和形状是怎么样的，当scan的origin不在当前的submap内，但是points在submap中时是怎么更新和匹配submap的，这些疑问还需要结合源代码做进一步的分析。</p>
</blockquote>
<h1 id="Closing-Loops"><a href="#Closing-Loops" class="headerlink" title="Closing Loops"></a>Closing Loops</h1><p>Cartograoher通过创建大量的小submap来实现大场景的建图。为了消除小submap带来的累积误差，通过优化所有scan和submap的位姿，来提高准确度。优化方法参考了论文[2]。</p>
<p>A. Optimization problem<br>回环的优化问题与scan matching的优化问题类似，都是通过构造非线性最小二乘的方式进行的。形式如下：<br>$$\underset {\Xi^m,\Xi^s }{argmin} \dfrac{1}{2} \sum_{ij} \rho( E^2({\xi}_i^m, {\xi}_j^s; \Sigma_{ij}, {\xi}_{ij} ) ) \qquad  (\mathrm{spa})<br>$$<br>其中$\Xi^m = \{ {\xi}_i^m  \}_{i=1,..,m}$是submap的位姿，$\Xi^s= \{ {\xi}_j^s  \}_{j=1,..,s}$是scan的位姿，这些位姿是在世界坐标系下的。submap位姿和scan位姿之间存在约束条件。${\xi}_ij$表示scan在submap坐标系下的位姿，$\Sigma_{ij}$是相应的协方差矩阵，这个协方差矩阵可以通过[14]的方式获得，也可以通过$(\mathrm{cs})$公式获得。<br>残差E的计算公式如下：<br>$$<br>E^2(\xi_i^m, \xi_j^s;\Sigma_{ij},\xi_{ij})=e(\xi_i^m, \xi_j^s;\xi_{ij})\Sigma_{ij}^{-1} e(\xi_i^m, \xi_j^s;\xi_{ij})  \\<br>e(\xi_i^m, \xi_j^s;\xi_{ij}) = \xi_{ij} - \begin{pmatrix}<br>R^{-1}_{\xi_{i}^m}(t_{\xi_{i}^m} - t_{\xi_{j}^s})  \\<br>\xi_{i;\theta}^m - \xi_{j;\theta}^s<br>\end{pmatrix}<br>$$</p>
<blockquote>
<p>论文读到这里，实在是不明白这个公式怎么就约束回环了。经过一些自己的思考，大概理了下思路：在Local 2D SLAM中所有的约束都是scans在某一个submap中得到，也就是公式CS。但是仅靠scans在某一个submap中的约束是无法进行回环的。要进行回环优化必须添加回环约束。我认为这里的回环约束就是要找到scans在多个submap中的约束。实际上当前的scans可能可以观察到多个submap，也就是说当前的scans是可以被多个submap定位的，但是为了简化，在Local 2D SLAM中，只通过当前的submap来对scans进行定位。而loop scan matching就是尝试在其他的submap对scans进行定位，如果能够在其他submap中定位成功，那么就是一个回环约束。至于到底是不是这样的，还需要分析源码来进行验证，论文是看晕了。</p>
</blockquote>
<p>如果上述的解释是正确的。那么对残差公式就有了这样的解释。submap的位姿$\xi_i^m$和scan的全局位姿$\xi_j^s$是在Local 2D SLAM中得到的，其中$\xi_j^s$是由Local 2D SLAM中对该scan进行定位的submap的位姿计算相应的坐标计算得到的。$\xi_{ij}$是在回环scan matching中得到的。整个优化过程就是调整scan matching到的submap的位姿和对该scan进行Local定位的submap的位姿，从而最终得到全局一致性的地图。</p>
<p>B.Branch-and-bound scan matching<br>之前提到的回环约束关系$\xi_{ij}$就是通过这里的方法得到的，也是整篇论文最核心的地方。<br>首先看一下pixel-accurate match的匹配过程。<br>$$<br>\xi^* = \underset{\xi \in W}{argmax} \sum_{k=1}^K M_{nearest}(T_{\xi}h_k) \qquad  (\mathrm{BBS})<br>$$<br>其中$W$是搜索空间，$M_{nearest}$就是该pixel对应的grid point的M值。之后可以通过CS公式进一步提高$\xi$匹配的准确度。</p>
<p>搜索空间和搜索步长的选择是决定pixel-accurate match是否高效的关键。<br>论文给出了搜索步长的计算方式。<br>$$<br>d_{max} = \underset{k=1,…K}{max} \left \| h_x \right \|  \\<br>\delta_{\theta} = arccos (1-\dfrac{r^2}{2d_{max}^2}) \\<br>w_x = \left \lceil \dfrac{W_x}{r} \right \rceil \quad w_y = \left \lceil \dfrac{W_y}{r} \right \rceil \quad w_{\theta} = \left \lceil \dfrac{W_{\theta}}{\delta_{\theta}} \right \rceil<br>$$<br>其中$W_x = W_y=7m$，$W_{\theta}$，因此搜索空间就可以确定了。此时搜索空间的大小是7m*7m。<br>$$<br>\overline{W} = \{ -w_x,…,w_x \} \times \{ -w_y,…,w_y \} \times \{-w_{\theta},…,w_{\theta} \} \\<br>W = \{ \xi_0 + (rj_x, rj_y, \delta_{\theta}j_{\theta}):(j_x,j_y,j_{\theta}) \in \overline{W}  \}<br>$$<br>有了搜索空间和搜索步长，就可以得到最原始的暴力搜索方式。如下图所示:<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/CartographerPaper-02.png"></p>
<p>为了进一步提高搜索效率，Cartograoher采用了branch and bound approach的方式。branch and bound approach是一种在问题的解空间树上搜索问题的解的方法，被Google套用在最优位姿的搜索中,从而将无法实时化暴力解优化到可以满足实时化，不得不说真是厉害。所以学好算法是多么重要。<br>至于branch and bound approach到底是怎么做的，还是看论文吧。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] E. Olson, “M3RSM: Many-to-many multi-resolution scan matching,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), June 2015.<br>[2] K. Konolige, G. Grisetti, R. Kümmerle, W. Burgard, B. Limketkai, and R. Vincent, “Sparse pose adjustment for 2D mapping,” in IROS, Taipei, Taiwan, 10/2010 2010.<br>[3] F. Lu and E. Milios, “Globally consistent range scan alignment for environment mapping,” Autonomous robots, vol. 4, no. 4, pp. 333–349, 1997.<br>[4] F. Martı́n, R. Triebel, L. Moreno, and R. Siegwart, “Two different tools for three-dimensional mapping: DE-based scan matching and feature-based loop detection,” Robotica, vol. 32, no. 01, pp. 19–41,2014.<br>[5] S. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf, “A flexible and scalable SLAM system with full 3D motion estimation,” in Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR). IEEE, November 2011.<br>[6] M. Himstedt, J. Frost, S. Hellbach, H.-J. Böhme, and E. Maehle, “Large scale place recognition in 2D LIDAR scans using geometrical landmark relations,” in Intelligent Robots and Systems (IROS 2014),2014 IEEE/RSJ International Conference on. IEEE, 2014, pp. 5030–5035.<br>[7] K. Granström, T. B. Schön, J. I. Nieto, and F. T. Ramos, “Learning to close loops from range data,” The International Journal of Robotics Research, vol. 30, no. 14, pp. 1728–1754, 2011.<br>[8] G. Grisetti, C. Stachniss, and W. Burgard, “Improving grid-based SLAM with Rao-Blackwellized particle filters by adaptive proposals and selective resampling,” in Robotics and Automation, 2005. ICRA 2005. Proceedings of the 2005 IEEE International Conference on. IEEE, 2005, pp. 2432–2437.<br>[9] G. D. Tipaldi, M. Braun, and K. O. Arras, “FLIRT: Interest regions for 2D range data with applications to robot navigation,” in Experimental Robotics. Springer, 2014, pp. 695–710.<br>[10] J. Strom and E. Olson, “Occupancy grid rasterization in large environments for teams of robots,” in Intelligent Robots and Systems (IROS),2011 IEEE/RSJ International Conference on. IEEE, 2011, pp. 4271–<br>4276.<br>[11] R. Kümmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard,“g2o: A general framework for graph optimization,” in Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEE,2011, pp. 3607–3613.<br>[12] L. Carlone, R. Aragues, J. A. Castellanos, and B. Bona, “A fast and accurate approximation for planar pose graph optimization,” The International Journal of Robotics Research, pp. 965–987, 2014.<br>[13] M. Bosse and R. Zlot, “Map matching and data association for large-scale two-dimensional laser scan-based SLAM,” The International Journal of Robotics Research, vol. 27, no. 6, pp. 667–691, 2008.<br>[14] E. B. Olson, “Real-time correlative scan matching,” in Robotics and Automation, 2009. ICRA’09. IEEE International Conference on. IEEE, 2009, pp. 4387–4393.</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Cartographer </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ORB-SLAM2的Rviz可视化方案]]></title>
      <url>https://zhehangt.github.io/2017/04/27/SLAM/ORBSLAM/ORBSLAM2Rviz/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>ORB-SLAM的地图和定位可视化是通过Rviz进行展示的，而在ORB-SLAM2中，为了不依赖于ROS，ORB-SLAM2的可视化采用了pangolin库。而我的硕士课题有个需求，就是要使ORB-SLAM2的结果和Cartographer的结果同时显示在同一个可视化工具中，而Cartographer是采用Rviz显示的，还定制了专用的Rviz插件。思考再三，决定为ORB-SLAM2重新添加基于Rviz的可视化模块。</p>
<h1 id="ORB-SLAM2的Rviz可视化"><a href="#ORB-SLAM2的Rviz可视化" class="headerlink" title="ORB-SLAM2的Rviz可视化"></a>ORB-SLAM2的Rviz可视化</h1><p>简单分析一下ORB-SLAM中关于Rviz的可视化，总结了以下几点：<br>1.ORB-SLAM的Rviz视化用了单独的一个类来完成可视化信息的发布：MapPublisher类<br>2.所有的可视化信息都是Rviz的Mark类型，根据发布的地图点，关键帧，Covisibility Graph，Spanning Tree和相机轨迹，设计了不同的Mark类型。<br>3.所有的可视化信息，包或地图，轨迹等都是从ORB-SLAM中的Map类中获取的。<br>4.每次获得一帧图像，进行Track后，利用MapPublisher类发布可视化信息。<br>5.在配置相应的Rviz，使其可以接收可视化信息。</p>
<p>明白了这几点之后，在ORB-SLAM2中添加Rviz可视化模块就很简单了，主要对源代码做两个改动。<br>1.添加MapPublisher类和配置Rviz，并在每次Track之后利用MapPublisher类发布可视化信息，这里可以直接复用ORB-SLAM中的MapPublisher类和Rviz。<br>2.为Map类添加返回相关信息的接口。<br>3.特别要注意ORB-SLAM2的坐标系下，z轴是朝前的，而Rviz的坐标系下，z轴是朝上的，因此要做相应的转换。</p>
<p>通过以上几个修改，就能在Rviz中显示ORB-SLAM2的地图构建结果和相机位姿了。简单到让我很佩服ORB-SLAM2代码的简洁和松耦合。</p>
<h1 id="ORB-SLAM2的坐标系转换"><a href="#ORB-SLAM2的坐标系转换" class="headerlink" title="ORB-SLAM2的坐标系转换"></a>ORB-SLAM2的坐标系转换</h1><p>在对ORB-SLAM2进行实验的过程中，我采用的喜闻乐见的Turtlebot。我们知道ORB-SLAM2的地图构建和定位都是基于自己的map坐标系的，与Turtlebot的坐标系并不产生任何关联。但是我希望将map坐标系融入到Turtlebot的坐标系中，方便之后针对Turtlebot对ORB-SLAM2进行修改和扩展。<br>首先简单了解下Turtlebot的坐标系。如下图所示。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBSLAM2Rviz-01.png"><br>Turtlebot的坐标系遵循<strong>odom — base_link — other_link</strong>的标准格式。<strong>other_link</strong>中包含相机的坐标系<strong>camera_rgb_frame</strong>。</p>
<p>因此要将ORB-SLAM2的map坐标系融合到Turtlebot坐标系中，即形成<strong>map — odom — base_link — other_link</strong>的标准格式，需要计算<strong>map — odom</strong>之间的转换关系。<br>ORB-SLAM2得到的是在map坐标系下相机的位姿，也就是说<strong>map — camera_rgb_frame</strong>之间的转换是已知的， 在配合上<strong>odom — camera_rgb_frame</strong>之间的转换关系，<strong>map — odom</strong>之间的转换关系就不难求解了。具体计算方法可以看看以下的伪代码。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//根据世界坐标系map下相机的位姿和里程计坐标系odom下相机的位姿，计算相机坐标系map和里程计坐标系odom之间的旋转矩阵和平移向量</span></div><div class="line"></div><div class="line">r_map_camera = r_map_odom * r_odom_camera</div><div class="line">--&gt; r_map_odom = r_map_camera * r_odom_camera^(<span class="number">-1</span>);</div><div class="line"></div><div class="line"><span class="comment">// C_m和C_o表示相机在map坐标系下和odom坐标系下的坐标</span></div><div class="line">C_m = r_map_odom * C_o + t_map_odom</div><div class="line">C_m = r_map_camera * [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] + t_map_camera  = t_map_camera</div><div class="line">C_o = r_odom_camera * [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] + t_odom_camera  = t_odom_camera</div><div class="line">--&gt; t_map_odom = t_map_camera - r_map_odom * t_odom_camera;</div><div class="line"><span class="comment">// r_map_odom 和 t_map_odom就是map坐标系和odom坐标系之间的转换关系</span></div></pre></td></tr></table></figure></p>
<p>根据以上的思路，在配合ROS中的TF包，很快就能完成map坐标系和Turtlebot坐标系之间的融合。以后如果要查找map坐标系和Turtlebot其他坐标系之间的转换关系，比如深度相机坐标系，IMU的坐标系等，就可能借助TF完成非常快速和方便的查询。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>感觉没啥能总结的。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ORB-SLAM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ORB-SLAM2:an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras 论文笔记]]></title>
      <url>https://zhehangt.github.io/2017/04/24/SLAM/ORBSLAM/ORBSLAM2Paper/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>针对单目相机无法直接获得景深信息的缺点，作者在ORB-SLAM只支持单目相机的基础上添加了双目相机和RGBD相机的支持，形成了ORB-SLAM2。这篇博客是ORB-SLAM2的论文笔记。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>单目相机具有成本低，设置简单的优点，但是同样存在尺度不确定、初始化需要足够视差，最终造成尺度漂移、无法处理纯旋转等问题。而通过双目相机和RGBD相机，以上问题都可以得到很好的解决。<br>ORB-SLAM2为SLAM的发展作出了以下几个<strong>贡献</strong>：<br>1.这是第一个同时提供单目，双目和RGB-D接口的SLAM开源系统，并且包含回环检测，重定位和地图重用。<br>2.通过BA对RBG-D进行优化，效果优于state-of-the-art的ICP或photometric and depth error minimization<br>3.通过使用近距离和远距离的双目点以及单目观测，使得其双目的精确度要高于state-of-the-art的直接使用双目的SLAM系统<br>4.通过禁用建图来实现利用已有地图，进行轻量级的定位。</p>
<h1 id="ORB-SLAM2"><a href="#ORB-SLAM2" class="headerlink" title="ORB-SLAM2"></a>ORB-SLAM2</h1><p>ORB-SLAM2的框架和ORB-SLAM的框架几乎是一模一样的，包含了三个线程：Tracking，Local Mapping和Loop Closing。主要的区别还是在对于双目相机和RBG-D相机前端，是如何进行追踪并构建后端优化问题的。因此与ORB-SLAM相同的部分就不再赘述，详细可参考上一篇关于ORB-SLAM论文的博客。</p>
<p><strong>A.Monocular, Close Stereo and Far Stereo Keypoints</strong><br>ORB-SLAM2是基于特征的SLAM系统，因此当从输入的图像中提取特征之后，图像不需要被保存而是直接丢弃，因此可以说ORB-SLAM2与传感器之间是相互独立的，重要的还是特征提取的过程，如图所示。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBSLAM2Paper-01.png"></p>
<p><strong>Steroe keypoints</strong><br>Steroe Keypoints用三维坐标$X_s=(u_L, v_L, v_R)$来定义。其中$(u_L, v_L)$表示左边图像的像素点坐标，$v_R$表示右边图像的横坐标。<br>对于<strong>双目相机</strong>，首先从左边图像中提取ORB特征，然后在右边图像中寻找匹配，通过这种方式可以快速的找到一系列的Steroe keypoints。<br>对于<strong>RGB-D相机</strong>，从RGB图像中提取ORB特征，此时提取的ORB特征的像素坐标作为左边图像的像素坐标，然后根据每个特征对应的深度信息恢复出该ORB特征在右边图像的像素坐标，从而得到Steroe Keypoints。基线设置为8cm。<br>一个Keypoint会被分为far和close两种。如果Steroe Keypoints对应的深度小于基线的40倍则为close，反之则为far。close的关键点可以从一帧图像中直接三角化得到准确的scale，translation和rotation信息，因为其深度信息是可靠的。而far的关键点可以提供准确的rotation信息，而scale和translation信息并不可靠，因此需要通过多视图方式来对远距离的关键点进行三角化。<br><strong>Monocular keypoints</strong><br>Monocular keypoints用二维坐标$X_m=(v_L,v_L)$来定义。对于无法提供深度信息的关键点，只能通过多视图的方式进行三角化，但无法恢复scale信息，只能恢复出rotation和translation信息。</p>
<p><strong>B.System Bootstrapping</strong><br>使用双目或者RGB-D相机不需要像单目相机那样进行复杂的初始化，而只需要把第一帧作为关键帧，得到初始地图。</p>
<p><strong>C.Bundle Adjustment with Monocular and Stereo Constraints</strong><br>论文中提到了三种BA方式：motion-only BA，local BA和full BA。与ORB-SLAM中提到的BA方式也是一一对应的。<br>ORB-SLAM2中的BA增加了从地图点到Steroe keypoint的投影方式，从而使Steroe keypoint可以直接作为误差计算对象出现在BA中。</p>
<p><strong>D. Loop Closing and Full BA</strong><br>使用双目或者RGB-D相机不会出现尺度漂移的问题，因此在对回环候选帧进行位姿优化时，不再需要使用sim3相似变换，使用so3刚体变换就可以了。而进行full BA的闭环优化时，鉴于其计算量可能会比较大，因此在这里会新开一个线程专门处理full BA，这里会涉及到full BA过程中如果有新的回环被检测到该怎么办的问题。论文阐述的做法是直接停止当前正在进行的full BA，闭合最新的回环和full BA。新开线程处理full BA还会存在full BA完成时，如何把full BA的结果与full BA过程中信添加的关键帧和地图点融合，论文提到的做法是full BA执行过程中，先不把新的关键帧和地图点加进来，而是等full BA完成时，根据优化结果对新的关键帧和地图点做一次矫正，然后再进行关键帧和地图点的添加。</p>
<p><strong>E. Keyframe Insertion</strong><br>ORB-SLAM2的关键帧添加策略与ORB-SLAM基本保持一致。唯一的区别就是基于Steroe Keypoint添加了一个条件。在大尺寸的场景中，只有存在足够多的close Steroe Keypoint才能保证位姿评估的准确性，因此在Tracking过程中，两帧之间匹配的close Steroe Keypoint少于100时，并且当前帧可以提取多余70个close Steroe Keypoint时，当前帧就可以作为关键帧进行添加。这种策略对于大场景下的定位非常关键。</p>
<p><strong>F. Localization Mode</strong><br>ORB-SLAM2在ORB-SLAM的基础上添加了定位模式，使其可以利用已有的地图进行高效的定位。在这种模式下Local Mapping和Loop Closing线程被停止，只通过Tracking线程对相机位姿进行追踪。用到的就是用两帧之间的特征匹配做motion-only BA，以及根据已有地图做local BA。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>可以说ORB-SLAM2是非常优秀并且易于学习的SLAM系统，它涉及了基于视觉特征的SLAM的方方面面。作者最后也提到了基于ORB-SLAM2的未来工作，包括多相机融合，鱼眼相机的支持，生成大尺度的稠密地图以及提高系统鲁棒性等。最后我希望自己未来能够在这些方面作出贡献。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Mur-Artal, Raul, and Juan D. Tardos. “ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras.” arXiv preprint arXiv:1610.06475 (2016).APA    </p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ORB-SLAM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ORB-SLAM:a Versatile and Accurate Monocular SLAM System 论文笔记]]></title>
      <url>https://zhehangt.github.io/2017/04/20/SLAM/ORBSLAM/ORBSLAMPaper/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>关于ORB-SLAM，想必每个SLAM领域的入门者都会学习一番。过去几个月，我也在基于ORB-SLAM2的开源代码做硕士课题。ORB-SLAM的论文[1]因此也看了很多遍。网上已经有关于这篇论文的中文翻译，但有些地方由于每个人的表达方式不同，并不是很好理解。因此在此也用自己的表达方式对ORB-SLAM的论文进行简单记录。<br>当然也很有可能，这篇博客只有我自己可以看懂。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>论文是从Bundle Adjustment(BA)对于SLAM的作用展开的。通过BA可以为相机定位和地图构建提供更精确的评估。关于BA在博客<a href="https://zhehangt.github.io/2017/03/07/SLAM/PnP/">3D-2D的运动估计</a>里做过一次简单的介绍。以前BA由于计算量比较大，因此被认为无法用于实时SLAM，特别是视觉SLAM，但是随着移动计算能力的快速发展，BA已经可以在视觉SLAM中达到实时的效果。要运用BA，相应的视觉SLAM算法必须满足以下几个方面：<br>1.要对关键帧之间的特征进行匹配以及地图点和特征之间进行关联<br>2.为了防止计算量过大，要去除冗余的关键帧<br>3.关键帧和地图点之间的匹配和关联要尽可能的稠密，也就是说关键帧中的观察到的地图点能够提供足够多的视差以及足够的回环<br>4.要提供一个尽可能准确的初始值用于BA的非线性优化<br>5.对于局部地图，优化只在局部进行，与全局尺寸无关，从而具备更好的可扩展性<br>6.对于全局地图，要有回环检测和全局优化</p>
<p>其实从ORB-SLAM的很多设计方式上看，都是奔着满足上面这几个方面去的，也算BA运用到视觉SLAM中的一个简单的指导。</p>
<p>ORB-SLAM的成功离不开第一个基于特征的视觉SLAM：PTAM[2]。ORB-SLAM基本延续了PTAM的基本思路，在PTAM的基础上做了很多方面的改进。<br>ORB-SLAM主要有以下几点优点：<br>1.在所有的模块中都采用相同的图像特征。这些模块包括追踪，建图，重定位，回环，因此使得我们的系统更加的简单，高效和可靠。通过使用ORB特征，使得可以在不采用GPU的情况下实时运行，并在不同的视角和光照下都具有良好的不变性。<br>2.能够实时运行在大场景的环境中。通过使用Covisibility Graph，追踪和局部地图构建都在Covisibility Graph的一部分中处理，因此与全局的地图尺寸无关。<br>3.实时的回环检测的优化是基于Essential Graph进行的，Essential Graph是通过生成树树进行维护，是基于Covisibility Graph强关联边和回环边构建的。<br>4.实时的重定位，能够从追踪丢失中恢复追踪以及在已有地图中进行定位。<br>5.地图初始化时可以根据是否为平面自动选择相应的模型。<br>6.对于地图点和关键帧的选择采用一种survival of the fittest的方法，生成时放宽尺度但选择时提高要求。这种策略可以剔除冗余的关键帧，从而增强追踪的鲁棒性以及长时间运行的能力。</p>
<h1 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a>System Overview</h1><p><strong>A.Feature Choice</strong><br>我们设计的主要思想之一就是用于建图和追踪的特征可以同时用于重定位和回环检测。这使得我们的系统更有效率。选择ORB特征[3]是因为其计算和匹配速度快，具备旋转不变性。ORB特征已经在位置识别中取得了很好的性能[4]。<br><strong>B. Three Threads: Tracking, Local Mapping and Loop Closing</strong><br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ORBSLAMPaper-01.png"></p>
<p>如图1所示，系统共有Tracking，Local Mapping和Loop Closing三个线程在同时运行。<br>Tracking：基于每一帧来对地图进行定位，并会决定什么时候插入新的关键帧。通过与前一帧进行特征匹配来对当前帧的位姿进行初始化估计，然后用motion-only BA对位姿作进一步优化。当Tracking丢失时通过位置识别来进行全局的定位。完成了与前一帧的匹配和位姿估计之后，可以通过得到的位姿将与其他关键帧进行特征匹配，也就是从一个局部地图里面进行搜索匹配使相机位置得到进一步优化。最后Tracking线程来判断当前帧是不是关键帧。</p>
<p>Local Mapping：通过处理关键帧和执行local BA，从而得到一个较优的地图。对于新的关键帧，会Covisibility Graph寻找更多的特征匹配，并三角化得到新的地图点。对于新的地图点会进行严格筛选，Local Mapping也同样负责删除一些冗余帧。</p>
<p>Loop Closing：对每个新的关键帧都要进行闭环检测，以确认是否有闭环。如果闭环被检测到，通过计算相似变换来得到闭环的累积误差。然后对齐闭环处的两帧并融合重复的地图点。最后，执行有相似约束的位姿图优化[5]，消除回环误差，确保全局地图的一致性。这里的优化是基于Essential Graph进行，它是Covisibility Graph的子图。</p>
<p>所有的优化都是基于g2o实现的 Levenberg-Marquardt 算法。</p>
<p><strong>C.Map Points, KeyFrames and their Selection</strong><br>每个地图点$p_i$包含：<br>1.世界坐标系下的3D坐标$X_{w,i}$<br>2.视图方向$n_i$，对于每个能观察到该地图点的关键帧的中心到地图点有一个视图方向，$n_i$是所有能观察到该地图点的关键帧的视图方向的平均值，用单位向量表示<br>3.ORB特征描述子$D_i$，对于所有能够观察到该地图点的关键帧都计算一个ORB特征描述子，$D_i$是其中汉明距离最小的。<br>4.根据ORB特征尺度不变性的约束，计算出能观测到该点的最大距离$d_{max}$和最小距离$d_{min}$</p>
<p>每个关键帧$K_i$包含：<br>1.相机的位姿$T_{i,w}$，也是从世界坐标系转换到相机的坐标系的刚体变换矩阵<br>2.相机内参，包括焦距和相机主点<br>3.所有从图像帧提取的ORB特征，不管是否已经关联了地图点，都通过畸变模型进行矫正</p>
<p>地图点和关键帧通过宽松策略创建，而剔除机制会非常严格，剔除冗余的关键帧和错误匹配或不可追踪的地图点。从而使运行期间地图具备扩展性，增强在极端环境下追踪的鲁棒性。</p>
<p><strong>D.Covisibility Graph and Essential Graph</strong></p>
<p>关键帧之间的视图内容关联信息(Covisibility information)在系统的几个模块上都非常有用,关联信息用权重图像间接地表示。每个节点都是一个关键帧，如果两个关键帧可以观察到相同的地图云点，根据地图云点数目赋予边权重值$\theta$。</p>
<p>我们通过位姿图优化来纠正回环。为了防止Covisibility Graph非常稠密，我们通过建立Essential Graph来保留所有的关键帧节点，但是只保留少量的边，从而保留足够的信息来产生精确的优化结果。系统从第一个关键帧开始增量式地构建一个生成树，它是Covisibility Graph中边数最少的连通子图。当新的关键帧插入时，它加入树中，并连接到能够观察到相同地图点最多的关键帧上。当一个关键帧通过筛选策略被删除时，系统会根据关键帧所在的位置更新生成树。Essential Graph包含了生成树，是保留了Covisibility Graph中高度关联(θmin = 100)的边和回环边的子图，构成强连通的关键帧连接图。</p>
<p><strong>E.Bags of Words Place Recognition</strong><br>基于DBoW2[7]，系统嵌入了基于图像词袋的位置识别模块，完成闭环检测和重定位功能。视觉单词是离散化的描述子空间，视觉单词组成视觉字典。视觉字典是离线创建的，创建视觉字典的ORB特征描述子是从大量图像中提取的。如果图像足够多，同一个视觉字典可以用于多个不同的环境。系统增量式的构建一个包含倒序索引的数据库，用来存储视觉单词，从而可以高效地检索关键帧。当关键帧通过筛选机制被删除时，数据库也会更新。</p>
<p>关键帧在视图上可能会存在重叠，检索数据库时，可能不止一个高分值的关键帧。DBoW2认为是图像重叠的问题，就将时间上接近的所有图像的分值相加。但这样并没有考虑同一个地方的关键帧在不同时刻被观察到的情况。因此我们依据关键帧是否是视图关联的进行分类。另外，我们的数据库返回的是分值高于最好分值75%的所有关键帧。</p>
<p>利用词袋模型还可以加速特征匹配。详见[4,7]。</p>
<h1 id="Automatic-Map-Initialization"><a href="#Automatic-Map-Initialization" class="headerlink" title="Automatic Map Initialization"></a>Automatic Map Initialization</h1><p>由于单目相机无法直接得到景深信息，因此需要通过多张图像来恢复环境的三维结构，这个过程通常被称为地图初始化。地图初始化需要两帧之间有足够的视差，基于视差计算出两帧之间的位姿变换，并通过三角化得到初始的地图点。依据观察到的场景是否是平面的，单目相机的初始化通常分为两种方法：基于单应矩阵的平面初始化和基于基础矩阵的非平面初始化。ORB-SLAM的自动初始化就是同时计算这两种模型，然后用启发式的方法选择最合适的模型来完成初始化。主要的计算流程如下。</p>
<p><strong>1.Find initial correspondences</strong><br>从当前帧$F_c$中提取ORB特征，并与参考帧$F_r$做特征匹配$x_c \leftrightarrow x_r$。如果没有找到足够多的匹配特征，就重设参考帧。</p>
<p><strong>2.Parallel computation of the two models</strong><br>在两个线程中并行的计算单应矩阵$H_{cr}$和基础矩阵$F_{cr}$:<br>$$<br>x_c = H_{cr}x_r  \qquad x_c^TF_{cr}x_r=0<br>$$<br>单应矩阵的计算方法是normalized DLT，基础矩阵的计算方法是8点法，具体计算方法见神书多视图几何[8]。</p>
<p>为了公平比较单应矩阵和基础矩阵哪个更好，论文设计了一些技巧，比如求解时设定相同的迭代次数，并且用于且解的特征匹配点也是相同的。<br>对于每一次迭代，都计算一次symmetric transfer errors，基于这个误差求得单应矩阵和基础矩阵的最终得分$S_H$和$S_F$。具体计算过程可以参见原文。</p>
<p><strong>3.Model selection</strong><br>如果在平面场景选择了基础矩阵进行初始化，得到初始化的地图往往是错误。为了能够在平面场景时选择单应矩阵，在非平面场景时选择基础矩阵，通过计算$R_H = \dfrac{S_H}{S_H+S_F}$,当$R_H &gt; 0.45$时,选择单应矩阵，反之选择基础矩阵。这是一种启发式的方式。</p>
<p><strong>4.Motion and Structure from Motion recovery</strong><br>当模型被选定后，就可以通过得到的矩阵计算相关的运动假设。对于单应矩阵可以恢复出8种假设。对于每种假设匹配好的特征点进行验证。只有当三角化得到的地图点都在相机的前方并且重投影误差足够小，才认为本次的地图初始化是成功的。对于基础矩阵也是如此。</p>
<p><strong>5.Bundle adjustment</strong><br>执行一次full BA。除了第一帧不变之外，所有关键帧和地图点都会称为BA优化的对象。</p>
<h1 id="Tracking"><a href="#Tracking" class="headerlink" title="Tracking"></a>Tracking</h1><p>Tracking线程用来处理从相机中得到的每一帧图像。对于每一帧图像，Tracking线程会通过如下几个步骤进行处理。</p>
<p><strong>1.ORB Extraction</strong><br>提取ORB特征涉及到几个参数。第一个参数是scale levels，第二个参数是scale factor。这两个参数应该是用来建立图像金字塔，提高ORB特征的尺度不变性。第三个参数是提取ORB的特征数。对于512 × 384 到 752 × 480分辨率，提取1000个。对于更大的分辨率，提取2000个。为了保证提取的ORB特征能够均匀分布，通过将每个scale level的图像进行网格划分，并在每个网格中提取至少5个特征点作为阈值。根据每个网格中实际提取的特征点数，会对阈值进行相应的调整。最后会计算ORB描述子。</p>
<p><strong>2.Initial Pose Estimation from Previous Frame</strong><br>如果上一帧的追踪成功，就用相同的速率运动模型进行位姿的估计，然后从前一帧看到的地图点云与当前帧做匹配，根据匹配结果进行位姿优化。这里被称为motion BA。如果匹配失败，就加大搜索范围。</p>
<p><strong>3.Initial Pose Estimation via Global Relocalization</strong><br>如果追踪失败，则将当前帧转化为词袋，然后检索词袋数据库，在所有关键帧中查找ORB特征的匹配。找到对应的匹配之后，进行PnP算法求解当前的位姿。得到一个位姿估计值之后，可以通过搜索当前帧与其他关键帧之间的匹配，从而进一步优化位姿。</p>
<p><strong>4.Track Local Map</strong><br>当有了对当前相机位姿的估计之后，可以通过建立局部地图的方式来提高位姿评估的精确度。局部地图$K_1$包含一系列关键帧，这些关键帧可以和当前帧观察到相同的地图点，局部地图$K_2$也是一系列关键帧，这些关键帧与$K_1$中的关键帧是相邻的关联关系。$K_1$中和当前帧观察到相同的地图点数目最多的关键帧称为$k_{ref}$。所有在$K_1$和$K_2$可以看到的地图点都会在当前帧中进行一次搜索匹配：<br>1）直接计算地图点到当前帧的投影$x$，如果投影在图像边界之外，则直接剔除。<br>2）计算当前帧的方向$v$和地图点的视图方向$n$的乘积，如果$v\cdot n &lt; cos(60^{\circ})$，则剔除。<br>3）计算地图点到相机中心的距离d，如果$d\notin[d_{\min},d_{\max}]$,则剔除。<br>4）计算当前帧的尺度$d/d_{\min}$。<br>5）比较地图点ORB描述子与当前帧中还未匹配的ORB特征，如果匹配到的ORB特征在尺度和位置上都接近，则找到了地图点和当前帧的一组匹配。<br>相机位姿会根据匹配的地图点进行进一步优化。</p>
<p>PS:步骤2中提到的motion BA和步骤3,4中提到的位姿优化都是指固定当前帧中能看到的地图点，然后优化当前帧的位姿。</p>
<p><strong>5.New Keyframe Decision</strong><br>最后Tracking线程会判断当前帧是不是满足关键帧的条件，只有满足以下所有条件才能称为关键帧：<br>1）距离上一次全局重定位已经超过了20帧图像。（保证好的重定位）<br>2）Local Mapping处于空闲状态，或者距离上一次插入关键帧已经超过了20帧图像<br>3）当前帧可以观察到超过50个地图点。（保证好的追踪）<br>4）当前帧与参考帧同时可以观察到的地图点少于90%。参考帧是$K_1$中与当前帧同时可以观察到的地图点最多的关键帧。（增加视差变化）</p>
<h1 id="Local-Mapping"><a href="#Local-Mapping" class="headerlink" title="Local Mapping"></a>Local Mapping</h1><p>Local Mapping处理每一个被Tracking线程判断为关键帧的新关键帧$k_i$。主要有以下几个处理步骤：</p>
<p><strong>1.KeyFrame Insertion</strong><br>首先更新Covisibility Graph，图中插入节点$k_i$以及更新与$k_i$相关联的其他关键帧之间的边。<br>其次更新生成树，将关键帧连接到参考帧上。<br>最后计算关键帧的词袋表达，有利于为三角化新的地图点提供数据关联。</p>
<p><strong>2.Recent Map Points Culling</strong><br>地图点能够保留下来必须经过严格的筛选。在创建了三个关键帧之后，所有新创建的地图点都必须满足以下两个条件：<br>1）能观察到该点的关键帧不应该少于理论上能观测到该点的关键帧数目的1/4。<br>2）地图点被创建之后，接下来应该至少有3个关键帧可以观察到这个地图点<br>在任何时刻，如果观察到地图点的关键帧少于3个，那么该地图点会被剔除。这种情况会发生在进行局部BA时，一些异常的关键帧被剔除了。</p>
<p><strong>3.New Map Point Creation</strong><br>对于$k_i$与在Covisibility Graph中其他相连的关键帧集合$K_c$，搜索$k_i$中和$K_c$未与地图点关联的ORB特征之间的匹配，然后三角化出新的地图点。新地图点的创建要满足地图点位置在相机的前方，重投影误差小以及进行尺度的检查。新的地图点是通过两个关键帧创建的，但是可以通过Tracking线程中的Track Local Map，实现地图点和其他关键帧中未匹配地图点的ORB特征进行匹配。<br>确定新地图点的相关属性。</p>
<p><strong>4.Local Bundle Adjustment</strong><br>局部BA针对$k_i$,相关联的$K_c$和所有能被观察的地图点进行优化的。其他可以看到相同的地图点但是不与$k_i$相关联也会加入到BA中，但是只作为约束条件，是固定不变的。优化过程中或者优化结束后，异常点会被剔除。</p>
<p><strong>5.Local Keyframe Culling</strong><br>Local Mapping会剔除冗余的关键帧，从而有利于BA优化和长时间运行时关键帧的维护开销。<br>如果一个关键帧中观察到的地图点超过90%能被其他三个关键帧观察到，则剔除该关键帧。这个策略是[9]的启发。</p>
<h1 id="Loop-Closing"><a href="#Loop-Closing" class="headerlink" title="Loop Closing"></a>Loop Closing</h1><p>当Local Mapping线程处理完一个关键帧$k_i$之后，Loop Closing线程会尝试利用$k_i$做回环检测。主要有以下几个步骤：</p>
<p><strong>1.Loop Candidates Detection</strong><br>计算$k_i$与在Covisibility Graph中相关联的关键帧($\theta_{\min}=30$)的词袋向量的相似度,保留最小值$s_{\min}$。然后检索位置识别数据库并剔除所有小于$s_{\min}$的关键帧。另外，所有与当前帧相连的关键帧都会被剔除。如果连续三个候选关键帧的检索结果是一致的，则认为$k_i$确实为闭环候选关键帧。</p>
<p><strong>2.Compute the Similarity Transformation</strong><br>单目SLAM系统有7个自由度，3个平移，3个旋转，1个尺度因子，这七个自由度上，地图都有可能发生漂移[5]。因此要闭合某个回环，我们需要计算从当前关键帧$K_i$到回环关键帧$K_l$的相似变换矩阵，以获得回环的累积误差。计算相似变换也可以作为回环的几何验证。<br>我们首先计算与$k_i$中ORB特征关联的地图点和回环候选关键帧的对应关系。此时，对每个候选回环，我们有了一个3D到3D的对应关系。我们对每个候选回环执行RANSAC迭代，通过Horn方法[10]找到相似变换。如果我们用足够的有效数据找到相似变换$S_{il}$，我们就可以优化它，并搜索更多的对应关系。如果$S_{il}$有足够的有效数据，我们再优化它。当支持$S_{il}$的有效数据足够多时，就可以认为$k_l$回环被接受。这里的优化，所有的地图点都是固定的，只优化当前帧的位置。</p>
<p><strong>3.Loop Fusion</strong><br>回环矫正的第一步是融合重复的地图点，插入与回环闭合相关的相似视图的新边缘。在步骤2中通过相似变换$S_{il}$已经矫正了当前关键帧的位姿$T_{iw}$，现在可以将矫正扩散到与当前帧相关联的关键帧上，这样回环两端就可以对齐。所有回环关键帧和相关联的关键帧可以观察到的地图点都会被重投影到$k_i$和与$k_i$相关联的关键帧，类似于Tracking线程中的Track Local Map。所有匹配的地图点和用于计算$S_{il}$会被融合。融合过程中所有的关键帧将会更新它们的边，这些新更新的边将非常有效的用于闭合回环。</p>
<p><strong>4.Essential Graph Optimization</strong><br>为了有效的闭合回环，基于Essential Graph做一次位姿优化，此时的优化只固定初始位姿，其他所有的位姿进行尺度漂移的优化，即Sim3的位姿优化。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>实验部分就暂不分析了。至此，ORB-SLAM的总体流程和框架应该都非常明了了，对于其中的具体实现细节还需要结合作者的开源代码来看。<br>对于回环检测的部分，特别是Sim3优化的部分的理解还比较羞涩，需要依据[5,11,12]做进一步学习。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Mur-Artal R, Montiel J M M, Tardos J D. ORB-SLAM: a versatile and accurate monocular SLAM system[J]. IEEE Transactions on Robotics, 2015, 31(5): 1147-1163.<br>[2] G. Klein and D. Murray, “Parallel tracking and mapping for small AR workspaces,” in IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR), Nara, Japan, November 2007, pp. 225–234.<br>[3] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: an efficient alternative to SIFT or SURF,” in IEEE International Conference on Computer Vision (ICCV), Barcelona, Spain, November 2011, pp. 2564–2571.<br>[4] R. Mur-Artal and J. D. Tardós, “Fast relocalisation and loop closing in keyframe-based SLAM,” in IEEE International Conference on Robotics and Automation (ICRA), Hong Kong, China, June 2014, pp. 846–853.<br>[5] H. Strasdat, J. M. M. Montiel, and A. J. Davison, “Scale drift-aware large scale monocular SLAM.” in Robotics: Science and Systems (RSS),Zaragoza, Spain, June 2010.<br>[6] H. Strasdat, A. J. Davison, J. M. M. Montiel, and K. Konolige, “Double window optimisation for constant time visual SLAM,” in IEEE International Conference on Computer Vision (ICCV), Barcelona, Spain, November 2011, pp. 2352–2359.<br>[7] D. Gálvez-López and J. D. Tardós, “Bags of binary words for fast place recognition in image sequences,” IEEE Transactions on Robotics,vol. 28, no. 5, pp. 1188–1197, 2012.<br>[8] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision, 2nd ed. Cambridge University Press, 2004.<br>[9] W. Tan, H. Liu, Z. Dong, G. Zhang, and H. Bao, “Robust monocular SLAM in dynamic environments,” in IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Adelaide, Australia, October 2013, pp. 209–218.<br>[10] F. Endres, J. Hess, J. Sturm, D. Cremers, and W. Burgard, “3-d mapping with an rgb-d camera,” IEEE Transactions on Robotics, vol. 30, no. 1,pp. 177–187, 2014.<br>[11] H. Strasdat, “Local Accuracy and Global Consistency for Efficient Visual SLAM,” Ph.D. dissertation, Imperial College, London, October 2012.<br>[12] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, “Bundle adjustment a modern synthesis,” in Vision algorithms: theory and practice, 2000, pp. 298–372.<br>[13] <a href="http://www.cnblogs.com/luyb/p/5447497.html" target="_blank" rel="external">路游侠的博客 ORB-SLAM(5)优化</a> </p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ORB-SLAM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于ROS的运行态插件切换机制]]></title>
      <url>https://zhehangt.github.io/2017/04/15/ROS/ROSDynamicPlugin/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在研究生课题中需要基于ROS实现动态加载算法库，并且系统运行是可以手动切换算法库。比如系统刚开始运行时，调用的是算法库A。随着系统的运行，发现此时调用算法B会更好，但是又不能停止系统，导致信息丢失，因此运行过程中的动态切换。<br>经过调研，发现可以通过Pluginlib和Dynamic Reconfigure这两个库来实现。</p>
<h1 id="Pluginlib"><a href="#Pluginlib" class="headerlink" title="Pluginlib"></a>Pluginlib</h1><p>pluginlib是一个C++库, 用于在ROS包里面动态的加载或卸载插件。插件本质上讲就是一个动态库，用这些动态库实现某些功能。因此通过切换动态库，系统就可以实现不同的功能。从而达到不用修改系统源码，只需要编写和添加插件的方式，就可以实现系统功能的扩展。</p>
<p>利用pluginlib编写插件有以下几个步骤：<br><strong>1.创建插件基类，定义统一接口。</strong><br>假设我要编写一个SLAM插件，则首先要编写SLAM插件的基类，假设其包名为slam_system。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">namespace</span> slam &#123;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * @brief The SLAMBase class</div><div class="line"> * all SLAM plugins should inherit this base class, and implement pluginlib-required methods.</div><div class="line"> */</div><div class="line">    <span class="keyword">class</span> SLAMBase  &#123;</div><div class="line">    <span class="keyword">public</span>:</div><div class="line">    </div><div class="line">        <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Activate</span><span class="params">()</span> </span>= <span class="number">0</span>;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">Shutdown</span><span class="params">()</span> </span>= <span class="number">0</span>;</div><div class="line"></div><div class="line">    &#125;;</div><div class="line"></div><div class="line">&#125; <span class="comment">// end namespace</span></div></pre></td></tr></table></figure></p>
<p><strong>2.创建插件类</strong><br>所有SLAM插件都要继承SLAM插件的基类，并实现积累中定义好的接口。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">namespace</span> rgbd_slam&#123;</div><div class="line"></div><div class="line">	<span class="keyword">class</span> Slam : <span class="keyword">public</span> slam::SLAMBase &#123;</div><div class="line"></div><div class="line">	<span class="keyword">public</span>:</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">void</span> <span class="title">Activate</span><span class="params">()</span></span>&#123;</div><div class="line">        <span class="comment">// implementment</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">void</span> <span class="title">Shutdown</span><span class="params">()</span></span>&#123;</div><div class="line">        <span class="comment">// imimplementment</span></div><div class="line">        &#125;</div><div class="line"></div><div class="line">	&#125;;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在这里插件类和插件基类往往不在一个package里面，因此要在插件类这边的CMakeLists.txt里面和package.xml里面定义好依赖。</p>
<p><strong>3.导出插件，并编译为动态链接库</strong></p>
<p>要使插件类能够被正确连接，需要四个步骤。<br>首先是使用PLUGINLIB_EXPORT_CLASS宏，对插件类进行标记，说明这个类可以被动态加载。这个宏被定义在pluginlib/class_list_macros.h头文件中，通常放置于cpp文件的底部。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pluginlib/class_list_macros.h&gt;</span></span></div><div class="line"><span class="comment">//宏操作的第一个参数为插件类的全名，第二个参数为插件类的基类全名，这两个名字都包括命名空间。</span></div><div class="line">PLUGINLIB_EXPORT_CLASS(rgbd_slam::Slam, slam::SLAMBase);</div></pre></td></tr></table></figure>
<p>然后将其编译为动态链接库。因此在CMakeLists.txt文件中添加。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">add_library(rgbd_slam</div><div class="line">	src/rgbd_slam.cpp</div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>然后执行 catkin_make 就生成了动态链接库，默认生成库路径为工作空间下 devel/lib/ 文件夹。</p>
<p><strong>4.将插件加入ROS，使其可被加载</strong><br>接着编写插件描述文件。新建rgbd_slam.xml。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;library path=&quot;lib/librgbd_slam&quot;&gt;</div><div class="line">  &lt;class name=&quot;rgbd_module&quot; type=&quot;rgbd_slam::Slam&quot; base_class_type=&quot;slam::SLAMBase&quot;&gt;</div><div class="line">    &lt;description&gt;This is a rgb_slam plugin.&lt;/description&gt;</div><div class="line">  &lt;/class&gt;</div><div class="line">&lt;/library&gt;</div></pre></td></tr></table></figure>
<p>library 标签指出了包含插件的动态库的相对路径，这里为 lib 文件夹下名为 librgbd_slam.so 动态链接库，library 标签中省去了库的后缀.so。class 标签指出了期望从动态库中导出的插件类。type，指明插件类的namespace::class。 base_class ，指明插件的基类的namespace::class。。name，使用name的方法标识插件。</p>
<p>最后在package.xml文件中申明rgbd_slam.xml的位置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;export&gt;</div><div class="line">  &lt;!-- Other tools can request additional information be placed here --&gt;</div><div class="line">  &lt;hi_slam plugin=&quot;$&#123;prefix&#125;/rgbd_module.xml&quot; /&gt;</div><div class="line">&lt;/export&gt;</div></pre></td></tr></table></figure></p>
<p><strong>5.检测插件类</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rospack plugins --attrib=plugin slam_system</div></pre></td></tr></table></figure></p>
<p>slam_system是定义插件基类的包名。</p>
<p><strong>6.使用插件类</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//slam_system是插件基类package的名字。slam::SLAMBase是namespace::基类</div><div class="line">pluginlib::ClassLoader&lt;slam::SLAMBase&gt; loader(&quot;slam_system&quot;, &quot;slam::SLAMBase&quot;);</div><div class="line">boost::shared_ptr&lt;slam::SLAMBase&gt; slam = poly_loader.createInstance(&quot;rgbd_slam::Slam&quot;);</div><div class="line">//or</div><div class="line">boost::shared_ptr&lt;slam::SLAMBase&gt; slam = poly_loader.createInstance(&quot;rgbd_module&quot;);</div><div class="line"></div><div class="line">slam-&gt;Activate();</div><div class="line">slam-&gt;Shutdown();</div></pre></td></tr></table></figure>
<p>到这里，一个完整的pluginlib机制就算完成了。<br>以后只需要修改rgbd_slam的实现，而不需要修改slam_system包，就能实现rgbd_slam功能的修改。如果需要多种slam的实现，比如laser_slam，则只需要像rgbd_slam的实现方式一样进行添加即可。</p>
<p><strong>总结Pluginlib:</strong></p>
<ul>
<li>创建插件基类，定义统一接口</li>
<li>继承插件基类编写插件类，实现统一接口</li>
<li>编译插件类，得到动态链接库</li>
<li>编写xml描述文件，并在package.xml中申明描述</li>
<li>基于pluginlib::ClassLoader和boost::shared_ptr调用插件</li>
</ul>
<h1 id="Dynamic-Reconfigure"><a href="#Dynamic-Reconfigure" class="headerlink" title="Dynamic Reconfigure"></a>Dynamic Reconfigure</h1><p>由于我们需要在系统运行是可以手动切换算法库，比如系统运行时，我们希望能够发送一个指令，使其从rgbd_slam转换为laser_slam。通过Dynamic Reconfigure机制可以非常方便的实现这种切换功能。</p>
<p>首先来了解下什么是Dynamic Reconfigure。Dynamic Reconfigure提供了不必重启节点，就可以更改节点参数的方法。因此借助ROS的参数的改变，即可实现插件的切换。</p>
<p>利用Dynamic Reconfigure有以下几个步骤</p>
<p><strong>1.创建和使用cfg文件。</strong></p>
<p>在cfg文件夹下创建slam.cfg文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">PACKAGE = &apos;slam_system&apos;</div><div class="line"></div><div class="line">from dynamic_reconfigure.parameter_generator_catkin import *</div><div class="line"></div><div class="line">gen = ParameterGenerator()</div><div class="line"></div><div class="line">gen.add(&quot;slam&quot;, str_t, 0, &quot;The name of the plugin for the slam.&quot;, &quot;rgbd_slam&quot;)</div><div class="line"></div><div class="line">exit(gen.generate(PACKAGE, &quot;slam_node&quot;, &quot;slam_node&quot;))</div></pre></td></tr></table></figure></p>
<p>cfg其实是一个基于python的模块，首先创建一个ParameterGenerator对象，然后调用其add()函数将参数添加到参数列表中。add()的参数含义分别是：参数名，参数类型，级别，描述，缺省值，最小值，最大值。</p>
<p>这里我添加了一个参数slam，类型是string，缺省值是rgbd_slam。<br>最后一行是告知generator创建必要的文件并退出程序。第二个参数是cfg文件依附的节点名，第三个参数是生成头文件名称的前缀。这里会生成一个叫做slam_nodeConfig.h的头文件。</p>
<p>编辑完成之后，我们需要添加运行权限才能使用.cfg文件。<br>最后还要在CMakeList中添加依赖。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">generate_dynamic_reconfigure_options(</div><div class="line">    cfg/slam.cfg</div><div class="line">)</div><div class="line">add_dependencies(slam_node $&#123;PROJECT_NAME&#125;_gencfg)</div></pre></td></tr></table></figure></p>
<p><strong>2.监听参数修改</strong><br>监听参数修改指在参数发生改变时，触发注册的回调函数。在我们的需求中，就是在回调函数中完成插件的切换。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#include &lt;dynamic_reconfigure/server.h&gt;</div><div class="line">#include &lt;slam_system/slam_nodeConfig.h&gt;</div><div class="line"></div><div class="line">dynamic_reconfigure::Server&lt;slam_system::slam_nodeConfig&gt; server;</div><div class="line">dynamic_reconfigure::Server&lt;slam_system::slam_nodeConfig&gt;::CallbackType f;</div><div class="line">f = boost::bind(&amp;ConfigCb, _1, _2);</div><div class="line">server.setCallback(f);</div></pre></td></tr></table></figure></p>
<p>其中ConfigCb是一个回调函数，每次参数发生变化时会被触发。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">void ConfigCb(slam_node::slam_nodeConfig &amp;config, uint32_t level)</div><div class="line">&#123;</div><div class="line">//利用config信息完成插件切换</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>3.利用rqt_reconfigure可视化调整参数</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rosrun rqt_reconfigure rqt_reconfigure</div></pre></td></tr></table></figure></p>
<p><strong>总结Dynamic Reconfigure：</strong></p>
<ul>
<li>在package.xml文件中加入dynamic_reconfigure编译依赖和运行依赖。</li>
<li>创建.cfg文件，添加参数名到参数列表。</li>
<li>修改CMakeLists.txt。添加generate_dynamic_reconfigure_options以及add_dependencies。</li>
<li>在程序中加入两个头文件，并声明动态调整服务，绑定其回调函数。在回调函数中实现对参数对程序的影响</li>
</ul>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.<a href="http://www.rosclub.cn/post-157.html" target="_blank" rel="external">ROS中pluginlib的使用总结</a></p>
<p>2.<a href="http://www.rosclub.cn/post-159.html" target="_blank" rel="external">ROS的参数应用以及动态调整（下）</a></p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> ROS </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[学习ROS的一些思考]]></title>
      <url>https://zhehangt.github.io/2017/04/10/ROS/ROSLearning/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>对于每个要准备进入机器人领域的小白来说，ROS应该会是每个人都绕不过的坎。用ROS做开发大概有两个月了，对于如何学习ROS有了一点点简单的看法和思考。</p>
<h1 id="什么是ROS"><a href="#什么是ROS" class="headerlink" title="什么是ROS"></a>什么是ROS</h1><p>首先看看ROS wiki对于ROS的定义：ROS (Robot Operating System, 机器人操作系统) 提供一系列程序库和工具以帮助软件开发者创建机器人应用软件。它提供了硬件抽象、设备驱动、函数库、可视化工具、消息传递和软件包管理等诸多功能。<br>这两个月，我也一直在思考，到底ROS是什么。<br>从最狭义的ROS来讲，ROS就是通信机制。鉴于机器人的控制比较复杂，对于机器人常见的编程范式“观察 - 判断 - 决策 - 行动”(OODA)来说，会涉及大量的信息通信问题，因此ROS设计了两种通信机制：基于发布订阅的消息通信和基于服务的RPC通信。有了这些通信机制，在进行机器人开发时，我们就可以设计出非常松耦合的分布式OODA软件框架。基于这种松耦合的优点，在开发过程中产生了很多很多的函数库和开发工具，使得机器人开发者可以快速复用，方便机器人应用的开发，调试。</p>
<p>总的来说可以从以下几个方面看待ROS。<br>首先是<strong>硬件抽象和设备驱动</strong>。很多比较常用的硬件在ROS中都提供了相应的包，使得开发者可以非常方便的获取传感器数据以及控制硬件。比如通过openni的包，可以非常方便的从kinect中获取彩色图像和深度图像。通过turtlebot bringup包可以实现对turtlebot的控制。<br>第二是<strong>构建工具和通信机制</strong>。基于通信机制和应用需求，构造出ROS Package和分布式的ROS Node，这些Package和Node构成了分布式的机器人软件框架。<br>第三是<strong>仿真工具和可视化工具</strong>。ROS提供了一系列仿真工具和可视化工具来提高机器人应用的开发效率。<br>第四是<strong>函数库和第三方开源包</strong>。ROS提供了一系列在机器人应用开发中非常实用的函数库，比如坐标转换、插件机制等等。其次很多开发者也通过开源的方式为ROS贡献优秀的算法和工具。比如在SLAM领域就有很多支持ROS的开源算法。</p>
<h1 id="如何学习ROS"><a href="#如何学习ROS" class="headerlink" title="如何学习ROS"></a>如何学习ROS</h1><p>从我学习ROS的经历来看，学习ROS门槛低、入门快，但是ROS是典型的从入门到放弃，当真正使用ROS进行开发时，就会发现然而一切都没有那么简单。因此在这里只能简单介绍一些能够快速入门ROS的方法。</p>
<p>首先是要了解ROS的基本用法，包括熟悉构建ROS Package和ROS Node的过程，使用发布订阅获取消息，使用服务端和客户端实现RPC调用，Rosbag，Roslaunch，TF，rviz，gazebo等等等等。这方面的内容ROS wiki是很好的入门材料，还有一本我们学校老师翻译的《机器人操作系统(ROS)浅析》，也是很好的入门资料。</p>
<p>其次就是要阅读和编写一定的代码量，来提高ROS的编码水平，逐步提高对ROS的设计思想和核心概念的理解。这方面的资料推荐Turtlebot的ROS Wiki主页和《ROS By Example》这本书。通过这两份资料，可以从一个Turtlebot这样一个简单的机器人出发，借助ROS实现很多有意思的功能。</p>
<p>接下来应该就是从实际的应用需求出发，详细学习ROS的核心模块和核心工具，没事再看看大牛的博客，这一阶段真是印了一句老话，修行靠个人。</p>
<p>最后推荐一下IDE。<br>刚开始用过Clion，但是Clion对与ROS的支持并不完善，弃用。<br>转战ROS版的Qt Creator，目前来说还算好用，就是丑。<br>听过有个叫RoboWare的IDE不错，打算有机会用用。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>以上。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> ROS </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[SLAM系统的鲁棒性]]></title>
      <url>https://zhehangt.github.io/2017/04/07/SLAM/SLAMRobustness/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>SLAM在这两年得到了快速的发展，特别是在SLAM系统实现层面，越来越多的大牛贡献出了开源代码。基于视觉特征的ORB-SLAM，基于半稠密的LSD、DSO，基于稠密的Elastic Fusion、RGBD-SLAM，基于激光的GMapping、Cartographer等，都展示出各个思路在解决SLAM这个问题上的强大能力。<br>但是在实际的应用中，这些SLAM系统总是或多或少的会失效，因为在求解SLAM问题时，目前的SLAM系统总是希望环境是小范围的，长时间不发生变化的。这种对<strong>环境静止的假设</strong>在实际生活中往往是不成立的。因此在实际应用中，开源的SLAM系统常常由于环境的动态变化造成系统崩溃或建图和定位失败。<br>总得来说，SLAM在健壮、容错、鲁棒这条路上还有很长的一段路要走。<br>这篇博客的大部分观点来自论文[1]。</p>
<h1 id="什么是鲁棒性"><a href="#什么是鲁棒性" class="headerlink" title="什么是鲁棒性"></a>什么是鲁棒性</h1><p>鲁棒性的中文解释是系统的健壮性，也就是说系统在异常和危险情况下，不会发生死机、崩溃等情况。但是目前的SLAM系统在环境发生动态变化时，往往会产生异常。</p>
<p>简单的举个特例。ORB-SLAM利用ORB特征在图像之间寻找匹配，对相机的位置进行追踪。但是当环境中动态变化的物体太多，比如人的走动，椅子的移动等，或者说环境条件发生了变化，比如光线变化，季节变化等，导致即使是相同位置的图像，其外观也发生了变化。这些变化很容易引起ORB特征的匹配失败，从而造成对相机追踪的失败。虽然ORB-SLAM添加了重定位模块，用词袋模型来解决追踪失败时相机位置丢失的问题，但是重定位模块也会依赖于视觉特征，因此ORB-SLAM在处理追踪失败这个问题上还有进一步优化的空间。<br>其实不仅仅是ORB-SLAM存在这种问题，所有基于视觉的SLAM系统或多或少的都存在这种问题。也就是说环境的动态变化是最让目前的SLAM系统头疼的问题，但是在实际的应用中，环境的动态变化恰恰是无法避免的。</p>
<p>SLAM要走向应用，往往需要长时间的运行，往往需要适应动态变化的环境，因此SLAM的鲁棒性的发展会是使其能真正走向应用的关键之一。</p>
<h1 id="什么造成鲁棒性问题"><a href="#什么造成鲁棒性问题" class="headerlink" title="什么造成鲁棒性问题"></a>什么造成鲁棒性问题</h1><p>要解决鲁棒性问题，我们首先从造成鲁棒性问题的原因说起。SLAM的鲁棒性问题主要分为两个层面：传感器层面和算法层面。<br>传感器层面是指通过传感器本身是否可靠，也就是通过传感器采集到的数据是否可靠。</p>
<p>首先<strong>环境</strong>会影响传感器。举个例子，在黑暗环境下，摄像头往往无法获得彩色图像，因此目前基于视觉的SLAM系统都是无法在黑暗环境下工作的，换句话中，当环境光条件发生巨大变化时或者从白天进入到黑夜时，视觉SLAM是会失效和崩溃的。还有个例子就是RGBD相机，在室外这种宽阔并且光线强度大的环境中，RGBD相机往往无法正确获取深度图像。总的来说，限于目前传感器技术并不是完美的，因此并不能保证不同的环境下，传感器都能正确工作。但从我目前的了解来看，还没有关于在不同环境下采集到的传感器数据，对于SLAM算法的求解是否会产生影响的相关研究。<br>其次<strong>传感器自身</strong>会老化，会故障，会导致传感器的精确度下降，最终使得传感器数据异常。传感器本身出现问题可能已经超出了SLAM研究本身的范围，但是一个鲁棒的SLAM系统至少可以检测出传感器数据的异常，从而将减少建图定位结果错的离谱的情况，甚至导致系统崩溃的可能性。但是SLAM系统如何检测传感器数据异常也是一个有待于研究的问题。</p>
<p>算法层面主要包含<strong>数据关联</strong>问题和<strong>参数调整</strong>问题。<br>数据关联主要是指将传感器观测与空间中的实际物体进行关联。多个传感器观测与空间中同一个物体有了关联之后，就可以对机器人的位姿进行评估。造成数据关联错误，在我看来大致可以分为两个方面。首先是<strong>环境的动态变化</strong>。在前文中也提到过，在动态变化的环境中，数据关联是非常容易发生错误的。明明是空间中的同一个物体，但由于一些环境条件的变化或者摆放位置的变化，导致在多次传感器观测中被认为是不同物体，这就造成了错误的数据关联。其次是<strong>环境的特殊性</strong>。也就是说在某些特殊场景下得到的传感器观测数据，某些数据关联方法是无法正确完成数据关联的。根据no free lunch theorem，不会存在某种传感器观测和数据关联方法可以通吃所有类型的场景。<br>除了短时间内的数据关联，还有长时间的数据关联问题。长时间的数据关联主要表现在回环检测问题上，提高地图和定位结果的全局一致性。错误的数据关联不仅会造成当前位姿估计的错误，还会造成回环检测和后端优化的错误。最终导致的结果就是，全是错的错的错的。<br>参数调整问题其实一定程度上也是数据关联的具体形式，因为很多参数设置就是为了能够得到比较好的数据关联。比如在ORB-SLAM2中对于ORB特征点的提取和匹配有专门的参数配置。一个合适的参数配置可以使SLAM系统在特定环境中更可靠的运行。从这个层面上来讲，如果SLAM能够自动调整参数，那么也是可以在一定程度上提高SLAM系统的鲁棒性。</p>
<h1 id="相关研究进展"><a href="#相关研究进展" class="headerlink" title="相关研究进展"></a>相关研究进展</h1><p>从当前的研究成果来看，很多大牛在致力于解决SLAM系统的鲁帮性问题。在此将论文[1]中引用的论文作简单罗列，方便以后进一步学习。</p>
<p>首先是回环检测中的数据关联问题。在视觉SLAM中，[2]词袋模型在回环检测问题上被广泛使用,并且展现出非常可靠的性能表现。但是词袋模型无法处理光线变化强烈的情况。<br>因此催生了匹配序列[3]，收集不同的视觉外观并用统一的表达方式[4]，同时使用空间信息和外观信息[5]。这方面的综述可以参考[6]。<br>在激光SLAM中也有类似的回环检测。比如[7,8]。<br>在视觉SLAM中，对于回环的验证通常用RANSAC[9]，从而减少错误的数据关联。<br>在激光SLAM中，对于回环的验证是将回环点，放回已经构建的地图中，计算误差。</p>
<p>之前提到错误的回环会严重影响定位和构图结果[10]。因此一系列方法会提出来处理错误回环问题。其中[11,12,13,14,15]验证在优化期间因为某些约束产生的误差大小来防止错误的回环。[16,17]通过在优化执行之前首先进行错误回环并剔除，来避免产生错误的结果。</p>
<p>而动态环境中数据关联问题，SLAM系统需要能够检测，消除或者跟踪动态变化的物体。目前主流的方法就是消除环境中的动态部分[18]。也有把动态部分作为模型的一部分的方法[19,20,21]。如果环境的变化是周期性的，SLAM系统需要对环境构建多个地图[22,23]或者基于时间参数进行地图表示[24,25]。</p>
<p>前面提到了很多方法来避免产生错误的数据关联和回环检测，但是现在的SLAM系统在面对错误的数据关联时依旧非常脆弱。因此一个理想的SLAM方法可以在错误发生之前进行预判，并提供恢复机制在错误发生时进行恢复和重估计。目前还没有SLAM系统能有这样的能力。在面对传感器错误时，目前的SLAM系统也缺少相应的机制和方法来检验传感器数据的精确度。<br>关于在地图中进行重定位来增强鲁棒性已经被一些SLAM系统采用。词袋模型是视觉SLAM进行重定位的典型代表。此外也有用轨迹匹配的方法[26]。<br>也有一些研究工作采用多种传感器模块或者是异构的感知信息来增强鲁棒性[27,28,29,30,31]。<br>也有一些研究工作针对非刚体空间的三维重建方面的[32,33,34,35]。<br>在参数自动调整方面，[36]可以在线调整相机的内参。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>对于SLAM鲁棒性的问题，总结起来大概从以下几个步骤展开。<br>1.传感器数据的精确度检测。<br>2.数据关联方法的进一步提高。包括回环检测下的数据关联，动态场景下的数据关联等。<br>3.系统在应对错误时的机制，包括对错误产生的预判，错误产生后的恢复机制等。<br>5.对多传感器，异构传感器，异构信息等多方面的数据关联方法的探索。<br>6.参数的自动调整能力。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Cadena C, Carlone L, Carrillo H, et al. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age[J]. IEEE Transactions on Robotics, 2016, 32(6): 1309-1332.<br>[2] Gálvez-López D, Tardos J D. Bags of binary words for fast place recognition in image sequences[J]. IEEE Transactions on Robotics, 2012, 28(5): 1188-1197.<br>[3] M. Milford and G. Wyeth. SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),pages 1643–1649. IEEE, 2012<br>[4] W. Churchill and P. Newman. Experience-based navigation for long-term localisation. The International Journal of Robotics Research(IJRR), 32(14):1645–1661, 2013.<br>[5] K. L. Ho and P. Newman. Loop closure detection in SLAM by combining visual and spatial appearance. Robotics and Autonomous Systems (RAS), 54(9):740–749, 2006.<br>[6] S. Lowry, N. Snderhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke,and M. J. Milford. Visual Place Recognition: A Survey. IEEE Transactions on Robotics (TRO), 32(1):1–19, 2016.<br>[7] G. D. Tipaldi and K. O. Arras. Flirt-interest regions for 2D range data.In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 3616–3622. IEEE, 2010<br>[8] Hess W, Kohler D, Rapp H, et al. Real-time loop closure in 2D LIDAR SLAM[C]//Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 2016: 1271-1278.<br>[9] D. Scaramuzza and F. Fraundorfer. Visual Odometry [Tutorial]. Part I:The First 30 Years and Fundamentals. IEEE Robotics and Automation Magazine, 18(4):80–92, 2011.<br>[10] N. Sunderhauf and P. Protzel. Towards a robust back-end for pose graph SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 1254–1261. IEEE, 2012.<br>[11] P. Agarwal, G. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard.Robust map optimization using dynamic covariance scaling. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 62–69. IEEE, 2013.<br>[12] L. Carlone, A. Censi, and F. Dellaert. Selecting good measurements via l1 relaxation: a convex approach for robust estimation over graphs. In Proceedings of the IEEE/RSJ International Conference on Intelligent 21 Robots and Systems (IROS), pages 2667 –2674. IEEE, 2014.<br>[13] Y. Latif, C. Cadena, and J. Neira. Robust Loop Closing Over Time. In Proceedings of Robotics: Science and Systems Conference (RSS),2012.<br>[14] E. Olson and P. Agrawal. Inference on Networks of Mixtures for Robust Robot Mapping. In Proceedings of Robotics: Science and Systems Conference (RSS), 2012.<br>[15] N. Sunderhauf and P. Protzel. Towards a robust back-end for pose graph SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 1254–1261. IEEE, 2012.<br>[16] E. Olson, M. Walther, J. Leonard, and S. Teller. Single-Cluster Spectral Graph Partitioning for Robotics Applications. In Proceedings of Robotics: Science and Systems Conference (RSS), pages 265–272,2005.<br>[17] D. Sabatta, D. Scaramuzza, and R. Siegwart. Improved appearance-based matching in similar and dynamic environments using a vocabulary tree. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 1008–1013. IEEE, 2010.<br>[18] Neira J, Tardós J D. Data association in stochastic mapping using the joint compatibility test[J]. IEEE Transactions on robotics and automation, 2001, 17(6): 890-897.<br>[19] D. Rosen, J. Mason, and J. Leonard. Towards Lifelong Feature-Based Mapping in Semi-Static Environments. In Robotics: Science and Systems (RSS), workshop: The Problem of Mobile Sensors: Setting future goals and indicators of progress for SLAM, 2015.<br>[20] A. Walcott-Bryant, M. Kaess, H. Johannsson, and J. J. Leonard. Dynamic pose graph SLAM: Long-term mapping in low dynamic environments. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1871–1878. IEEE,2012.<br>[21] C. Wang, C. Thorpe, S. Thrun, M. Hebert, and H. Durrant-Whyte. Simultaneous localization, mapping and moving object tracking. The International Journal of Robotics Research (IJRR), 26(9):889–916,2007.<br>[22]  F. Dayoub, G. Cielniak, and T. Duckett. Long-term experiments with an adaptive spherical view representation for navigation in changing environments. Robotics and Autonomous Systems (RAS), 59(5):285–295, 2011.<br>[23] K. Konolige, J. Bowman, J. Chen, P. Mihelich, M. Calonder, V. Lepetit,and P. Fua. View-based Maps. The International Journal of Robotics Research (IJRR), 29(8):941–957, 2010.<br>[24] T. Krajnı́k, J. P. Fentanes, O. M. Mozos, T. Duckett, J. Ekekrantz, and M. Hanheide. Long-term topological localisation for service robots in dynamic environments using spectral maps. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4537–4542. IEEE, 2014.<br>[25] J. M. Santos, T. Krajnik, J. P. Fentanes, and T. Duckett. Lifelong Information-driven Exploration to Complete and Refine 4D Spatio-Temporal Maps. IEEE Robotics and Automation Letters, 1(2):684–691,2016.<br>[26] M. Brubaker, A. Geiger, and R. Urtasun. Lost! leveraging the crowd for probabilistic visual self-localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3057–3064. IEEE, 2013.<br>[27] R. W. Wolcott and R. M. Eustice. Visual localization within LIDAR maps for automated urban driving. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),pages 176–183. IEEE, 2014<br>[28] C. Forster, M. Pizzoli, and D. Scaramuzza. Air-Ground Localization and Map Augmentation Using Monocular Dense Reconstruction. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3971–3978. IEEE, 2013<br>[29] D. Majdik, A. L.and Verda, Y. Albers-Schoenberg, and D. Scaramuzza. Air-ground Matching: Appearance-based GPS-denied Urban Localization of Micro Aerial Vehicles. Journal of Field Robotics (JFR),32(7):1015–1039, 2015.<br>[30] B. Behzadian, P. Agarwal, W. Burgard, and G. D. Tipaldi. Monte Carlo localization in hand-drawn maps. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),pages 4291–4296. IEEE, 2015.<br>[31] W. Winterhalter, F. Fleckenstein, B. Steder, L. Spinello, and W. Burgard. Accurate indoor localization for RGB-D smartphones and tablets given 2D floor plans. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3138–3143. IEEE, 2015<br>[32] A. Agudo, F. Moreno-Noguer, B. Calvo, and J. M. M. Montiel. Sequential Non-Rigid Structure from Motion using Physical Priors. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),38(5):979–994, 2016.<br>[33] A. Agudo, F. Moreno-Noguer, B. Calvo, and J. M. M. Montiel. Real-Time 3D Reconstruction of Non-Rigid Shapes with a Single Moving Camera. Computer Vision and Image Understanding (CVIU), 2016, to appear.<br>[34] O. Grasa, E. Bernal, S. Casado, I. Gil, and J. M. M. Montiel. Visual SLAM for Handheld Monocular Endoscope. IEEE Transactions on Medical Imaging, 33(1):135–146, 2014.<br>[35] R. A. Newcombe, D. Fox, and S. M. Seitz. DynamicFusion: Reconstruction and tracking of non-rigid scenes in real-time. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 343–352. IEEE, 2015.<br>[36] Keivan N, Sibley G. Online SLAM with any-time self-calibration and automatic change detection[C]//Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015: 5775-5782.</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[四元数]]></title>
      <url>https://zhehangt.github.io/2017/03/27/SLAM/Quaternion/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在学习三维空间中的刚体运动时，我们提到四元数在描述旋转时只有四个自由度而且也没有奇异性。<br>这篇博客将简单记录四元数的相关内容</p>
<h1 id="四元数"><a href="#四元数" class="headerlink" title="四元数"></a>四元数</h1><p>旋转矩阵具有冗余性，欧拉角和旋转向量不冗余但是具有奇异性。<br>接下来出场的四元数就厉害了，只有四个自由度而且也没有奇异性。它是一种类似于复数的表达方式。<br>四元数内容较多，另外再单独记录。<br>四元数$q$拥有一个实部和三个虚部。如下：<br>$q=q_0+q_1 i+q_2 j + q_3k$<br>其中：<br>$<br>\left\{<br>\begin{align}<br>&amp;i^2=j^2=k^2=-1 \\<br>&amp;ij=k,ji=-k \\<br>&amp;jk=i,kj=-i \\<br>&amp;ki=j,ik=-j<br>\end{align}<br>\right.<br>$</p>
<p>假设某个旋转绕单位向量$n=[n_x, n_y, n_z]^T$进行了角度$\theta$的旋转，那么对应的四元数为：<br>$<br>q = [\cos{\dfrac{\theta}{2}}, n_x\sin \dfrac{\theta}{2}, n_y \sin \dfrac{\theta}{2}, n_z \sin \dfrac{\theta}{2}]<br>$</p>
<p>反之也可以从单位四元数中计算出对应旋转轴和夹角：<br>$<br>\left\{<br>\begin{align}<br>&amp;\theta = 2\arccos q_0 \\<br>&amp;[n_x, n_y, n_z]^T=[q_1, q_2, q_3]^T / \sin{\dfrac{\theta}{2}} \\<br>\end{align}<br>\right.<br>$</p>
<p>对$\theta$加上$2\pi$可以得到一个相同的旋转，但此时对应的四元数变成了$-q$。因此任意的旋转都可以由两个互为相反数的四元数表示。</p>
<h1 id="四元数的运算"><a href="#四元数的运算" class="headerlink" title="四元数的运算"></a>四元数的运算</h1><p>假设有两个四元数$q_a$和$q_b$。如下：</p>
<p>$<br>q_a=[s_a,v_a]=[s_a+x_ai+y_aj+z_ak] \\<br>q_b=[s_b,v_b]=[s_b+x_bi+y_bj+z_bk]<br>$</p>
<p>1.加法和减法<br>$q_a \pm q_b = [s_a \pm s_b, v_a \pm v_b]$</p>
<p>2.乘法<br>乘法是把$q_a$的每一项与$q_b$每项相乘，最后相加。<br>$q_aq_b = [s_a s_b - v_a^Tv_b, s_av_b+s_bv_a + v_a \times v_b]$</p>
<p>3.共轭<br>四元数的共轭是把虚部去成相反数：<br>$q_a^* = [s_a, -v_a]$</p>
<p>四元数共轭与自己本身相乘，会得到一个实四元数，其实部为模长的平方：<br>$q^*q = [s_a^2 + v^Tv, 0]$</p>
<p>4.模长<br>$\parallel q_a \parallel = \sqrt{s_a^2+x_a^2+y_a^2+z_a^2}$</p>
<p>可以验证两个四元数乘积的模即为模的乘积。这保证单位四元数相乘后仍然是单位四元数。<br>$ \parallel q_a q_b \parallel = \parallel q_a \parallel \parallel q_b \parallel $</p>
<p>5.逆<br>$q^{-1}=q^* /  \parallel q \parallel ^2$ </p>
<p>6.数乘与点乘<br>$kq = [ks, kv]$</p>
<p>点乘是指两个四元数每个位置上的数值分别相乘：<br>$q_a q_b = s_as_b + x_ax_bi + y_ay_bj + z_a z_b k$ </p>
<h1 id="四元数的旋转表示"><a href="#四元数的旋转表示" class="headerlink" title="四元数的旋转表示"></a>四元数的旋转表示</h1><p>假设空间中有一点$p=[x,y,z]$，其绕旋转轴$n$，进行了角度为$\theta$的旋转得到了点$p’$，对应的四元数为q。</p>
<p>则可以验证：<br>$p’ = qpq^{-1}$</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>《视觉SLAM十四讲》第三讲</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 刚体运动 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[3D-3D的运动估计]]></title>
      <url>https://zhehangt.github.io/2017/03/17/SLAM/ICP/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>3D-3D的位姿估计问题是指，对于空间中的某一点，我们知道这个点在两个相机坐标系中的三维坐标，如何利用这两个三维坐标来求解这两个相机坐标系的运动就是3D-3D的位姿评估问题。</p>
<p>这个问题通常用迭代最近点(Iterative Closest Point,ICP)求解。</p>
<p>假设空间中的一系列点在第一个相机坐标系下的三维坐标为$C={c_1,…,c_n}$，在第二个相机坐标系下匹配的三维坐标为$C’={c_1’,…,c’_n}$。则有：<br>$\forall i, \ c_i=Rc_i’+t $ </p>
<p>对于ICP的求解主要分为两种方式：利用线性代数的求解和利用非线性优化方式求解。</p>
<h1 id="线性代数求解"><a href="#线性代数求解" class="headerlink" title="线性代数求解"></a>线性代数求解</h1><p>构造误差项：<br>$e_i = c_i - (Rc_i’+t) $ </p>
<p>将这个误差项构造成一个最小二乘问题：</p>
<p>$\min \limits _{R,t} J= \dfrac{1}{2} \sum \limits _{i=1} ^n \begin{Vmatrix}<br>c_i - (Rc_i’+t)<br>\end{Vmatrix} _2 ^2$</p>
<p>通过求解这个最小二乘问题，我们可以得到$R$和$t$。</p>
<p>求解过程真心没看懂啊。</p>
<p>首先计算两组点的质心：<br>$p=\dfrac{1}{n} \sum \limits _{i=1}^n p_i \\<br>p’=\dfrac{1}{n} \sum \limits _{i=1}^n p’_i<br>$ </p>
<p>然后计算每个点的去质心坐标：<br>$q_i = p_i - p \\<br>q_i’ = p’_i - p’<br>$ </p>
<p>接着构造矩阵：<br>$W=\sum \limits _{i=1}^n q_i  {q’}_i^T $ </p>
<p>W是一个3x3的矩阵，对W进行SVD分解，得：<br>$W=U \Sigma V^T$ </p>
<p>当$W$满秩时，$R$为：<br>$R=UV^T$<br>求得$R$后，$t$也就可以求出来了。</p>
<h1 id="非线性优化方法"><a href="#非线性优化方法" class="headerlink" title="非线性优化方法"></a>非线性优化方法</h1><p>非线性优化方法通过迭代的方式去寻找最优值。首先要用李代数来表达位姿，即：</p>
<p>$\min \limits _{\xi} J= \dfrac{1}{2} \sum \limits _{i=1} ^n \begin{Vmatrix}<br>c_i - \exp([\xi]_{\times}c_i’)<br>\end{Vmatrix} _2 ^2$</p>
<p>然后通过对李代数求导和迭代的方式计算极小值。我们之前讨论的李代数求导终于派上了用场。<br>而且在这个问题中，已经有理论可以证明ICP问题存在唯一解或无穷多解。在唯一解的情况下，只要能找到极小值解，这个极小值就是全局最优值。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>《视觉SLAM十四讲》第七讲</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> ICP </tag>
            
            <tag> 3D-3D </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[李代数求导]]></title>
      <url>https://zhehangt.github.io/2017/03/16/SLAM/LieAlgebra-02/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>上一篇我们废了这么大的劲从李群变成了李代数，那李代数到底是为了啥？<br>其实李代数是大有用处的。在SLAM问题中，如果要使地图和位姿的结果更准取，通常会涉及到优化问题，这个优化问题往往与旋转矩阵或者变换矩阵有关。但是之前我们提到过，$SO(3)$和$SE(3)$是不满足加法运算的，也就是说当$SO(3)$和$SE(3)$作为优化变量的时候，是无法对其求导的，不能求导那还怎么玩？这时候李代数拯救了世界。</p>
<h1 id="BCH公式"><a href="#BCH公式" class="headerlink" title="BCH公式"></a>BCH公式</h1><p>首先来看看$SO(3)$上进行乘法运算时，$\mathfrak{so}(3)$上是一种怎样的运算。</p>
<p>BCH公式定义了两个李代数指数映射乘积的完整形式。对完整形式做近似即可得到我们关心的两个公式：<br>$<br>R_1 R_2 = \textbf{exp}([\phi_1]_{\times})\textbf{exp}([\phi_2]_{\times}) \approx \begin{equation}<br>\begin{cases}<br>\textbf{exp}( J_l(\phi_2)^{-1} \phi_1 + \phi_2    )  &amp; \textbf{if  } \phi_1 \textbf{ is samll}\\<br>\textbf{exp}( J_r(\phi_1)^{-1} \phi_2+ \phi_1    )   &amp; \textbf{if  } \phi_2 \textbf{ is samll}<br>\end{cases}<br>\end{equation}<br>$</p>
<p>第一个称为左乘模型，第二个称为右乘模型。</p>
<p>$J_l$即为$\mathfrak{se}(3)$到$SE(3)$转换时求得的$J$，称为左乘近似雅可比：<br>$J_l = J=\dfrac{\sin \theta}{\theta} I + (1- \dfrac{\sin \theta}{\theta})aa^T + \dfrac{1-\cos \theta}{\theta}[a]_{\times}$</p>
<p>它的逆为：<br>$J_l^{-1} = \dfrac{\theta}{2} \cot \dfrac{\theta}{2} I + (1- \dfrac{\theta}{2}\cot \dfrac{\theta}{2})aa^T - \dfrac{\theta}{2}[a]_{\times}$</p>
<p>右乘雅可比仅需要对自变量取负号即可：<br>$J_r(\phi)=J_l(-\phi)$ </p>
<p>同样如果在李代数上做加法,则有：</p>
<p>$\textbf{exp}([\phi+ \Delta\phi]_{\times}) = \textbf{exp}([J_l \Delta\phi]_{\times}) \textbf{exp}([\phi]_{\times})=\textbf{exp}([\phi]_{\times})\textbf{exp}([J_r \Delta\phi]_{\times}) $ </p>
<p>在$\mathfrak{se}(3)$上的运算与$\mathfrak{so}(3)$类似，只是雅可比矩阵的求法不同，而且比较复杂，跳过吧。</p>
<h1 id="李代数求导1-0"><a href="#李代数求导1-0" class="headerlink" title="李代数求导1.0"></a>李代数求导1.0</h1><p>用李代数进行优化首先涉及的就是李代数的求导问题。<br>假设空间中的点p进行了一次旋转R，R对应的李代数为 $\phi$。则根据指数映射有$R=\mathbf{exp}([\phi]_{\times})$。根据导数定义对$\phi$求导，有：</p>
<p>$<br>\begin{equation}<br>\begin{aligned}<br>\dfrac{\partial(\textbf{exp}([\phi]_{\times})p)}{\partial\phi} &amp;= \lim_{\Delta \phi \rightarrow 0} \dfrac{\textbf{exp}([\phi + \Delta \phi]_{\times})p-\textbf{exp}([\phi]_{\times}) p}{\Delta \phi} \\<br>&amp;= … \\<br>&amp;= -[Rp]_{\times}J_l<br>\end{aligned}<br>\end{equation}<br>$ </p>
<p>公式推导过程中有BCH线性近似，泰勒展开舍去高阶项后近似，然后巴拉巴拉巴拉…最终得到了这个导数。但是这个导数包含了一个比较复杂的$j_l$，有没有更简单的呢？</p>
<h1 id="李代数求导2-0"><a href="#李代数求导2-0" class="headerlink" title="李代数求导2.0"></a>李代数求导2.0</h1><p>对$R$进行一次扰动$\Delta R$，扰动对应的李代数为$\Delta \phi$，则求导过程有：</p>
<p>$<br>\begin{equation}<br>\begin{aligned}<br>\dfrac{\partial(\textbf{exp}([\phi]_{\times})p)}{\partial(\phi))} &amp;= \lim_{\Delta \phi \rightarrow 0} \dfrac{\textbf{exp}([\Delta \phi]_{\times})\textbf{exp}([\phi]_{\times}) p-\textbf{exp}([\phi]_{\times}) p}{\Delta \phi} \\<br>&amp;= … \\<br>&amp;= -[Rp]_{\times}<br>\end{aligned}<br>\end{equation}<br>$ </p>
<p>曾经的$j_l$莫名奇妙的没了，或者说根本就没出现过。为什么同样是导数，会相差一个数呢？等我重新学完高数再来解释。</p>
<h1 id="相似变换群"><a href="#相似变换群" class="headerlink" title="相似变换群"></a>相似变换群</h1><p>对于单目相机，会有一种特殊的群，称为相似变换群$Sim(3)$,以及对应的李代数$\mathfrak{sim}(3)$。<br>单目相机存在尺度不确定性。如果在单目SLAM中使用$SE(3)表示位姿$，那么由于尺度不确定性与尺度漂移，整个SLAM过程中的尺度会发生变化，这在$SE(3)$中未能体现出来，因此在单目情况下我们一般会显示地把尺度表达出来。用数学语言来说，对于空间中的点p，在相机坐标系下要经过一个<strong>相似变换</strong>，而非欧式变换：</p>
<p>$p’=\begin{bmatrix}<br>sR &amp; t \\<br>0^T &amp; 1<br>\end{bmatrix} p= sRp + t$</p>
<p>在相似变换中，我们把尺度$s$表达了出来。它同时作用在$p$的三个坐标之上，对$p$进行了一次缩放。</p>
<p>$\mathfrak{sim}(3)$与$Sim(3)$也有对应的指数映射，对数映射。求导过程也类似。</p>
<p>$\mathfrak{sim}(3)=\begin{pmatrix}<br>\zeta = \begin{bmatrix}<br>\rho \\<br>\phi \\<br>\sigma<br>\end{bmatrix} \in \mathbb{R}^7,[\zeta]_{\times}=\begin{bmatrix}<br>\sigma I + [\phi]_{\times} &amp; \rho \\<br>0^T &amp; 0<br>\end{bmatrix} \in \mathbb{R}^{4\times4}<br>\end{pmatrix}$</p>
<p>$S = \textbf{exp}([\zeta]_{\times})=\begin{bmatrix}<br>e^\sigma \textbf{exp}([\phi]_{\times})  &amp; J_s \rho \\<br>0^T &amp; 1<br>\end{bmatrix}<br>$</p>
<p>其中：<br>$s=e^\sigma, t=J_s \rho$ </p>
<p>因为$Sp$是四维的齐次坐标，$\zeta$是七维向量，$\mathfrak{sim}(3)$的导数应该是4x7的雅可比矩阵。为了方便起见，记$Sp$的前三维组成向量q,那么：</p>
<p>$<br>\dfrac{ \partial Sp }{ \partial(\zeta) } = \begin{bmatrix}<br>I &amp; [-q]_{\times} &amp; q\\<br>0^T &amp; 0^T &amp; 0<br>\end{bmatrix}<br>$ </p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.《视觉SLAM十四讲》第四讲</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 李群李代数 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[李群李代数]]></title>
      <url>https://zhehangt.github.io/2017/03/15/SLAM/LieAlgebra-01/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>之前提到三维空间中的运动可以用旋转矩阵和变换矩阵来描述，旋转矩阵是一种<strong>特殊正交群SO(3)</strong>，而变换矩阵是<strong>特殊欧式群SE(3)</strong>。但是群到底是个什么玩意？这篇博客将会作简单介绍。</p>
<h1 id="李群"><a href="#李群" class="headerlink" title="李群"></a>李群</h1><p>群(Group)是一种集合加上一种运算的代数结构。把集合记作$A$，运算记做$\cdot$，那么群可以记做$G=(A,\cdot)$。群要求这个运算满足以下几个条件：<br>1.封闭性：$\forall a_1,a_2 \in A,\ \ a_1 \cdot a_2 \in A$<br>2.结合律：$\forall a_1,a_2,a_3 \in A, \ \ (a_1 \cdot a_2)\cdot a_3 = a_1 \cdot (a_2 \cdot a_3)$<br>3.幺元：$\exists a_0 \in A, \ s.t. \ \forall a \in A, \ a_0 \cdot a = a \cdot a_0 = a$<br>4.逆：$\forall a \in A, \ \exists a^{-1} \in A \ \ s.t. \ \ a \cdot a^{-1}=a_0$</p>
<p>可以验证旋转矩阵和矩阵乘法构成群，变换矩阵和矩阵乘法也构成群。<br>李群是指具有连续(光滑)性质的群。$SO(3)$和$SE(3)$在实数空间上是连续的，因此都是李群。</p>
<h1 id="李代数"><a href="#李代数" class="headerlink" title="李代数"></a>李代数</h1><p>考虑旋转矩阵$R$是正交矩阵，因此有：<br>$RR^T=I$<br>不妨设$R$会随时间变化，即为时间的函数$R(t)$，此时有：<br>$R(t)R(t)^T=I$ </p>
<p>对等式两边求导有：<br>$\dot{R}(t)R(t)^T+R(t)\dot{R}(t)^T=0$<br>$\dot{R}(t)R(t)^T=-(\dot{R}(t)R(t)^T)^T$</p>
<p>因此$\dot{R}(t)R(t)^T$是一个反对称矩阵，记对应的三维向量为$\phi(t)$，则$[\phi(t)]_{\times}=\dot{R}(t)R(t)^T$</p>
<p>此时有：<br>$\dot{R}(t)=[\phi(t)]_{\times}R(t)$ </p>
<p>也就是说对于每个旋转矩阵，存在一个反对称矩阵，两者的乘积即为这个旋转矩阵的导数。其实这个反对称矩阵对应的三维向量就是李代数。每个李群都有与之对应的李代数。李代数描述了李群的局部性质。</p>
<p>旋转矩阵$SO(3)$对应的李代数为$\mathfrak{so}(3)$。</p>
<p>变换矩阵$SE(3)$对应的李代数为$\mathfrak{se}(3)$。值得注意的是$\mathfrak{se}(3)$位于一个六维空间中。假设$\mathfrak{se}(3)$的元素为$\xi$，则有：<br>$\mathfrak{se}(3)=\{ \xi=\begin{bmatrix}<br>\rho \\<br>\phi<br>\end{bmatrix} \in \mathbb{R}^6,\rho \in \mathbb{R}^3, \phi \in \mathfrak{so}(3), [\xi]_{\times} = \begin{bmatrix}<br>[\phi]_{\times} &amp; \rho  \\<br>0^T &amp; 0<br>\end{bmatrix} \in \mathbb{R}^{4 \times 4}   \}$</p>
<p>$\xi$ 的前三维与变换矩阵中的平移向量相关，后三维则与旋转向量相关(其实就是旋转向量)，而$[\xi]_{\times}$不再严格代表反对称矩阵。</p>
<p>李代数的数学定义不展开详述了。</p>
<h1 id="指数与对数映射"><a href="#指数与对数映射" class="headerlink" title="指数与对数映射"></a>指数与对数映射</h1><p>通过求解$R$关于 $\phi$ 的微分方程我们可以得到两者之间存在指数关系，即：<br>$R=\mathbf{exp}([\phi]_{\times})$</p>
<p>但是矩阵作为指数是个什么鬼，要怎么算？在李群和李代数中，称为<strong>指数映射</strong>。</p>
<p>由于$\phi$是三维向量，可以定义$\phi=\theta a$。其中$\theta$表示方向，$a$表示大小。</p>
<p>用泰勒展开式可得：<br>$<br>\begin{equation}<br>\begin{aligned}<br>R = \mathbf{exp}([\phi]_{\times}) &amp;= \mathbf{exp}(\theta [a]_{\times}) = \sum^{\infty   }_{n=0} \dfrac{1}{n!}(\theta [a]_{\times})^n \\<br>&amp;= … \\<br>&amp;= \cos \theta I + (1-\cos\theta)aa^T + \sin \theta [a]_{\times}<br>\end{aligned}<br>\end{equation}<br>$</p>
<p>这个公式与我们在三维空间中的刚体运动里提到的旋转矩阵和旋转向量之间的转换一模一样。也就是说此处的指数映射就是罗德里格斯公式。至此我们得到了$\mathfrak{so}(3)$到$SO(3)$的转换。</p>
<p>那么如何从$SO(3)$中得到对应的$\mathfrak{so}(3)$呢？这就是李群和李代数中的<strong>对数映射</strong>。<br>在之前我们已经介绍过如何用旋转矩阵来求解旋转向量，此处的求解方式也是相同的。即：<br>$\theta = \arccos(\dfrac{tr(R)-1}{2})$<br>由于旋转轴上的向量在旋转后不发生改变，说明：<br>$Rn = n$<br>因此转轴$n$是矩阵$R$特征值1对应的特征向量。求解此方程，再归一化，就得到了旋转轴。</p>
<p>需要注意的是从$R$中求解 $\phi$ 并非是一一对应的。从一个$R$中可以求解多个相应的$\phi$。从几何的角度来理解，旋转是具有周期性的，因此会存在多个$\phi$。 </p>
<p>说了$SO(3)$，我们再说说$SE(3)$。$SE(3)$和$\mathfrak{se}(3)$之间同样存在指数映射和对数映射。<br>指数映射形式如下：</p>
<p>$<br>T = \mathbf{exp}([\xi]_{\times}) = \begin{bmatrix}<br>R &amp; J\rho\\<br>0^T &amp; 1<br>\end{bmatrix}<br>$</p>
<p>其中$J=\dfrac{\sin \theta}{\theta} I + (1- \dfrac{\sin \theta}{\theta})aa^T + \dfrac{1-\cos \theta}{\theta}[a]_{\times}$</p>
<p>从$SE(3)$到$\mathfrak{se}(3)$的求解往往不会直接使用对数映射，而是转换到对于的$\mathfrak{so}(3)$上。对于$\rho$的求解可以构造$t=J\rho$的方程。</p>
<p>至此旋转矩阵和变换矩阵与对应的李代数之间的转换方式就讲完了。至于李代数到底有什么用我们留到下一篇再讲。</p>
<p>总结起来就是下图。</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/LieAlgebra-01.png">
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.《视觉SLAM十四讲》第四讲</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 李群李代数 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[3D-2D的运动估计]]></title>
      <url>https://zhehangt.github.io/2017/03/07/SLAM/PnP/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>之前提到通过对极几何的方式来求解相机的运动估计，即2D-2D的运动估计。但是通过对极几何来对相机进行运动估计会面临几个问题。<br>1.存在初始化的问题<br>2.不能求解纯旋转<br>3.尺度不一致问题</p>
<p>因此当已知特征点在空间中的三维坐标以和图像对应的像素坐标，我们不再使用对极几何的方式进行相机的运动估计，而采用3D-2D的方式进行运动估计。这种求解方法称为PnP(Perspective-n-Point)。</p>
<p>对于单目相机，特征点的三维坐标的求解需要先通过对极几何初始化，然后通过三角化测量求得。<br>对于双目或RGB-D相机，深度信息直接可得，因此可以直接使用PnP进行。</p>
<p>3D-2D方法不需要使用对极约束，最少可以用三个匹配点获得运动估计，因此是一种非常重要的相机运动估计方式。</p>
<p>PnP有很多种求解方式，例如用三对点进行估计的P3P，直接线性变换(DLT)，EPnP，UPnP等。此外还有非线性优化的方式，构建最小二乘问题并迭代求解，也就是常用的Bundle Adjustment。</p>
<p>在SLAM问题中，通常的做法是先使用P3P或者EPnP等方法估计相机位姿，然后构建最小二乘优化问题对估计值进行调整(Bundle Adjustment)。因此我们将从这两个方面出发来完成3D-2D运动估计。</p>
<p>在2D-2D的运动估计中，我们通过两幅图像中的特征点像素坐标，通过计算得到了运动估计。<br>在3D-2D的运动估计中，我们将利用空间点的三维坐标和图像中的特征点像素坐标，来进行运动估计。</p>
<h1 id="P3P"><a href="#P3P" class="headerlink" title="P3P"></a>P3P</h1><p>P3P仅需要使用三对匹配点，就可以完成相机的位姿估计。</p>
<p>先从几何的角度出发，假设空间中有$A,B,C$三点，投影到成像平面中有$a,b,c$三点，在PnP问题中，$A,B,C$在世界坐标系下的坐标是已知的，但是在相机坐标系下的坐标是位置的。$a,b,c$的坐标是已知的。PnP的目的就是要求解$A,B,C$在相机坐标下的坐标值。如下图所示。需要注意的是三角形$abc$和三角形$ABC$不一定是平行的。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PnP-02.png"></p>
<p>根据余弦定理有：<br>$<br>OA^2 + OB^2 - 2OA \cdot OB \cdot \cos(a,b) = AB^2 \\<br>OB^2 + OC^2 - 2OB \cdot OC \cdot \cos(b,c) = BC^2 \\<br>OA^2 + OC^2 - 2OA \cdot OC \cdot \cos(a,c) = AC^2<br>$</p>
<p>记$x=\dfrac{OA}{OC}$，$y=\dfrac{OB}{OC}$，因为$A,B,C$在相机坐标系中的坐标未知，因此$x$，$y$是未知的。<br>另记$u=\dfrac{BC^2}{AB^2}$，$w=\dfrac{AC}{AB}$，根据$A,B,C$的世界坐标，$u,w$是可以求出的。<br>通过一系列的转化可以得到两个等式：</p>
<p>$(1-u)y^2-ux^2-\cos(b,c)y+2uxy \cos(a,b) +1 = 0 \\<br>(1-w)x^2-wy^2-\cos(a,c)x+2wxy \cos(a,b) +1 = 0$ </p>
<p>该方程组是关于x,y的一个二元二次方程，可以通过吴消元法求解。最多可能得到四个解，因此在三个点之外还需要一组匹配点进行验证。<br>至此，通过x和y就可以求得A，B，C在相机坐标下的坐标值。因此3D-2D问题转变成了3D-3D的位姿估计问题。而带有匹配信息的3D-3D位姿求解非常容易。<br>关于3D-3D的位姿估计，我们留到下一篇讲。</p>
<p>P3P只利用三组匹配点的信息，加一组匹配点用于验证。当给定的匹配点很多时，难以利用更多的信息。而且如果匹配点噪声明显或者匹配错误，P3P算法会失败。因此大牛们在此基础上又提出了EPnP、UPnP等方法。等有时间再谈一谈这些解法的思路。</p>
<h1 id="Bundle-Adjustment"><a href="#Bundle-Adjustment" class="headerlink" title="Bundle Adjustment"></a>Bundle Adjustment</h1><p>假设某空间点坐标为$P_i = [X_i, Y_i, Z_i]$，其投影的像素坐标为$p_i=[u_i,v_i]$。这些在PnP问题里都是已知的。在相机坐标系下有$c=[x_i, y_i, z_i]$，这个坐标通过P3P或者其他解法有了粗略的估计。根据针孔相机模型可得：</p>
<p>$z_i p_i = KTP_i = K \exp([\xi]_{\times})P_i$ </p>
<p>根据这个等式可以构造出一个最小二乘问题：</p>
<p>$ \xi^* = \arg \min \limits _{\xi} \dfrac{1}{2} \sum\limits  _{i=1} ^n \begin{Vmatrix}<br>p_i - \dfrac{1}{z_i} K \exp([\xi]_{\times})P_i<br>\end{Vmatrix} _2 ^2$</p>
<p>该问题的误差项，是将像素坐标与3D点按照当前估计的位姿进行投影得到的位置相比较得到的误差，所以称之为<strong>重投影误差</strong>。如图下图所示。</p>
<img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/PnP-01.png">
<p>这个最小二乘问题主要优化两个变量，第一是对相机位姿的优化，也就是对李代数的优化，第二是对空间点$P$的优化，也就是P点的优化。因此涉及到了两个关键的求导问题。</p>
<p>关于李代数的求导，涉及到之前介绍的扰动模型进行，此处直接给出求导结果：<br>$\dfrac{\partial e}{\partial \Delta\xi} = -\begin{bmatrix}<br>\frac{f_x}{z} &amp; 0  &amp; - \frac{f_x x}{z^2} &amp; -\frac{f_xxy}{z^2} &amp; f_x+\frac{f_xx^2}{z^2}  &amp; -\frac{f_xy}{z} \\<br>0 &amp; \frac{f_y}{z} &amp; -\frac{f_y y}{z^2} &amp; -f_y-\frac{f_yy^2}{z^2} &amp; \frac{f_y xy}{z^2} &amp; \frac{f_y x}{z}<br>\end{bmatrix}$ </p>
<p>关于位姿的求导，有：</p>
<p>$\dfrac{\partial e}{\partial \Delta\xi} = -\begin{bmatrix}<br>\frac{f_x}{z} &amp; 0  &amp; - \frac{f_x x}{z^2}  \\<br>0 &amp; \frac{f_y}{z} &amp; -\frac{f_y y}{z^2}<br>\end{bmatrix}R$ </p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>《视觉SLAM十四讲》第七讲</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> PnP </tag>
            
            <tag> 3D-2D </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[单目相机中的三角化测量]]></title>
      <url>https://zhehangt.github.io/2017/03/06/SLAM/Triangularization/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>利用两幅图像以及匹配的特征点，我们已经能够利用对极几何估计相机的运动，即得到旋转矩阵$R$和平移向量$t$。但是如何进一步得到特征点在三维空间中的位置？这就涉及到三角化测量的方法。</p>
<h1 id="三角化测量"><a href="#三角化测量" class="headerlink" title="三角化测量"></a>三角化测量</h1><p>假设P是三维空间中的某一点，在两幅图像中的像素坐标分别为$p_1$和$p_2$，在相机坐标系下的坐标为$c_{1}$和$c_{2}$，根据针孔相机中的坐标变换公式可知：<br>$z_{1}p_1=Kc_{1}$<br>$z_{2}p_2=Kc_{2}$<br>其中$z_{1}$是$c_{1}$在$Z$轴上的坐标，$z_{2}$是$c_{2}$在$Z$轴上的坐标，$K$是相机的内参矩阵。</p>
<p>不妨令：<br>$p_1=Kx_{1}$<br>$p_2=Kx_{2}$<br>其中$x_{1},x_{2}$称为点P在相机坐标系下的归一化坐标。</p>
<p>根据三维空间中的运动模型有：<br>$c_2 = Rc_1 + t \\<br>k^{-1}z_1p_1 = R k^{-1}z_2p_2 + t \\<br>z_1 x_1 = z_2 R x_2 + t$</p>
<p>同时左乘$x_1$的反对称矩阵可得：</p>
<p>$z_1 [x_1]_{\times} x_1 = 0 = z_2 [x_1]_{\times}Rx_2 + [x_1]_{\times}t$</p>
<p>上述等式的右边是个关于未知变量$z_2$的方程，求解此方程即可得到$z_2$。有了$z_2$，$z_1$也就迎刄而解了。</p>
<p>但由于误差的存在，上述方程通常不会严格成立，所以更常见的做法是求最小二乘解而不是零解。</p>
<h1 id="三角测量的矛盾"><a href="#三角测量的矛盾" class="headerlink" title="三角测量的矛盾"></a>三角测量的矛盾</h1><p>三角化测量的前提是两幅图像之间存在平移。有平移才会由对极几何中的三角形，才谈得上三角化测量。<br>如下图所示，当平移很小时，深度估计的不确定性会增大。因此要使深度的精确度增加，则需要增加平移，但是增加平移会导致图像的外观发生明显的变化，使得特征提取和匹配的难度增加，这称为三角测量的矛盾。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/Triangularization-01.png"></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.《视觉SLAM十四讲》第七讲</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 三角化测量 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[单目相机中的对极几何]]></title>
      <url>https://zhehangt.github.io/2017/03/05/SLAM/EpipolarGeometry/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在图像特征匹配中我们提到，有了特征匹配，就可以估计相机的位姿变换，这个过程被称为对极几何。</p>
<h1 id="对极几何"><a href="#对极几何" class="headerlink" title="对极几何"></a>对极几何</h1><img src="http://oeaxm0g1o.bkt.clouddn.com/images/EpipolarGeometry-01.jpg">
<p>假设相机由如图的位姿变换。$P$为空间中的某一点，在两幅图像中的像素坐标分别为$p_1$和$p_2$，在相机坐标系下的坐标为$c_{1}$和$c_{2}$，根据针孔相机中的坐标变换公式可知：<br>$z_{1}p_1=Kc_{1}$<br>$z_{2}p_2=Kc_{2}$<br>其中$z_{1}$是$c_{1}$在$Z$轴上的坐标，$z_{2}$是$c_{2}$在$Z$轴上的坐标，$K$是相机的内参矩阵。</p>
<p>由于$C_0,C_1,P$三点共面，所以有：<br>$\overrightarrow{C_0 P} \cdot (\overrightarrow{C_0 C_1}\times \overrightarrow{C_1 P}) = 0 \Rightarrow  c_{1} \cdot (t\times Rc_{2}) = 0 $<br>其中$R$为旋转矩阵，$t$为平移向量。</p>
<p>又因为两个向量的叉乘可以表示为第一个向量的反对称矩阵乘以另一个向量，因此上式可以转变为：<br>$ c_{1}^T  [t]_{\times} R c_{2} = 0 $<br>其中$[t]_{\times}$为反对称矩阵，具体形式为：<br>$[t]_{\times}=\begin{bmatrix}<br>0 &amp; -a_3 &amp; a_2 \\<br>a_3 &amp; 0 &amp; -a_1 \\<br>-a_2 &amp; a_1 &amp; 0<br>\end{bmatrix}$</p>
<p>代入$p_1$和$p_2$可得：<br>$ p_{1}^T K^{-T} [t]_{\times} R K^{-1} p_{2} = 0 $</p>
<p>一般用$E=[t]_{\times} R$，称为<strong>本质矩阵</strong>(Essential Marix)。<br>一般用$F=K^{-T} [t]_{\times} R K^{-1}$，称为<strong>基础矩阵</strong>(Fundamental Matrix)。<br>因此上式可以转变为：<br>$c_{1}^T E c_{2} = 0$<br>$p_{1}^T F p_{2} = 0$<br>这两个等式称为<strong>对极约束</strong>。</p>
<p>但是在2D-2D问题中，我们只有像素坐标$p_{1}$和$p_{2}$，$c_{1}$和$c_{2}$都是未知的，那该怎么办？<br>此时可以引入一个归一化坐标的概念。即令：<br>$<br>x_1 = \dfrac{c_1}{z_1} = K^{-1}p_1 \\<br>x_2 = \dfrac{c_2}{z_2} = K^{-1}p_2<br>$</p>
<p>因此有：<br>$<br>x_1Ex_2=0<br>$<br>其中$x_1$和$x_2$可以从像素坐标$p_1$和$p_2$中求得。$E$和$F$之间存在转换关系：$F=K^{-T}EK^{-1}$</p>
<p>对极约束简洁地给出了两幅图像之间的空间位姿关系，于是相机位姿估计问题变为以下两步：<br>1.根据两幅图像的像素坐标，求出$E$或者$F$。<br>2.根据$E$或者$F$，求出$R$，$t$。</p>
<h1 id="本质矩阵求解"><a href="#本质矩阵求解" class="headerlink" title="本质矩阵求解"></a>本质矩阵求解</h1><p>本质矩阵相对于基础矩阵来说，相差一个相机的内参矩阵，在SLAM问题中，这个矩阵往往是已知的。因此在估计相机位姿变化时，通常求解形式相对简单的本质矩阵。</p>
<p>考虑一对匹配点的像素坐标$p_1$为$[u_1, v_1, 1]$，$p_2$为$[u_2, v_2, 1]$，则根据对极约束有:</p>
<p>$(u_1, v_1, 1)\begin{pmatrix}<br>e_1 &amp; e_2 &amp;  e_3 \\<br>e_4 &amp; e_5&amp; e_6 \\<br>e_7 &amp; e_8&amp; e_9<br>\end{pmatrix}(u_2, v_2, 1)=0$</p>
<p>另$e=[e_1, e_2,…,e_9]^T$，则上式可转化为：<br>$[u_1u_2, u_1v_2, u_1, v_1u_2, v_1v_2, v_1 , u_2, v_2, 1] \cdot e = 0$<br>对于这个线性方程，存在尺度等价性，即对$e$乘以任何常数，等式依旧成立。因此即使$e$有9个未知变量，只需要8个方程，构成线性方程组即可对$e$进行求解。这就是求解本质矩阵最经典的<strong>八点法</strong>。</p>
<p>旋转矩阵是一个正交矩阵，因此从本质矩阵恢复t和R通常对本质矩阵进行奇异值分解：<br>$E=U \Sigma V^T$<br>则：<br>$R=UR_Z^T(\pm \frac{\pi}{2})V^T$<br>$[T]_{\times}=U R_Z^T(\pm \frac{\pi}{2}) \Sigma U^T$</p>
<p>其中$R_Z^T(\pm \frac{\pi}{2})$表示绕Z轴旋转90度得到的旋转矩阵。<br>因此$R$,$T$共有4种组合情况。如图所示。<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/EpipolarGeometry-02.png"><br>只有第一种解正确的，因为P点在两个相机中的深度都是为正的。因此只要把任意一点代入四种解中，检测该点在两个相机下的深度，就可以确定哪个解是正确的。</p>
<p>本质矩阵有几个非常重要的<strong>特性</strong> ：<br>1）基础矩阵是由对极约束定义的。由于对极约束是等式为零的约束，所以对$E$乘以任意非零常数后，对极约束依然满足。因此$E$具有不同尺度下的尺度等价性。<br>2）根据$E=[t]_{\times}R$可以证明，本质矩阵E的奇异值必定是$[\sigma ,\sigma ,0]^T$的形式，这称为本质矩阵的内在性质。<br>3）由于平移和旋转各有三个自由度，故$[t]_{\times}$共有6个自由度。但由于E具有尺度等价性，故$E$实际上有五个自由度。</p>
<p>因为一些误差的存在，通过8点法计算得到的本质矩阵$E$可能不会严格满足上述的特性2，因此通常会对奇异值矩阵做调整。通常的做法是，对八点法求得的E进行SVD分解后，会得到奇异值矩阵$\Sigma=\textbf{diag}(\sigma_1, \sigma_2, \sigma_3)$。不妨设$\sigma_1 \geq  \sigma_2 \geq \sigma_3$，取：<br>$E = U \textbf{diag}(\dfrac{\sigma_1 + \sigma_2}{2},\dfrac{\sigma_1 + \sigma_2}{2},0)$<br>由于$E$具有尺度等价性，因此更简单的做法是将奇异值矩阵取为$\textbf{diag}(1,1,0)$。</p>
<h1 id="单应矩阵"><a href="#单应矩阵" class="headerlink" title="单应矩阵"></a>单应矩阵</h1><p>除了基本矩阵和本质矩阵，还有一种单应矩阵(Homography)$H$，它描述了两个平面之间的映射关系。若场景中的特征点都落同一个平面上(比如墙，地面等)，则可以通过单应性来进行运动估计。</p>
<p>假设P在三维空间中的某个平面上，这个平面满足：<br>$n^TP+d=0$</p>
<p>则有：<br>$p_2 = K(R-\dfrac{tn^T}{d})K^{-1}p_1$</p>
<p>中间部分记为$H$，称为单应矩阵。于是<br>$p_2 = Hp_1$</p>
<p>单应矩阵的定义与旋转、平移以及平面的参数有关。对上式展开可得：</p>
<p>$\begin{pmatrix}<br>u_2 \\<br>v_2 \\<br>1<br>\end{pmatrix}=\begin{pmatrix}<br>h_1 &amp; h_2 &amp;  h_3 \\<br>h_4 &amp; h_5&amp; h_6 \\<br>h_7 &amp; h_8&amp; h_9<br>\end{pmatrix}\begin{pmatrix}<br>u_1 \\<br>v_1 \\<br>1<br>\end{pmatrix}$</p>
<p>简单转换可得：<br>$u_2 = \dfrac{h_1 v_1+h_2 v_1+h_3}{h_7 v_1+h_8 v_1+h_9}$</p>
<p>$v_2 = \dfrac{h_4 v_1+h_5 v_1+h_6}{h_7 v_1+h_8 v_1+h_9}$</p>
<p>这样一组匹配点就可以构造2个方程。因为像素坐标为齐次坐标，所以单应矩阵乘以任意常数项，等式仍然成立。因此即使单应矩阵的未知变量为9个，但其自由度为8。因此需要4组匹配点，构造8个方程，来对单应矩阵进行求解。与本质矩阵相似，求出单应矩阵后需要对其进行分解，才可以得到相应的旋转矩阵R和平移向量t。</p>
<p>单应性在SLAM中具有重要意义。当特征点共面时或相机发生纯旋转时，基础矩阵的自由度下降，这就出现了所谓的退化。这时候继续用8点法求解基础矩阵会导致受到噪声的影响增加。为了避免这种情况，通常会同时估计基础矩阵F和单应矩阵H，选择重投影误差比较小的那个作为最终的运动估计矩阵。</p>
<h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>首先由于$E$具有尺度等价性，因此它分解得到的$t$和$R$也有一个尺度等价性。而$R\in SO(3)$自身具有约束，因此可以认为$t$也具有尺度等价性。$t$的尺度等价性直接导致$t$的大小与现实世界中平移的大小并不是对应的。也就是说在单目SLAM中，每次都要确定$t$的尺度，通常的做法是对$t$进行归一化。这称为单目SLAM的<strong>初始化</strong>。初始化之后就可以用3D-2D来计算相机运动，初始化之后的轨迹和地图的单位，就是初始化时固定的尺度。<br>其次即使可以用单应矩阵来处理纯旋转的情况，但是由于纯旋转缺少位移，因此无法对空间点进行三角化。也就是说对于单目SLAM来说，初始化必须要有位移。<br>最后我们提到用8点法求解本质矩阵。但是实际中往往匹配点会远远多于8对，此时有两种求解思路。<br>当匹配正确率比较高的时候，可以构造最小二乘。<br>当匹配错误率比较高的时候，可以用随机采样一致性$\textrm{(RANSAC)}$求解。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>《视觉SLAM十四讲》第七讲</li>
<li><a href="http://blog.csdn.net/heyijia0327/article/details/50758944" target="_blank" rel="external">Monocular slam 的理论基础(1)</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 对极几何 </tag>
            
            <tag> 2D-2D </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[图像特征匹配]]></title>
      <url>https://zhehangt.github.io/2017/03/03/SLAM/FeatureMatching/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>视觉SLAM是指通过摄像头采集周围环境的图像，根据图像来进行定位和建图。也就是说可以通过一系列的图像，来计算机器人的位姿。<br>基于特征点法的视觉SLAM是目前比较成熟的解决方案。这篇博客将简单聊一聊什么是特征点，以及特征点的匹配问题。</p>
<h1 id="特征点"><a href="#特征点" class="headerlink" title="特征点"></a>特征点</h1><p>我们知道图像在计算机中用像素矩阵来表示，而特帧是图像信息的另一种数字表达形式。那有了像素矩阵为什么还要搞出一套特征表示出来？主要出于以下几个考虑：<br>1）我们希望在环境选取一些比较有代表性的点，这些点在相机视角发生少量变化后，依旧可以从各个图像中找到。如果用像素矩阵直接表示这些点，不同图像之间变化会非常大，不容易找到。图像特征就是来解决这个问题，这些点被称为特征点。<br>2）用特征点表示可以降低变量的维度，有利于大规模的计算。</p>
<p>在计算机视觉领域，大牛们经过长期的研究，设计了很多很牛逼的特征点，如SIFT，SURF，ORB等等。为什么说这些特征点牛逼？主要有以下几个方面：<br>1）可重复性：环境中相同的区域，可以在不同的图像中被找到<br>2）可区别性：环境中不同的区域，计算得到的特征表示不同<br>3）高效性：在同一图像中，特征点的数量远小于像素的数量<br>4）本地性：特征仅与一小片图像区域有关</p>
<p>特征点由关键点和描述子两部分组成，比如当我们谈论ORB特征是，是指提取ORB关键点，并计算ORB描述子两件事情。关键点是值该特征点在图像里的位置，有些特征点还具有朝向、大小等信息。描述子描述了该关键点周围像素的信息，两个特征点的描述子在向量空间上的距离可以用来表示其相似程度，距离越小越相似。</p>
<p>在目前的SLAM方案中，ORB是质量和性能之间较好的折中，因此将着重介绍ORB特征。</p>
<h1 id="ORB特征"><a href="#ORB特征" class="headerlink" title="ORB特征"></a>ORB特征</h1><p>ORB特征的关键点称为“Oriented Fast”，是一种改进的FAST角点。描述子称为BRIEF。</p>
<p>先简单介绍一下什么是FAST角点。FAST角点的主要思想是：如果一个像素与它邻域的像素差别较大，那它可能是角点。相比于其他角点检测算法，FAST只需要比较像素亮度的大小，因此计算速度非常快。</p>
<p>FAST角点的<strong>计算方法</strong>：<br>对于图像中某个像素$p$，亮度为$I_p$，设置一个阈值$T$。以像素$p$为中心，选取半径为3的圆上的16个像素点。如果有连续$N$个点的亮度大于$I_p+T$或小于$I_p-T$，那么像素$p$可认为是FAST角点。</p>
<p>FAST角点的<strong>缺点</strong>：<br>数量大且不确定。ORB做了改进。首先对原始的FAST角点分别计算Harris响应值，然后选取前N个具有最大响应值的角点，作为最终的角点集合。<br>FAST角点不具有方向信息，并且由于它固定取半径为3的圆，因此存在尺度问题：远看像角点的地方，接近以后看可能就不是角点了。ORB在FAST角点的基础上添加了尺度和旋转的描述。尺度不变性由构造图像金字塔，并在金字塔的每一层上检测角点来实现，金字塔是指对图像进行不同层次的降采样，以获得不同分辨率的图像。而特征的旋转是由灰度质心法实现的。其方向的定义就是几何中心$O$与质心$C$。</p>
<p>有了关键点之后就是为每个关键点计算其描述子。ORB采用的描述子是为BRIEF添加了方向信息，因此称为Steer BRIEF。简单的来说就是随机的挑选关键点周围的两个像素，对比其大小，根据大小关系设置为0或1。因此得到的描述子是由许多个0和1组成的描述向量。具体的计算方法不展开说了。</p>
<h1 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h1><p>考虑到两个时刻的图像，如果在图像$I_t$中提取到特征点$x_t^m$，在图像$I_{t+1}$提取到特征点$x_{t+1}^n$，如何寻找这两个集合元素的对应关系呢？之前我们提到描述子在向量空间上的距离可以用来表示其相似程度，距离越小越相似。因此最简单的办法就是对每个特征点$x_t^m$，计算它与所有的$x_{t+1}^n$之间距离，然后选择其中距离最小，作为匹配点。这种做法称为暴力匹配。<br>但是当特征点数量很大时，暴力匹配法的运算量将变得很大，因此通常用快速近似最近邻(FLANN)方法来进行匹配。<br>当完成了图像之间的特征匹配，就可以利用这些匹配来计算，为了拍摄这两张图像相机经过了怎样的位姿变换。这部分内容，下回分解。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.《视觉SLAM十四讲》第七讲</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 特征匹配 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SLAM中的非线性优化]]></title>
      <url>https://zhehangt.github.io/2017/03/02/SLAM/NonlinearOptimization/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在SLAM的基本概念中，我们提到可以用非线性优化的方式来对SLAM问题进行求解。<br>这篇博客将从非线性优化的角度，谈一谈对SLAM问题求解的思路。</p>
<h1 id="SLAM问题"><a href="#SLAM问题" class="headerlink" title="SLAM问题"></a>SLAM问题</h1><p>回顾SLAM问题的运动方程和观测方程。<br>运动方程：$x_k=f(x_{k-1}, u_k)$<br>观测方程：$z_k = h(x_k)$</p>
<p>SLAM问题求解的关键，就是对$x$进行估计，使得运动方程和观测方程尽可能的成立。<br>根据运动方程和测量方程，可以构造误差项：<br>$e_{x,k} = x_k - f(x_{k-1},u_k)$<br>$e_{z,k} = z_k - h(x_k)$</p>
<p>因此可以将误差项构造成最小二乘问题：<br>$ J(x)=\sum_k e_{x,k}^T R_k^{-1} e_{x,k} + \sum_ke_{z,k}^T Q_k^{-1} e_{z,k}  $</p>
<h1 id="最小二乘问题"><a href="#最小二乘问题" class="headerlink" title="最小二乘问题"></a>最小二乘问题</h1><p>对于一个函数形式比较简单的最小二乘问题，可以通过令目标函数导数为0，得到极值。它们可能是极大、极小或鞍点处的值，只要挨个比较它们的函数值大小即可。<br>遗憾的是，在SLAM问题中，函数形式往往比较复杂，因此通常通过迭代的方式求解。也就是从一个初始值$x_0$出发，不断地更新当前的优化变量$x_k=x_{k-1}+\Delta x$，使函数值不断减小，最终得到目标函数的最小值和对应的变量值。因此问题的关键变为了如何求解$\Delta x$。对于$\Delta x$的计算是优化领域中非常关键的问题，最常用的有两种方法：高斯牛顿法和列文伯格-马夸尔特方法。<br>对于这两种方法的详细介绍，等我看懂了，再另写博客。(&gt; &lt;)</p>
<h1 id="图优化理论"><a href="#图优化理论" class="headerlink" title="图优化理论"></a>图优化理论</h1><p>在SLAM问题中构造的最小二乘问题有一个特殊的地方在于虽然整个问题的目标函数的变量维度很高，但每个误差项都是简单的，仅与少量变量有关。因此为了直观的展示变量与变量之间的关系，可以把优化问题用图的方式来展现。其中顶点表示优化变量，边表示误差项。用图来表示一个优化问题有如下几个好处：<br>1）可以直观展示变量之间的关系结构，非常符合SLAM问题的表述<br>2）可以利用一些图论的方法来改进优化问题，比如去掉孤立顶点，优先优化边数较多的顶点等<br>3）在SLAM问题中，用图的这种形式可以非常方便的添加优化变量。</p>
<p>基于以上几个好处，图优化理论几乎已经称为SLAM问题中非线性优化的标准解法。<br>顺便提一句，g2o用c++实现了利用图优化进行优化求解的方法，很多开源的SLAM系统中都会用到g2o。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.《视觉SLAM十四讲》第六讲</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 非线性优化 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SLAM的基本概念]]></title>
      <url>https://zhehangt.github.io/2017/03/01/SLAM/SLAMConcept/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>SLAM到底是个啥？SLAM问题的求解思路是怎么样的？<br>这是一篇简单记录学习SLAM基本概念的博客。</p>
<h1 id="SLAM到底是个啥？"><a href="#SLAM到底是个啥？" class="headerlink" title="SLAM到底是个啥？"></a>SLAM到底是个啥？</h1><p>SLAM问题可以描述为: 机器人在某个未知环境中从某个未知位置开始移动,在移动过程中根据传感器数据进行自身定位估计，同时在自身定位的基础上增量式构造地图，从而实现机器人对未知环境的地图构建和在地图中对自身的位置进行定位。对于机器人来说，SLAM主要回答了两个问题：1）我在什么地方？ 2）周围环境是怎么样的？</p>
<p>要回答这两个问题，机器人需要通过传感器采集数据，然后根据这些数据推断出自身的位姿信息和所处的环境信息。假设机器人在某个时间$t$内通过传感器获得一系列连续的传感器数据，即观测值$Z$。依据机器人所配备的传感器，观测值$Z$可以是激光雷达数据，图像数据，里程计数据和惯性导航单元数据等。SLAM需要解决的就是通过观测值$Z$评估系统状态$X$。系统状态通常包含两部分，一部分用于表示机器人在环境中的位置，另一部分用于表示环境地图。通常，机器人会携带一个测量自身运动的传感器，比如里程计。因此可以构造一个评估函数，利用当前获得的传感器数据，从前一时刻的系统状态评估当前时刻的系统状态，如下所示：<br>$X_k=f(X_{k-1}, U_k,w_k)$<br>这里的$U_k$是运动传感器的数据，$w_k$为噪声。这里的$f$指代一种计算模型，当输入的运动传感器类型不同时，$f$的具体形式会千差万别。我们通常把它称为<strong>运动方程</strong>。</p>
<p>与运动方程相对应是<strong>观测方程</strong>。观测方程描述的是当机器人在$X_k$位置上利用传感器感知环境，产生了观测数据$Z_k$。此处同样用一个抽象的函数$h$来表述这个关系：<br>$Z_k = h(X_k, v_k)$<br>这里$v_k$表示此次观测的噪声。由于观测所用的传感器形式更多，因此这里的观测数据$Z$以及观测方程$h$也有许多不同的形式。</p>
<p>运动方程和观测方程描述了最基本的SLAM问题。因此SLAM问题可以作为状态估计问题，其中$Z$和$U$是已知的。当选定了传感器，运动方程和观测方程也是已知的，因此SLAM求解的目标就是对$X$进行估计，使得<strong>运动方程</strong>和<strong>观测方程</strong>等式两边尽可能的成立。</p>
<h1 id="SLAM问题的解决思路"><a href="#SLAM问题的解决思路" class="headerlink" title="SLAM问题的解决思路"></a>SLAM问题的解决思路</h1><p>从应用的角度来看，SLAM问题涉及很多方面，包括传感器的选择，对$X$的估计方式，地图的表示形式等。<br>从传感器的角度来讲，SLAM依赖的传感器主要包括激光传感器和视觉传感器两大类。<br>从$X$的估计方式来讲，SLAM的求解思路主要包括基于滤波的求解和基于非线性优化的求解。目前普遍认为非线性优化的方法要由于滤波方法。<br>从地图的表示形式来讲，SLAM得到的地图表示方式主要分为度量地图与拓扑地图，度量地图有二维，三维，稀疏，稠密等多种表现形式。</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>关于博客中提到的所有方面，等我懂了再写。(&gt; &lt;)</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[三维空间中的刚体运动]]></title>
      <url>https://zhehangt.github.io/2017/02/24/SLAM/RigidMotion/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>SLAM中一个很基本的问题就是计算机器人在三维空间中的位姿。位姿包括两个部分，第一是位置，其次是姿态。而机器人在三维空间中位姿的计算往往与三维空间中的刚体运动有关。<br>这篇博客是关于学习三维空间中如何表示位姿的简单记录。</p>
<h1 id="位置表示"><a href="#位置表示" class="headerlink" title="位置表示"></a>位置表示</h1><p>在一个三维空间中，建立三维坐标系之后，就可以用一个三维坐标来表示机器人的位置。<br>对于机器人的位置变换，可以用一个三维的平移向量来表示，比如机器人从初始位置为$(x_0, y_0, z_0)$经过一个平移向量$(a,b,c)$，可以得到平移后的位置$(x_0+a, y_0+b, z_0+c)$。相对来说，机器人的位置表达比较简单。</p>
<h1 id="姿态表示"><a href="#姿态表示" class="headerlink" title="姿态表示"></a>姿态表示</h1><p>在一个三维空间中，通常用一个三维向量来表示机器人的姿态。更直观的讲，机器人的姿态可以想象成机器人自带一个坐标系，这个坐标系的原点就是机器人，z轴表示机器人面向的方向，这样的一个坐标系可以通过表示机器人姿态的三维向量来构造。<br>对于机器人的姿态则相对复杂。通常有欧拉角，四元组，旋转向量，旋转矩阵等。</p>
<h1 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h1><p>在SLAM问题中，旋转矩阵是表示姿态变换最常用的方式。<br>假设空间中的某一点$P$，机器人旋转前的坐标系的单位正交基为$(e_1,e_2,e_3)$，$P$的坐标为$(x_1,y_1,z_1)$。机器人旋转后的坐标系的单位正交基为$(e’_1,e’_2,e’_3 )$，$P$的坐标为$(x’_1,y’_1,z’_1)$。<br>根据坐标的定义有：</p>
<p>$\begin{bmatrix}<br>e_1 &amp; e_2 &amp; e_3<br>\end{bmatrix}\begin{bmatrix}<br>a_1\\<br>a_2\\<br>a_3<br>\end{bmatrix}=\begin{bmatrix}<br>e’_1 &amp; e’_2 &amp; e’_3<br>\end{bmatrix}\begin{bmatrix}<br>a’_1\\<br>a’_2\\<br>a’_3<br>\end{bmatrix}$</p>
<p>在等式的左右同时左乘$\begin{bmatrix}<br>a_1^T\\<br>a_2^T\\<br>a_3^T<br>\end{bmatrix}$，可得</p>
<p>$\begin{bmatrix}<br>a_1\\<br>a_2\\<br>a_3<br>\end{bmatrix}=\begin{bmatrix}<br>e_1^T e’_1 &amp; e_1^T e’_2 &amp; e_1^T e’_3 \\<br>e_2^T e’_1 &amp; e_2^T e’_2 &amp; e_2^T e’_3 \\<br>e_3^T e’_1 &amp; e_3^T e’_2 &amp; e_3^ Te’_3<br>\end{bmatrix}\begin{bmatrix}<br>a’_1\\<br>a’_2\\<br>a’_3<br>\end{bmatrix}=Ra’$</p>
<p>矩阵$R$即为旋转矩阵。旋转矩阵是一个行列式为1的正交矩阵。同样所有行列式为1的正交矩阵都是旋转矩阵，因此可以把旋转矩阵做如下定义：<br>$SO(n)=\{R\epsilon \mathbb{R}^{n\times n} \mid RR^T=I,det(R)=1\}$<br>$SO(n)$称为特殊正交群，这个集合由n维空间的旋转矩阵组成，特别的，SO(3)就是三维空间下的旋转矩阵。</p>
<p>值得注意的是，旋转矩阵本质上是表示两个坐标系之间的旋转，而坐标系恰恰能够表示机器人姿态，因此旋转矩阵也可以描述机器人的旋转，即姿态变换。</p>
<h1 id="变换矩阵"><a href="#变换矩阵" class="headerlink" title="变换矩阵"></a>变换矩阵</h1><p>在旋转矩阵的基础之上，加上平移向量，就可以完整的刻画三维空间中的刚体运动，即：<br>$a’=Ra+t$</p>
<p>但是这种形式下会存在一个问题，假设我们进行了两次变换$R_1$,$t_1$和$R_2$,$t_2$。相应的三维空间中经历了从a点到b点到c点的变换。则满足公式：<br>$b=R_1a+t_1, c=R_2b+t_2$</p>
<p>从a到c的变换为：<br>$c=R_2(R_1a+t_1)+t_2$</p>
<p>这样的形式在变换多次之后会过于复杂。聪明的数学家引入了齐次坐标和变换矩阵的概念，使得三维空间中的刚体运动有如下的变换形式：</p>
<p>$\begin{bmatrix}<br>a’\\<br>1<br>\end{bmatrix}=\begin{bmatrix}<br>R &amp; t \\<br>0^T &amp; 1<br>\end{bmatrix}\begin{bmatrix}<br>a\\<br>1<br>\end{bmatrix} = T\begin{bmatrix}<br>a\\<br>1<br>\end{bmatrix}$</p>
<p>矩阵T即称为变换矩阵</p>
<p>此时从a到c的变换可表示为：<br>$c=T_2T_1a$<br>此处的a和c为相应齐次坐标。</p>
<p>变换矩阵的左上角为旋转矩阵，右侧为平移向量，左下角为0向量，右下角为1。这种矩阵称为特殊欧式群：</p>
<p>$SE(3)=\{T=\begin{bmatrix}<br>R &amp; t \\<br>0^T &amp; 1<br>\end{bmatrix} \epsilon \mathbb{R}^{4 \times 4} \mid R \epsilon SO(3), t\epsilon \mathbb{R}^3 \}$</p>
<h1 id="旋转向量"><a href="#旋转向量" class="headerlink" title="旋转向量"></a>旋转向量</h1><p>用旋转矩阵来表示旋转有两个缺点：<br>1.$SO(3)$的旋转矩阵有九个量，但一次旋转只有三个自由度。因此这种表达方式是冗余的<br>2.旋转矩阵自身两个约束，即必须是正交矩阵和行列式为1，这些约束会使得求解变得更困难。</p>
<p>从直观上讲，任意旋转都可以用一个旋转轴和一个旋转角来刻画。于是可以使用一个向量，其方向与旋转轴一致，而长度等于旋转角。这种向量称为旋转向量，只需要一个三维向量就可以描述旋转。旋转向量跟之后要介绍的李代数是相对应的。旋转向量到旋转矩阵的变换可由<strong>罗德里格斯公式</strong>求得。<br>假设旋转轴为$n$，角度为$\theta$，则旋转向量为$\theta n$对应的旋转矩阵$R$为：<br>$R=\cos \theta {I} + (1-\cos \theta)nn^T + \sin \theta [n]_{\times}$</p>
<p>反之有：<br>$\theta = \arccos(\dfrac{tr(R)-1}{2})$<br>由于旋转轴上的向量在旋转后不发生改变，说明：<br>$Rn = n$<br>因此转轴$n$是矩阵$R$特征值1对应的特征向量。求解此方程，再归一化，就得到了旋转轴。</p>
<h1 id="欧拉角"><a href="#欧拉角" class="headerlink" title="欧拉角"></a>欧拉角</h1><p>欧拉角是一种最为直观的姿态变换描述方式。在欧拉角的表示方式中，将旋转分解成沿三个坐标轴旋转的量：滚转角－俯仰角－偏航角(roll-pitch-yaw)。欧拉角的一个重大缺点就是万向锁问题：在俯仰角为$\pm90$度时，第一次旋转与第三次旋转将使用同一个轴，这被称为奇异性问题。<br>由于欧拉角不适于插值和迭代，所以在SLAM问题中通常不使用欧拉角来表示旋转，所以不再展开记录。</p>
<h1 id="四元数"><a href="#四元数" class="headerlink" title="四元数"></a>四元数</h1><p>旋转矩阵具有冗余性，欧拉角和旋转向量不冗余但是具有奇异性。<br>接下来出场的四元数就厉害了，只有四个自由度而且也没有奇异性。它是一种类似于复数的表达方式。<br>四元数内容较多，另外再单独记录。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>《视觉SLAM十四讲》第三讲</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> SLAM基础 </tag>
            
            <tag> 刚体运动 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[针孔相机模型]]></title>
      <url>https://zhehangt.github.io/2017/02/16/SLAM/CameraModel/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<blockquote>
<p>图片全部来源互联网，如有侵权，跟我说一声。</p>
</blockquote>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>这篇博客是关于学习针孔相机模型过程中的一些简单记录。</p>
<h1 id="坐标系"><a href="#坐标系" class="headerlink" title="坐标系"></a>坐标系</h1><p>相机模型中常常涉及到四个坐标系：图像像素坐标系，成像平面坐标系，相机坐标系和世界坐标系。人类真是复杂…</p>
<h2 id="图像像素坐标系"><a href="#图像像素坐标系" class="headerlink" title="图像像素坐标系"></a>图像像素坐标系</h2><p>图像像素坐标系通常简称为图像坐标系或者像素坐标系。如下图所示。<br><img title="图像像素坐标系" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/CameraModel-01.png"></p>
<p>像素坐标系的平面为相机的成像平面，原点在图像的左上方，u轴向右，v轴向下，像素坐标系的单位是像素(pixel)，也就是我们常说的分辨率。假设某个像素点p坐标为$(u,v)$</p>
<h2 id="成像平面坐标系"><a href="#成像平面坐标系" class="headerlink" title="成像平面坐标系"></a>成像平面坐标系</h2><p>成像平面坐标系和像素坐标系在同一个平面上，原点是相机光轴与成像平面的交点，通常情况下是成像平面的中点或者叫principal point。单位为物理单位，比如毫米。因此成像平面坐标系和像素坐标系只是原点和度量单位不同，两个坐标系之间相差了一个缩放比例和一个原点的平移。</p>
<p>假设p对应的成像平面坐标为$(x,y)$，$dx$和$dy$表示图像中每个像素在成像平面中的物理尺寸，即上面提到的缩放比例。成像平面的原点在像素坐标系中的坐标为$(u_0, v_0)$，则像素坐标系与成像平面坐标系之间有如下转换公式：<br>$\left\{\begin{matrix}<br>u= \dfrac{x}{dx}+u_{0}<br>\\<br>v= \dfrac{y}{dy}+v_{0}<br>\end{matrix}\right.\Rightarrow \begin{bmatrix}<br>u<br>\\<br>v<br>\\<br>1<br>\end{bmatrix} =\begin{bmatrix}<br>\frac{1}{dx} &amp; 0 &amp; u_0 \\<br>0 &amp; \frac{1}{dy} &amp; v_0 \\<br>0 &amp; 0 &amp; 1<br>\end{bmatrix} \begin{bmatrix}<br>x<br>\\<br>y<br>\\<br>1<br>\end{bmatrix}<br>$</p>
<h2 id="相机坐标系"><a href="#相机坐标系" class="headerlink" title="相机坐标系"></a>相机坐标系</h2><p>相机坐标系如下图所示。<br><img title="图像像素坐标系" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/CameraModel-02.png"><br>相机坐标系的原点是光心，$x_c$和$y_c$轴与像素坐标系$u$轴和$v$轴平行，$z_c$轴为相机的光轴。光心到像素平面的距离为焦距f。</p>
<p>由图可以看出相机坐标系上的点和成像平面坐标系上的点存在透视投影关系。假设p对应的相机坐标系下的点P的坐标为$(X_c,Y_c,Z_c)$,则成像平面坐标系与相机坐标系之间有如下转换关系：<br>$\left\{\begin{matrix}<br>x= f\dfrac{X_c}{Z_c}<br>\\<br>y= f\dfrac{Y_c}{Z_c}<br>\end{matrix}\right.\Rightarrow Z_c\begin{bmatrix}<br>x<br>\\<br>y<br>\\<br>1<br>\end{bmatrix} =\begin{bmatrix}<br>f &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; f &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0<br>\end{bmatrix} \begin{bmatrix}<br>X_c<br>\\<br>Y_c<br>\\<br>Z_c<br>\\<br>1<br>\end{bmatrix}<br>$</p>
<h2 id="世界坐标系"><a href="#世界坐标系" class="headerlink" title="世界坐标系"></a>世界坐标系</h2><p>在环境中选择一个参考坐标系来描述相机和物体的位置，该坐标系称为世界坐标系。相机坐标系和世界坐标系之间的关系可以用旋转矩阵$R$和平移向量$t$来描述。假设$P$在世界坐标系下的坐标为$(X_w, Y_w, Z_w)$,则相机坐标系与世界坐标系之间有如下转换关系：<br>$<br>\begin{bmatrix}<br>X_c<br>\\<br>Y_c<br>\\<br>Z_c<br>\\<br>1<br>\end{bmatrix} =\begin{bmatrix}<br>R &amp; t \\<br>0 &amp; 1<br>\end{bmatrix} \begin{bmatrix}<br>X_w<br>\\<br>Y_w<br>\\<br>Z_w<br>\\<br>1<br>\end{bmatrix}<br>$</p>
<h1 id="坐标系转换"><a href="#坐标系转换" class="headerlink" title="坐标系转换"></a>坐标系转换</h1><p>通过上述的四个坐标系可以实现从世界坐标系与像素坐标系之间的转换，如下所示：<br>$Z_c\begin{bmatrix}<br>u<br>\\<br>v<br>\\<br>1<br>\end{bmatrix} = \begin{bmatrix}<br>\frac{1}{dx} &amp; 0 &amp; u_0 \\<br>0 &amp; \frac{1}{dy} &amp; v_0 \\<br>0 &amp; 0 &amp; 1<br>\end{bmatrix} \begin{bmatrix}<br>f &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; f &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0<br>\end{bmatrix} \begin{bmatrix}<br>R &amp; t \\<br>0 &amp; 1<br>\end{bmatrix} \begin{bmatrix}<br>X_w<br>\\<br>Y_w<br>\\<br>Z_w<br>\\<br>1<br>\end{bmatrix} = \begin{bmatrix}<br>f_x &amp; 0 &amp; u_0 &amp; 0 \\<br>0 &amp; f_y &amp; v_0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0<br>\end{bmatrix} \begin{bmatrix}<br>R &amp; t \\<br>0 &amp; 1<br>\end{bmatrix} \begin{bmatrix}<br>X_w<br>\\<br>Y_w<br>\\<br>Z_w<br>\\<br>1<br>\end{bmatrix}<br>$<br>其中$\begin{bmatrix}<br>f_x &amp; 0 &amp; u_0 &amp; 0 \\<br>0 &amp; f_y &amp; v_0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0<br>\end{bmatrix}$称为内参数矩阵$K$，$\begin{bmatrix}<br>R &amp; t \\<br>0 &amp; 1<br>\end{bmatrix}$称为外参数矩阵$T$。<br>对于相机的内参数矩阵往往是已知的并且是固定的，而外参数矩阵在SLAM问题中往往是需要求解的，用于相机的位姿定位。<br>从世界坐标系到像素坐标系之间的转换关系可知，已知世界坐标系下的三维点坐标，只要已知内外参矩阵，就可以求得像素坐标。而如果已知像素坐标，即使已知内外参矩阵，其世界坐标下的三维点也不是唯一确定的，而是空间的一条直线。即单目相机只能测平面信息，而不能获取深度信息。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>《视觉SLAM十四讲》第五讲</li>
<li><a href="http://blog.csdn.net/chentravelling/article/details/53558096" target="_blank" rel="external">相机成像原理：世界坐标系、相机坐标系、图像坐标系、像素坐标系之间的转换</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> SLAM </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 相机模型 </tag>
            
            <tag> SLAM基础 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 07]]></title>
      <url>https://zhehangt.github.io/2016/10/17/ProbailisticRobotics/ProbabilisticRobotics-07/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>之前几个章节学习了贝叶斯滤波以及如何实现贝叶斯滤波，学习了如何对动作和测量建模。这一章节将利用这些知识来解决在地图已知的环境中的机器人定位问题。</p>
<h1 id="定位问题的分类"><a href="#定位问题的分类" class="headerlink" title="定位问题的分类"></a>定位问题的分类</h1><p>局部定位 vs 全局定位<br>1.位置追踪。机器人的初始位置是已知的。<br>2.全局定位。机器人的初始位置是未知的。<br>3.机器人绑架问题。机器人以为自己的初始位置是已知的，但实际上这个位置是错误的。</p>
<p>静态环境 vs 动态环境<br>1.静态环境。整个环境中只有机器人是运动的。<br>2.动态环境。整个环境中除了机器人，还有其他物体是运动的。</p>
<p>被动方法 vs 主动方法<br>1.被动定位。机器人并不是为了定位而移动的，机器人本身有其他的任务。<br>2.主动定位。机器人为了定位而移动的。</p>
<p>单机器人 vs 多机器人<br>1.单机器人定位。<br>2.多机器人定位。</p>
<h1 id="马尔可夫定位"><a href="#马尔可夫定位" class="headerlink" title="马尔可夫定位"></a>马尔可夫定位</h1><p>马尔可夫定位就是将地图信息融入到贝叶斯滤波中产生的变体。算法如下图所示。</p>
<img title="马尔可夫定位" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-07-01.png">
<p>值得注意的是对于不同的定位问题，$bel(x_0)$的初始化是有区别的。</p>
<p>对于位置追踪问题会使用指标函数。假设初始位置为$x_0$。<br>$bel(x_0)=\begin{cases}<br>1 &amp; if \ x_0=\bar{x_0} \\<br>0 &amp; otherwise<br>\end{cases}$<br>但指标函数无法表示一个概率分布，因此也常常使用高斯分布来表示。<br>$bel(x_0) \sim \mathcal N(x_0; \bar{x_0}, \Sigma)$</p>
<p>对于全局定位问题常常使用均匀分布。<br>$bel(x_0)=\dfrac{1}{\vert X \vert}$<br>其中$\vert X \vert$为空间面积。</p>
<p>对于其他情况，往往与某些特定知识相关。比如假设机器人在门附近，则机器人在所有门附近的状态置信度会明显比其他位置更高。</p>
<h1 id="扩展卡尔曼滤波定位"><a href="#扩展卡尔曼滤波定位" class="headerlink" title="扩展卡尔曼滤波定位"></a>扩展卡尔曼滤波定位</h1><p>这里阐述的扩展卡尔曼滤波定位是基于第五章的速度动作模型和第六章的基于特征的感知模型实现的，并且测量数据与对应的路标是已知的,即关联关系是已知的。算法流程如下图：<br><img title="扩展卡尔曼滤波定位" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-07-02.png"><br>此处的数学公式推导见7.5.3小节。</p>
<h1 id="评估关联关系"><a href="#评估关联关系" class="headerlink" title="评估关联关系"></a>评估关联关系</h1><p>上述的扩展卡尔曼滤波定位算法是在测量数据和对应的路标关联关系已知的前提下的。但在实际应用中，这种关联关系往往是不可知的，因此需要对关联关系进行评估。改进后的算法流程如下图：</p>
<img title="基于关联关系评估的扩展卡尔曼滤波定位" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-07-03.png">
<p>其中第6行到第12行以及第14行进行了关联关系的评估，得到了关联度最高的路标的索引。</p>
<p>此处的数学公式推导见7.6.2小节。</p>
<h1 id="多假设追踪"><a href="#多假设追踪" class="headerlink" title="多假设追踪"></a>多假设追踪</h1><p>当无法通过当前数据得出一个可靠的状态可信度时，可以通过多假设追踪算法(MHT)让假设继续传递，延迟可信度的判定，从而通过后续的测量数据解决这种不确定性，得到更可靠的结果。计算公式如下：</p>
<p>$bel(x_t) = \dfrac{1}{\sum_l \varphi_{t,l}}\sum_l \varphi_{t,l} \ \mathcal N(x_t; \mu_{t,l}, \Sigma_{t,l})$<br>其中$l$代表所有混合成分(mixture component)的索引。$\varphi$代表对应成分的权重。</p>
<p>关于MHT算法的细节以后再详细分析。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在实际应用中，以上几个算法依然会存在很多的问题。比如遍历路标时的搜索效率问题，特征之间的关联性问题以及检测到异常物体的处理问题。</p>
<p>最近几章的笔记写的很简单，主要是因为很多内容只停留在公式层面，无法与实际场景相联系。因此对书中的内容理解比较浅显。随着学习的深入，也许会对当前的一些笔记进行修正和深入。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 06]]></title>
      <url>https://zhehangt.github.io/2016/10/16/ProbailisticRobotics/ProbabilisticRobotics-06/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>之前谈到在利用贝叶斯滤波计算状态置信度时会用到两个重要的概率：状态转移概率和测量概率。这一章节就是围绕如何计算测量概率展开的。并且主要基于距离传感器得到的测量数据，并根据机器人的位姿和地图信息进行测量概率的计算。</p>
<h1 id="地图"><a href="#地图" class="headerlink" title="地图"></a>地图</h1><p>地图信息是由环境中的物体和其位置组成的列表来表示的。用$m=\{m_1, m_2,… ,m_N \}$表示。其中$N$表示环境中物体的总数。地图信息通常有两种索引方式：基于特征的表示方法和基于位置的表示方法。在基于特征的地图中，$n$是特征索引，每个$m$中包含特征和位置。特征地图提供了空间中所包含的所有物体的信息。在基于位置的地图中，$n$对应一个特定的位置。在平面地图中通常用$m_{x,y}$表示。位置地图提供了整个空间中每个位置的信息。</p>
<p>占用栅格地图(occupancy grid map)是一种典型的位置地图。它为地图中的每个坐标位置赋予数值代表该位置是否已经被占用。</p>
<h1 id="距离传感器的光线模型-Beam-Models"><a href="#距离传感器的光线模型-Beam-Models" class="headerlink" title="距离传感器的光线模型(Beam Models)"></a>距离传感器的光线模型(Beam Models)</h1><p>通过光线和声波可以测量机器人与周围物体之间的距离信息。常见的传感器有激光传感器和超声波传感器。这种测量模型会引入四种误差。<br>1.测量误差。受传感器本身精确度的影响，距离信息往往是有噪声的，这部分噪声对测量数据的影响用高斯模型$p_{hit}$来表示。又因为距离传感器往往有最大测量范围，因此要将高斯模型的变量取值限定在这个范围之内，对范围之内的高斯概率做标准化处理。<br>$p_{hit}(z_t^k \mid x_t, m)=\begin{cases}<br>\eta \ \mathcal N(z_t^k;\ z_t^{k\ast},\ \sigma_{hit}^2) &amp; if \ 0\leq z_t^k \leq z_max \\<br>0 &amp; otherwise<br>\end{cases}$<br>2.不可预测的物体。由于地图是固定的，而环境是动态变化的。当环境中出现新物体未被地图所包含，比如走动的人，则会造成误差。由于这部分误差会导致距离信息变短，因此用指数模型$p_{short}$。还需要将指数模型的变量限定在比实际距离小的范围内，并对范围内的指数概率做标准化处理。<br>$p_{short}(z_t^k \mid x_t, m)=\begin{cases}<br>\eta \ \lambda_{short}e^{-\lambda_{short} \ \ z_t^k} &amp; if \ 0\leq z_t^k \leq z_t^{k\ast} \\<br>0 &amp; otherwise<br>\end{cases}$<br>3.测量错误。由于障碍物表面材质问题或者测量角度问题，可能会导致测量失败，测量返回值往往是测量的最大值。这部分噪声用指标函数$p_{max}$表示。<br>$p_{max}(z_t^k \mid x_t, m)= I(z=z_{max})=\begin{cases}<br>1 &amp; if \ z=z_{max} \\<br>0 &amp; otherwise<br>\end{cases}$<br>4.随机误差。最后为无法解释的随机误差。用$p_{rand}$表示。<br>$p_{rand}(z_t^k \mid x_t, m)=\begin{cases}<br>\frac{1}{z_{max}} &amp; if \ 0\leq z_t^k &lt; z_{max} \\<br>0 &amp; otherwise<br>\end{cases}$</p>
<p>基于以上四种误差计算测量概率算法如下：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-06-01.png"></p>
<p>其中输入为测量数据$z_t$, 机器人的位姿$x_t$ 和地图信息$m$，$z_{hit},z_{short},z_{max},z_{rand}$分别为各个误差的权重。输出为当前观测$z_t$的概率。</p>
<p>在上述的模型中，存在很多未知的参数。<br>第一种参数设置的方式是根据经验手工设置参数。<br>第二种参数设置方式是根据数据得到参数。用$Z$表示一系列的测量数据，$X$表示一系列对应的位置数据，$\Theta$表示参数，因此只需要找到使$p(Z \mid X, m, \Theta)$概率最大的参数即可。具体算法如下图：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-06-02.png"></p>
<p>关于算法的推导过程见书本6.3.3小节。</p>
<h1 id="距离传感器的概率域模型-Likelihood-Fields"><a href="#距离传感器的概率域模型-Likelihood-Fields" class="headerlink" title="距离传感器的概率域模型(Likelihood Fields)"></a>距离传感器的概率域模型(Likelihood Fields)</h1><p>上述的光线模型存在的两个缺点：<br>1.通过光线模型计算出的测量概率在复杂场景下是不光滑的。即使机器人很小的位姿变化也会带来很大的测量概率的变化。这种不光滑会导致错失最优解或者陷入局部最优解。<br>2.计算开销大。</p>
<p>概率域模型的基本思想是计算出障碍物所处位置的全局坐标。要计算出这个全局坐标依赖于机器人的位姿，距离传感器在机器人上的具体位置以及距离传感器检测出与障碍物的距离。<br>与光线模型类似，概率域模型引入三种误差。<br>1.测量噪声。<br>$p_{hit}(z_t^k \mid x_t, m)=\varepsilon_{\sigma_{hit}^2}(dist^2)$</p>
<p>2.测量错误。<br>3.随机误差。</p>
<p>概率域模型计算测量概率算法如下<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-06-03.png"></p>
<p>其中$(x_{z_t^k} \ y_{z_t^k})$为检测到的障碍物经过转换后在全局坐标系中的坐标。$(x’ \ y’)$是实际地图中最接近检测到的障碍物的实际障碍物的坐标。这个搜索过程也是该算法中开销最大的步骤。$z_{random}$是随机误差的权重。</p>
<p>概率域模型的优点在于得到的概率分布是光滑的，并且预计算在二维空间中进行。但仍有几个不足。<br>1.没有考虑空间中的动态物体。<br>2.未考虑墙这种距离传感器无法穿过的物体。<br>3.无法在没有地图信息的环境中进行使用。<br>对于这些不足可以对概率域模型进行扩展。详见6.4.2小节。</p>
<h1 id="基于关联的感知模型"><a href="#基于关联的感知模型" class="headerlink" title="基于关联的感知模型"></a>基于关联的感知模型</h1><p>基于关联的感知模型是通过地图匹配来实现的。首先将传感器数据转换成局部地图，用$m_{local}$表示。再与全局地图$m$利用如下公式进行计算，得到一个关联度。</p>
<p>$\rho_{m,\ m_{local} \ \ ,  \ x_t}=\dfrac{\sum_{x,y}(m_{x,y}-\bar m)\cdot(m_{x,y,local}(x_t)-\bar m)}{\sqrt{\sum_{x,y}(m_{x,y}-\bar m)^2(m_{x,y,local}(x_t)-\bar m)^2}}$</p>
<p>其中$\bar m$是地图的平均值<br>$\bar m = \dfrac{1}{2N} \sum_{x,y}({m_{x,y}+m_{x,y,local}})$</p>
<p>如果局部地图是从测量数据中得到的，那么此时关联度即可作为测量概率。</p>
<p>概率域模型仅仅涵盖了末端的输出数据，并与空间中被占用的那部分空间关联。而地图匹配则涵盖了整个空间的信息，包括未被占用的那部分空间的空间信息。</p>
<h1 id="基于特征的感知模型"><a href="#基于特征的感知模型" class="headerlink" title="基于特征的感知模型"></a>基于特征的感知模型</h1><p>上述模型都是基于原始的传感器测量数据的，基于特征的感知模型则先从这些原始数据中进行特征提取，这么做的好处是可以大大降低计算复杂性。对于不同的感知数据通常有不同的特征提取方式。</p>
<p>路标(Landmark)是机器人应用中常用的概念。通常通过提取感知数据的特征来对应环境中某些独特的物体，这些物体就被称为路标。通常用如下公式从测量数据中提取特征：<br>$f(z_t)=\{f_t^1,f_t^2,…\}=\{(r_t^1, \phi_t^1, s_t^1), (r_t^2, \phi_t^2, s_t^2),… \}$<br>其中$r$代表距离，$\phi$代表方向，$s$表示签名(signature)。<br>通常认为特征之间是独立的。因此有：<br>$p(f(z_t) \mid x_t,m)=\prod_i p(r_t^i, \phi_t^i, s_t^i \mid x_t, m)$</p>
<p>若知道测量数据与哪个路标相对应，则可用以下算法计算测量概率：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-06-04.png"><br>其中$c_t^i$表示测量数据对应路标的索引。</p>
<p>还有一种基于采样的方法，可以得到机器人的位姿：<br><img alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-06-05.png"></p>
<p>以上基于特征的算法都假设特征与路标的关联关系是已知的，对于未知情况，即未知空间的探索，将在定位与地图构建章节展开详细论述。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本章关于测量概率的计算，提出了很多种模型。<br>距离传感器的光线模型是直接基于原始感知数据进行概率计算的。<br>距离传感器的概率域模型将原始感知数据转换为每个障碍物的全局坐标进行概率计算。<br>基于关联的感知模型将原始数据转化为局部地图进行概率计算。<br>基于特征的感知模型对原始数据进行特征提取(如路标)进行概率计算。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 05]]></title>
      <url>https://zhehangt.github.io/2016/10/13/ProbailisticRobotics/ProbabilisticRobotics-05/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>对机器人的控制动作(motion)进行建模是计算<strong>状态转移概率</strong>$p(x_t \mid u_t, x_{t-1})$的重要组成部分。以下的所有动作模型都是基于一个机器人在一个平面上移动的场景下展开的。</p>
<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><p>由于限定了机器人在一个平面上移动，因此可以用坐标和方向组成的向量来表示机器人的状态。即$(x, y, \theta)$。通常也称为机器人的位姿。<strong>状态转移概率</strong>$p(x_t \mid u_t, x_{t-1})$中，$x_t$表示t时刻机器人的位姿，$x_{t-1}$表示t-1时刻机器人的位姿。而$u_t$即为t时刻机器人的控制动作，在此场景下即为机器人从一个位姿转换到另一个位姿。因此对机器人的控制动作建模是计算状态转移概率的关键。<br>一般对机器人控制动作建模有两种方法：<br>1.工业机器人通常可以通过发送一个速度指令来对机器人进行控制，第一种控制动作建模方式就是利用这个速度指令来进行控制动作建模<br>2.通常机器人会提供里程计信息，包括运动的距离和转动的角度。第二种控制动作建模方式就是利用里程计信息来进行控制动作建模</p>
<p>通常来说里程计模型要比速度模型更准确，因为里程计信息是机器人运动之后通过编码器得到的，而速度是在机器人运动之前下达的。因此速度模型通常用来进行动作规划(motion planning)。而里程计模型通常用来做状态评估。</p>
<h1 id="速度动作模型"><a href="#速度动作模型" class="headerlink" title="速度动作模型"></a>速度动作模型</h1><p>在速度动作模型中，用向量$(v_t, \omega_t)$来表示动作$u_t$,其中$v_t$表示t时刻机器人的速度，$\omega_t$表示t时刻机器人的转速。</p>
<p>速度动作模型的闭合解算法如下图：<br><img title="速度动作模型的闭合解" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-01.png"><br>它的输入是t-1时刻的位姿$x_{t-1}$，t时刻的动作$u_t$，以及t时刻的假设位姿$x_t$。输出假设位姿$x_t$的概率。$\alpha_1…\alpha_6$是与机器人相关的动作误差系数。</p>
<p>$\mathbf{prob(x,b)}$函数计算x在0均值b方差下的概率值。计算方式通常有以下两种方法：<br><img src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-02.png"></p>
<p>根据粒子滤波，也可以通过从状态转移概率中采样得到当前时刻的位姿$x_t$，采样是基于$x_{t-1}$和$u_t$进行的。基于采样的速度动作模型算法如下图：<br><img title="基于采样的速度动作模型算法" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-03.png"><br>算法的输入是$x_{t-1}$和$u_t$,输出$x_t$。$\mathbf {sample}$函数是对控制数据增加噪声扰动。</p>
<p>$\mathbf {sample}$函数通常有以下两种计算方法：<br><img src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-04.png"></p>
<p>以上两种速度动作模型均有严格的数学推导，详见书籍5.3.3小节。</p>
<h1 id="里程计动作模型"><a href="#里程计动作模型" class="headerlink" title="里程计动作模型"></a>里程计动作模型</h1><p>在里程计模型中，用向量$(\bar x_{t-1}, \bar x_t)$来表示$u_t$,上划线表示该位姿是通过里程计得到的，是基于机器人内部坐标的，并非全局坐标系中的位姿。通过这两个位姿信息可以得到初始旋转角度，平移和最终旋转角度。如图所示：<br><img src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-05.png"></p>
<p>里程计动作模型的闭合解算法如下图：<br><img title="里程计动作模型的闭合解算法" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-06.png"><br>它的输入是t-1时刻的位姿$x_{t-1}$，t时刻的动作$u_t$，以及t时刻的假设位姿$x_t$。输出假设位姿$x_t$的概率。$\alpha_1…\alpha_4$是与机器人相关的动作误差系数。</p>
<p>基于采样的里程计动作模型算法如下图：<br><img title="基于采样的里程计模型" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-07.png"><br>与基于采样的速度动作模型算法基本类似，区别仅在于动作$u_t$的表示方式不同。</p>
<p>以上两种里程计动作模型均有严格的数学推导，详见书籍5.4.3小节。</p>
<h1 id="动作与地图"><a href="#动作与地图" class="headerlink" title="动作与地图"></a>动作与地图</h1><p>以上的动作模型均未考虑机器人所在空间的地图信息。机器人在移动时是与所处空间相关的，比如某些位置已经被占用了，机器人就无法移动到这些位置上。因此需要将地图信息融入到状态转移概率中去。此时状态转移概率为$p(x_t \mid u_t, x_{t-1}, m)$。</p>
<p>当$x_{t-1}$与$x_t$距离足够小时(通常认为小于机器人的半径),可以用以下公式近似：<br>$p(x_t \mid u_t, x_{t-1}, m) = \eta \ p(x_t \mid u_t, x_{t-1}) \ p(x_t \mid m)$</p>
<p>融入地图信息的动作模型如图所示：<br><img title="融入地图信息的动作模型" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-05-08.png"><br>在占用地图(occupancy maps)中，如果位置已经被占用，$p(x_t \mid m)=0$</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本章给出了两种基本的动作模型：速度动作模型和里程计动作模型。每种模型均有闭合解版本和采样版本。这些动作模型都是基于机器人在一个平面移动的场景下的。而在实际应用中，机器人的状态可能会更加复杂，动作也更加复杂，因此不同的场景下往往会有不同的动作模型。但是原理是通用的，本章的动作模型能很好地起到抛砖引玉的作用。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 04]]></title>
      <url>https://zhehangt.github.io/2016/10/11/ProbailisticRobotics/ProbabilisticRobotics-04/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>高斯滤波依赖于对后验概率进行特定函数形式(高斯函数)的假定，因此当某些概率分布并不符合高斯分布时，高斯滤波就显示出无法避免的短板。而非参数滤波不需要这种假定。因此非参数滤波可以表示更复杂的状态可信度模型，但同样会增加计算的复杂性。本章主要介绍了两种非参数方法，包括直方图滤波和粒子滤波。</p>
<h1 id="直方图滤波"><a href="#直方图滤波" class="headerlink" title="直方图滤波"></a>直方图滤波</h1><p>直方图滤波应用于离散状态时，被称为离散贝叶斯滤波。算法流程如下：<br><img title="离散贝叶斯滤波" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-04-01.png"><br>其实离散贝叶斯滤波就是将第二章的贝叶斯滤波算法中的积分形式代替为求和形式。<br>其中$x_i$代表某种状态，某一时刻，某种状态的可信度用$p_{k,t}$表示。</p>
<p>将离散的状态进行合并就得到了连续的状态空间，其误差由离散状态的粒度决定的<br>$range(X_t)=x_{1,t} \cup x_{2,t} \cup … \ x_{k,t} $</p>
<p>对于每个区域的$\mathbf X_{k,t}$,通常用平均值进行估计。<br>$\hat x_{k,t}=\ \mid \mathbf X_{k,t} \mid ^{-1} \int_{\mathbf x_{k,t}} x_t  \ dx_t$</p>
<p>因此可以得到区域的状态转移概率和测量概率<br>$p(\mathbf x_{k,t} \mid u_t, \mathbf x_{i,t-1}) \approx \dfrac{\eta}{\mid\mathbf x_{k,t}\mid} p(\hat x_{k,t} \mid u_t, \hat x_{i,t-1})$<br>$p(z_t \mid \mathbf x_{k,t}) \approx p(z_t \mid \hat x_{k,t})$</p>
<p>因此对连续的状态空间进行分解的分解技术是直方图滤波实现的关键技术之一，主要分为静态分解和动态分解。静态分解提前设定了固定的分解方式。Grid representation是静态分解的典型例子。动态分解则根据后验分布的形状动态选择分解方式。Density trees是动态分解的典型例子。</p>
<h1 id="粒子滤波"><a href="#粒子滤波" class="headerlink" title="粒子滤波"></a>粒子滤波</h1><p>粒子滤波的主要思想是通过状态可信度的随机样本来表示状态可信度的后验概率。这是一种蒙特卡罗的思路。</p>
<p>粒子滤波的算法流程如下：<br><img title="粒子滤波" alt="..." src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-04-02.png"><br>其中 $\mathcal X$代表粒子群，用粒子群来表示状态可信度的概率分布<br>1.第四行通过$t-1$时刻的粒子群表示的概率分布以及当前的控制信息$u_t$来产生$t$时刻的粒子群，该粒子群就是对当前状态可信度概率的评估。<br>2.第五行根据测量数据计算每个粒子的重要因子(importance factor)<br>3.第八行到第十一行被称为resampling 或者 importance resampling。根据第二步中的重要因子对粒子群进行重采样。重要因子中融合了测量数据，因此重采样过程就是对可信度概率的评估根据测量数据进行修正。这同样是通过粒子群来表达的。</p>
<p>粒子滤波存在四个方面的近似误差，这四个方面也是改进粒子滤波的重要方面。<br>1.用来表示概率分布的粒子数M是有限的。在M=1的极限情况下，重采样阶段会舍弃所有的测量信息。幸运的是M越大，这种影响越轻微。<br>2.在重采样阶段的随机性问题。不断的重采样会导致最终结果趋于一个固定值而丢失了状态的多样性。<br>减少这个误差有两个思路，第一是减少重采样的频率，第二是低方差采样(low variance sampling)。<br>3.粒子滤波得到的评估概率分布(proposal distribution)和经过修正的目标概率分布(target distribution)的分歧。<br>4.在高维状态空间中，对某些状态周围可能未采样到粒子，因此无法对这些状态进行概率评估。简单的做法也是增大M的数值。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这一章节的内容比较抽象，包括直方图滤波中的区域概率的估计以及连续的状态空间的分解。包括粒子滤波中的用粒子群来表示概率分布，通过粒子群重采样来修正概率分布。对这方面的内容有进一步的学习和理解之后，再深化这篇文章的内容。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 03]]></title>
      <url>https://zhehangt.github.io/2016/10/09/ProbailisticRobotics/ProbabilisticRobotics-03/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>用高斯滤波对状态进行递归评估是贝叶斯滤波最常用的实现方式。</p>
<p>多变量高斯分布的密度函数：<br>$p(x) = \mathbf {det}(2 \pi \Sigma)^{-\frac{1}{2}} \mathbf{exp} \{ -\dfrac{1}{2}(x- \mu)^T \Sigma^{-1}(x-\mu)  \}$</p>
<blockquote>
<p>$\Sigma 表示协方差， \mu表示均值$</p>
</blockquote>
<p>接下来将从最基本的高斯滤波算法：卡尔曼滤波，扩展的卡尔曼滤波，信息滤波三个方面进行介绍。</p>
<h1 id="卡尔曼滤波"><a href="#卡尔曼滤波" class="headerlink" title="卡尔曼滤波"></a>卡尔曼滤波</h1><p>卡尔曼滤波是学习贝叶斯滤波算法实现的最佳学习资料。卡尔曼滤波用于对线性系统的过滤和预测。只适用于连续状态的计算。利用卡尔曼滤波实现贝叶斯滤波必须满足以下三个假设：<br>1.状态转移概率函数 $p(x_t \mid u_t, x_{t-1})$ 必须是线性函数,即：<br>$x_t = A_t x_{t-1} + B_tu_t+ \epsilon _t$ （线性高斯模型）<br>2.测量概率函数 $p(z_t \mid x_t)$也是线性函数，即：<br>$z_t = C_tx_t + \delta _t$<br>3.初始可信度函数$bel(x_0)$满足高斯分布<br>$bel(x_0)=p(x_0)$</p>
<p>基于以上三个假设即可证明任意时刻的可信度函数$bel(x_t)$均满足高斯分布</p>
<p>卡尔曼滤波算法的流程如图所示：<br><img title="卡尔曼滤波" alt="图片说明" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-03-01.png"><br>卡尔曼滤波的输出是t时刻的<strong>状态可信度</strong>$bel(x_t)$，通过均值$u_t$和协方差$\Sigma_t$表示，输入是t-1时刻的<strong>状态可信度</strong>，通过均值$u_{t-1}$和协方差$\Sigma_{t-1}$表示，为了更新参数还需要t时刻的控制数据$\mu_t$和测量数据$z_t$<br>第二行和第三行根据<strong>控制数据</strong>计算当前时刻状态可信度的预测值。<br>第四行到第六行根据<strong>测量数据</strong>对预测值进行修正得到状态可信度。其中$K_t$称为卡尔曼增益。卡尔曼增益代表了观测数据对预测值的修正程度。$Q_t$代表噪声$\delta _t$的方差。</p>
<h1 id="扩展卡尔曼滤波"><a href="#扩展卡尔曼滤波" class="headerlink" title="扩展卡尔曼滤波"></a>扩展卡尔曼滤波</h1><p>上述提到的卡尔曼滤波假设状态转移概率函数和测量概率函数都是线性的，而实际中这种假设往往是不成立的。因此提出扩展卡尔曼滤波来表示非线性情况。状态转移概率函数和测量概率函数表示如下：<br>$x_t=g(u_t,x_{t-1})+\epsilon_t$<br>$z_t=h(x_t)+\delta_t$</p>
<p>扩展卡尔曼滤波的关键就是对g和h函数进行线性化。常用的线性化方法有<strong>泰勒展开</strong>。<br>$g(u_t,x_{t-1})\approx g(u_t,\mu_{t-1})+g’(u_t,\mu_{t-1})(x_{t-1}-\mu_{t-1})\\ \qquad \quad \ \   = g(u_t,\mu_{t-1}) + G_t(x_{t-1}-\mu_{t-1}) $</p>
<blockquote>
<p>其中$G_t$为雅可比行列式</p>
</blockquote>
<p>$h(x_t) \approx h(\bar \mu_t)+h’(u_t)(x_t-\bar \mu_t)<br>\\ \quad \ \ \   = h(\bar \mu_t) + H_t(x_t-\bar \mu_t) $</p>
<p>扩展卡尔曼滤波算法的流程如图所示：<br><img title="扩展卡尔曼滤波" alt="图片说明" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-03-02.png"></p>
<p>扩展卡尔曼滤波的局限性在于通过泰勒展开来近似状态转移和观测，因此扩展卡尔曼滤波的准确程度依赖于状态转移和观测是否是线性近似的。<br>其他的线性近似手段包括<strong>无损卡尔曼滤波</strong>和<strong>矩匹配</strong>(moments matching)</p>
<h1 id="信息滤波"><a href="#信息滤波" class="headerlink" title="信息滤波"></a>信息滤波</h1><p>信息滤波是高斯分布的对偶形式，与卡尔曼滤波和扩展卡尔曼滤波的区别在于：卡尔曼滤波和扩展卡尔曼滤波是通过均值和方差来代表高斯分布，而信息滤波是通过信息矩阵和信息向量来表示的。表示如下：<br>$\Omega = \Sigma^{-1}$<br>$\xi = \Sigma^{-1}\mu$</p>
<p>改写多变量高斯分布可得：<br>$p(x)=\mathbf{det}(2\pi\Sigma)^{-\frac{1}{2}} \mathbf{exp}\{ -\frac{1}{2}\mu^T\Sigma^{-1}\mu \} \mathbf{exp}\{ -\frac{1}{2}x^T\Sigma^{-1}x+x^T\Sigma^{-1}\mu  \} $</p>
<p>其中$\mathbf{det}(2\pi\Sigma)^{-\frac{1}{2}} \mathbf{exp}\{ -\frac{1}{2}\mu^T\Sigma^{-1}\mu \}$为与x无关的常数，因此用$\eta$表示，代入信息矩阵和信息向量可得：<br>$p(x)=\eta \ \mathbf{exp}\{ -\frac{1}{2}x^T\Omega x + x^T \xi\}$</p>
<p>信息滤波算法的流程如图所示：<br><img title="信息滤波" alt="图片说明" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-03-03.png"></p>
<p>扩展信息滤波算法的流程如图所示：<br><img title="扩展信息滤波" alt="图片说明" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-03-04.png"></p>
<p>信息过滤有以下几个优缺点：<br>1.可以用$\Omega=0$表示全局的不确定性<br>2.适用于多机器人的问题。因为将其转化为负对数函数时，贝叶斯规则转化为加法形式<br>3.矩阵求逆，计算开销大</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本章介绍的卡尔曼滤波方法是实现贝叶斯算法最主要的算法之一，也是最成熟的算法之一。基于卡尔曼滤波针对特定的问题还有很多不同变种，希望在今后的学习中能加深理解。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 02]]></title>
      <url>https://zhehangt.github.io/2016/09/28/ProbailisticRobotics/ProbabilisticRobotics-02/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>概率机器人学的核心理念是通过感知数据来进行状态评估。在周围的环境信息都是已知的情况下，机器人就很容易决定该执行什么行为。但是环境信息往往是不能直接观察到的，而是需要通过感知数据来进行推断的。概率状态评估算法就是通过计算环境信息的概率分布，提供给机器人使用。<br>本章将从概率学的基本概念，机器人与环境的相互作用模型，进行状态评估的贝叶斯滤波算法和实现贝叶斯算法的相关问题这四个方面展开。</p>
<h1 id="概率学基本概念"><a href="#概率学基本概念" class="headerlink" title="概率学基本概念"></a>概率学基本概念</h1><p>这一小节介绍了几个简单的概率论方面的知识，内容比较简单，只贴公式。<br>概率密度函数：$p(x)$</p>
<p>联合分布函数：$p(x,y)=p(X=x\ and\ Y=y)$</p>
<p>x和y相互独立，则：$p(x,y)=p(x)\ p(y)$</p>
<p>条件概率密度函数：$p(x\mid y)=\dfrac{p(x,y)}{p(y)}$</p>
<p>x和y相互独立：$p(x\mid y)=\dfrac{p(x)\ p(y)}{p(y)}=p(x)$</p>
<p>概率求和：$p(x)=\int p(x \mid y)\ p(y)\ dy$</p>
<p>贝叶斯公式：$p(x \mid y)=\dfrac{p(y \mid x)\ p(x)}{p(y)}$</p>
<p>y与x相互独立，所以常常将$p(y)^{-1}$写成标准化因子$\mu$： $p(x \mid y)=\mu\ p(y\mid x)\ p(x)$</p>
<blockquote>
<p>$\mu$标准化的概念在书中出现很多次，标准化的作用就是将一个数值映射到[0,1]区间上，使其成为一个概率值。标准化因子的取值就是对其要标准化的对象求和或积分之后的倒数。</p>
</blockquote>
<p>期望：$E[X] = \int x\ p(x)\ dx$</p>
<p>方差：$Cov[x] = E[X-E[x]]^2 = E[x^2]-E[x]^2$</p>
<p>多条件：$p(x \mid y,z)=\dfrac{p(y\mid x,z)\ p(x \mid z)}{p(y \mid z)}$</p>
<p>x，y基于条件z独立：$p(x,y \mid z) = p(x \mid z) \ p(y \mid z)  \nRightarrow p(x,y)=p(x)\ p(y)$</p>
<blockquote>
<p>与x和y相互独立不等价</p>
</blockquote>
<h1 id="机器人与环境的相互作用"><a href="#机器人与环境的相互作用" class="headerlink" title="机器人与环境的相互作用"></a>机器人与环境的相互作用</h1><p>这一小节介绍了机器人与环境的相互作用，并为这种相互作用建立数学模型。</p>
<p><strong>状态</strong>(state)代表机器人自身信息的状态变量和所处环境信息的状态变量的集合，并且这些信息是会对未来产生影响的。状态用$x$表示。<br>典型的状态变量有以下几个方面：<br>1.机器人的位姿，包括位置和朝向，通常称为运动学状态。<br>2.驱动器的配置信息。比如joints of robotic manipulators。<br>3.机器人的速度和关节的速度。<br>4.机器人的位置和周围的物体。<br>5.机器人周围正在运动的物体的位置和速度。<br>6.其他状态变量，比如电量。</p>
<p>机器人与环境的相互作用有两种方式：<br>1.机器人通过传感器收集环境状态的信息。称为measurement。<br>2.机器人通过控制动作来使环境状态发生变化。称为motion。</p>
<p>因此会产生两种数据：<br>1.<strong>测量数据</strong>。提供对当前时刻的环境状态的观测信息。用$z_t$表示。<br>2.<strong>控制数据</strong>。提供对当前环境状态进行改变的行为信息。用$u_t$表示。</p>
<p>基于之前的状态、测量数据和控制数据得出当前状态的概率分布，即<br>$ p(x_{t} \mid x_{0:t-1}, z_{1:t-1}, u_{1:t}) $</p>
<p>当状态信息是完全的，当前的状态只与上一时刻的状态信息和当前的控制信息有关。即状态转移概率(state transition probability)<br>$p(x_t \mid x_{0:t-1}, z_{1:t-1}, u_{1:t})=p(x_t \mid x_{t-1}, u_t)$</p>
<p>类似的，当状态信息是完全的，当前的测量数据只与当前的状态有关。即测量概率(measurement probability)<br>$p(z_t \mid x_{0:t}, z_{1:t-1}, u_{1:t}) = p(z_t \mid x_t)$</p>
<blockquote>
<p>这两个公式是机器人与环境交互模型的关键。在下一节中通过贝叶斯算法计算可信度就是通过迭代这两个公式</p>
</blockquote>
<p>概率机器人学中另外一个关键概念是<strong>可信度</strong>(belief)，可信度代表的是从测量数据和控制数据推断出的当前状态的可信程度。<br>可信度计算公式:<br>$bel(x_t) = p(x_t \mid z_{1:t},u_{1:t})$</p>
<p>但是往往在得到测量数据 $z_t$ 之前对 $x_t$ 的可信度进行计算，作为一种对 $x_t$ 的预测，即：<br>$\overline{bel(x_t)} = p(x_t \mid z_{1:t-1},u_{1:t})$</p>
<p>通过$\overline{bel(x_t)}$计算$bel(x_t)$称为修正或者测量更新。</p>
<p>概率机器人学解决问题的途径往往就是通过计算状态可信度，从中选择可信度最高的状态，作为当前机器人的状态。</p>
<h1 id="贝叶斯滤波"><a href="#贝叶斯滤波" class="headerlink" title="贝叶斯滤波"></a>贝叶斯滤波</h1><p><strong>贝叶斯滤波算法</strong>是最为常用的计算可信度的算法。算法流程如下：<br><img title="贝叶斯滤波算法" alt="图片说明" src="http://oeaxm0g1o.bkt.clouddn.com/images/ProbabilisticRobotics-02-01.png"></p>
<p>直观上讲就是根据状态转移概率和前一时刻的状态可信度得出当前状态可信度的猜测，然后再利用测量概率对猜测进行修正。</p>
<blockquote>
<p>书中给出了严格的数学推导来证明贝叶斯滤波的正确性。见2.4.3小节。</p>
</blockquote>
<h1 id="表示与计算"><a href="#表示与计算" class="headerlink" title="表示与计算"></a>表示与计算</h1><p>在用各种方法进行贝叶斯滤波实现的时候，要考虑三个方面。<br>1.计算的效率<br>2.近似的准确性<br>3.实现的难易程度</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>机器人领域的核心问题就是准确地得到机器人每一时刻的状态。在概率机器人学中，是根据状态的可信度，选取可信度最高的状态作为机器人当前时刻的实际状态。贝叶斯滤波就是为了计算状态可信度而存在的。贝叶斯滤波也是整本书的理论基础。关于机器人状态，控制，观测的最重要的两个概率公式：状态转移概率和测量概率都是为贝叶斯滤波服务的。本书也是从贝叶斯滤波展开的，包括实现贝叶斯滤波的几种常用算法，如卡尔曼滤波和粒子滤波等。还有关于如何对状态转移概率中的控制动作(motion)建模和对测量(measurement)建模。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[《Probabilistic Robotics》 读书笔记 01]]></title>
      <url>https://zhehangt.github.io/2016/09/28/ProbailisticRobotics/ProbabilisticRobotics-01/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>接触机器人这个领域已经一个月了，在未确定研究课题之前，希望能通过阅读经典书籍，夯实基础。<br>通过Probabilistic Robotics 读书笔记这一系列博客，希望记录自己在阅读《Probabilistic Robotics》时的心得体会。这一系列博客的重点在于思路的总结和理解，对于公式的推导过程会尽量省略。<br>作为还未入门的新手，加之是英语书籍，学习过程中必然有理解的偏差，望指正。</p>
<h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><p>第一章节论述了概率机器人学的基本哲学理念。<br>机器人通过传感器感知外部环境，然后通过驱动器执行行为，影响外部环境，在这过程中总是充满了各种不确定性。主要体现在五个方面。<br>1.环境。物理世界的本质就是动态和不可预测的。<br>2.传感器。传感器本身受限于物理规律，其次传感器本身存在噪声。<br>3.机器人。机器人驱动器执行行为时存在噪声。<br>4.模型。模型是对真实世界的抽象，其中必然存在误差。<br>5.计算。由于计算资源的限制，常常通过近似的方法来达到低响应时间的目的。</p>
<p>基于如此多的不确定性，因此要在机器人学中应用概率论的方法。即为概率机器人学。概率机器人学所依赖的是对所有的空间用概率分布的形式来表示，而不是某个“最好猜测“。从而可以将以上五个方面的不确定都容纳到数学模型中。通过概率信息来进行决策，可以为机器人领域中面临的难题提供新的解决方法。</p>
<blockquote>
<p>书中通过机器人定位的实例，描述如何利用概率的方法来完成机器人的定位。</p>
</blockquote>
<p>总的来说，在机器人学中，基于概率的方法具有更高的鲁棒性，可以处理更复杂与未知的环境。而实际上，概率机器人学也是机器人评估问题或者地图构建问题中已知的唯一解决方法。其次概率机器人学与传统的计划算法(classical planning algorithms)相比，所依赖模型的精确程度更低。最后概率机器人学适用于任何关于感知和行动的问题。<br>然而概率机器人学也有弊端，第一在于计算的效率问题，第二在于需要近似，这也是概率论常常面临的局限。因此提高计算效率和近似的准确度是概率机器人学研究的重点所在。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>刚开始接触概率机器人学，对书中的很多观点理解得并不是很深刻。本科的某个老师说过，书本的第一章往往是最难的，我觉得还是很有道理。希望通过接下来的学习，进一步加深概率机器人学的哲学观。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 读书笔记 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Probabilistic Robotics </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器人系统中的常见概念]]></title>
      <url>https://zhehangt.github.io/2016/09/25/RoboticsConcept/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>研一学了一年的机器学习，结果研二半路出家，要搞机器人。真是没有一点点防备。WTF。<br>过去一个月，读了一些关于机器人系统的综述，接触到了多机器人，群机器人，云机器人这些概念，这篇文章总结了一些个人对这些概念的简单理解，并对相关论文进行简单介绍。</p>
<h1 id="多机器人"><a href="#多机器人" class="headerlink" title="多机器人"></a>多机器人</h1><p>在论文中经常出现multi-robot这个词，对应的系统称为multi-robot system，简称MRS。一般翻译为多机器人。多机器人的概念是与单机器人相对应的。有些时空相关的任务是单个机器人无法完成的，比如要在同一时间在两个不同的地方完成数据采集。另一方面多个机器人同时协作完成某个工作时能够比单个机器人效率更高。比如利用多个机器人对某个空间进行地图构建。<strong>因此多机器人系统通常的定义为：多个机器人协同工作，完成单个机器人无法完成的任务，或改善工作过程，并获得更优的系统性能。</strong><br>多机器人系统往往是针对任务进行构建的。常见的任务包括聚集(Aggregation)，觅食(Foraging)，编队(Formation)，探索与覆盖等。根据特定的任务，多机器人系统强调对机器人的全局调度、协调与分配，因此往往离不开一个控制系统，这个控制系统可以是集中式的，也可以是分布式的。在控制系统的协调之下，多机器人系统在完成任务时会呈现出明显的协作能力。</p>
<p>论文“<em>协作多机器人系统研究进展综述</em>”是2011年的一篇中文综述。这篇文章从体系结构，环境感知和优化控制三个方面对多机器人系统进行阐述，包括常用的解决方法以及所面临的挑战。</p>
<p>论文“<em>A Survey and Analysis of Multi-Robot Coordination</em>”是2013年的综述。这篇文章从单机器人与多机器人的对比，多机器人协作与竞争的对比，资源冲突，静态调度和动态调度，显式通信和隐式通信的对比，任务配分和行为规划，集中式决策和分布式决策这几个方面展开了讨论，并且引用了很多相关工作。引用的相关工作很多都是顶级会议的文章，因此引用质量比较高。</p>
<h1 id="群机器人"><a href="#群机器人" class="headerlink" title="群机器人"></a>群机器人</h1><p>在论文中经常出现的另一种机器人系统称为swarm robot，一般翻译为群机器人。群机器人的提出主要是<strong>仿生学</strong>在多机器人系统上的应用。在观察蜂群蚁群等生物在进行群体运作机制时发现，这些生物群体在进行群体活动时不依赖控制系统，而是通过局部交互和自组织作用，使整个系统呈现协作，有序的状态。这种群体活动方式在运作过程中并不关心全局的信息。群机器人系统就是利用这种生物运作机制构建的。</p>
<p>以下将从任务，系统的组成以及完成协作的方式三个方面对群机器人系统和多机器人系统进行对比。<br>从任务上来说，群机器人系统与多机器人系统有很多相似的地方，但总的来说群机器人适合于大空间，无时间要求的重复性任务，而多机器人系统适合于协作性较强的，相对复杂的任务。这是由系统组成和协作方式决定的。<br>从系统组成上来看，群机器人系统往往由相同的机器人组成而且数目众多，每个机器人的智能水平比较低，因此功能都相对简单。多机器人系统的组成不限定机器人必须是相同的。多机器人系统可以由不同类型的机器人组成，而且往往规模比较小。每个机器人的智能水平比较高，功能相对复杂。<br>从协作方式上来看，群机器人系统通过本地交互，得到全局的协作，在机器人进行交互和协作时往往不知道全局的信息和任务进度情况。而多机器人系统恰恰相反。多机器人系统的控制系统往往可以得到全局的信息和任务进度情况，并根据这些信息对所有机器人进行调度，协调与分配，从而使机器人系统达到最终的协作能力。<br>因此无论是群机器人的本地交互，还是多机器人的全局控制，如何设计算法实现协作机制是两者关注的重点。</p>
<p>论文”<em>Towards practical application of swarm robotics: overview of swarm tasks</em>“是2014年的综述，对群机器人的任务进行了简单的阐述。论文篇幅比较短。</p>
<p>论文“<em>An Overview of Swarm Robotics: Swarm Intelligence Applied to Multi-robotics</em>”是2015的综述。这篇文章主要说明了多机器人和群机器人之间的区别，并且介绍了一些成功应用的群机器人项目，以及仿真平台。</p>
<p>论文”<em>A review of swarm robotics tasks</em>“是2016年的综述。这篇文章将群机器人的任务分为了9大类，并对这9大类任务从Methods，Algorithms和Analysis三个方面对每一类任务进行了详细的阐述，很有指导意义。论文篇幅比较长。</p>
<p>论文“<em>Programmable self-assembly in a thousand-robot swarm</em>”是2014年Nature杂志上的一篇文章。在这篇文章中，大规模机器人群体根据指定的形状，只通过局部的交互，完成了指定形状的编队。这是群机器人的典型应用。</p>
<h1 id="云机器人"><a href="#云机器人" class="headerlink" title="云机器人"></a>云机器人</h1><p>Cloud Robotics是2010年才被首次提出的概念，一经提出就引起广泛的讨论。与多机器人系统和群机器人系统关注怎么设计机器人之间的协作方式来完成指定任务不同，云机器人强调的是机器人本身能力的扩展，而且这种能力的扩展是在通过云端提供服务的帮助下完成。从定义上来讲，<strong>云机器人将计算和存储卸载到云端</strong>，利用云端来进行计算和存储，从而极大的增强了计算和存储能力。从这个定义上来讲，云机器人系统是单机器人的。但当云端对多个云机器人上传的数据进行分析与共享，并为同一个任务目标为多个机器人提供任务规划和服务时，此时云机器人系统是多机器人的，从完成任务的角度上来说，此时的云机器人系统甚至就是一个多机器人系统。总的来说，云机器人系统是工具性质的，平台性质的，其目的就是扩展机器人的能力，而不增加机器人本身的复杂程度。利用这样的工具和平台，增强机器人的能力之后，即可以构建单机器人任务，也可以构建多机器人任务。</p>
<p>云机器人系统的研究包括机器人如何进行计算卸载，云端如何进行资源调度和分配，安全问题，通信问题等等。这些研究的很多方面与多机器人系统是类似的。但与传统的多机器人系统相比，在完成相同任务时云机器人系统有几个明显的优势。第一，云计算为机器人提供了强大的计算能力。第二，存储在云端的大数据，解决了机器人数据存储能力有限的缺点。第三，机器人的协作可以通过云端的数据分析与共享来完成。而在面对不同任务时，云机器人系统具有扩展性高，可利用的资源更丰富等优点。但将云机器人扩展到多机器人的难点在于如何分析与共享机器人上传的数据，使这些机器人依靠云端形成协作的能力。这与多机器人系统关注的协作方式是有相同点的。</p>
<p>云机器人系统另外一个很重要的关注点在于如何寻找机器人完成某些任务时，关于计算和存储的共性需求，寻找到这些共性需求之后，云端就可以根据这些需求设计服务。因此云机器人率先在SLAM，抓取任务方面取得了一些实质性的研究进展。因为在SLAM和抓取任务中，数据量和计算量都比较大，根据这些需求设计云端服务相对简单。</p>
<p>论文“<em>Cloud Robotics: Architecture, Challenges and Applications</em>”是2012年的综述，是云机器人方面比较早并且引用比较多的综述。文章提出了M2M层和M2C层的系统结构，并且提出了Peer-Based Model，Proxy-Based Model ，Clone-Based Model三种计算模型。</p>
<p>论文”<em>云机器人的研究进展</em>“和”<em>云机器人: 概念, 架构与关键技术研究综述</em>“是2014年的中文综述，与2012年的英文综述类似。</p>
<p>论文”<em>Cloud Robotics: Current Status and Open Issues</em>“是2016年的综述。这篇文章从云集算，大数据，开源资源，机器人协作学习和网络连接等角度展开了关于云机器人的讨论，相对来说与上述的三篇综述有角度上的区别。</p>
<p>论文”<em>Study and evaluation of intensive distributed computing platforms on external systems for embedded systems</em>“是2015年的一篇博士论文，以构造一个立体视觉(Stereo Vision)的导航助手(Navigation Assistance)为例，分析了云机器人的优势所在，并尝试建立云机器人系统通用的设计模式和理论模型。在关于云机器人在SLAM，物体监测和追踪等应用方面的总结很详细，并且引用的论文都比较新。值得深入研究。</p>
<p>关于上述论文中出现率极高的几个云机器人项目，包括DAvinCi，RoboEarth,UNR-PF,Rapyuta等，都是深入云机器人领域研究极佳的材料。<br>“<em>DAvinCi: A cloud computing framework for service robots</em>” 2010<br>“<em>RoboEarth: A World Wide Web for Robots</em>” 2011<br>“<em>Unr-pf: An open-source platform for cloud networked robotic services</em>” 2012<br>“<em>Rapyuta: The roboearth cloud engine</em>” 2013</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>本文从多机器人，群机器人和云机器人这三个概念出发，对这几个机器人系统展开了简单的讨论，未深入到具体的技术细节。这些讨论都是基于个人的理解，不足之处望指正。<br>关于上述提到的几个云机器人项目，学习之后再进行深入探讨和总结。<br>以上。</p>
]]></content>
      
        <categories>
            
            <category> 机器人事业 </category>
            
            <category> 杂谈 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Github+Hexo博客搭建思路]]></title>
      <url>https://zhehangt.github.io/2016/09/22/BuildBlog/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>这篇文章主要记录了利用Github+Hexo进行博客搭建的思路，但是略过了很多具体的细节。因此其实这篇文章提供的大多都是为什么，而不是怎么做。以及一些踩过的坑。整个搭建过程基于Ubuntu16.04，搭建时间2016年9月。</p>
<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>1.利用Hexo在本地构建博客<br>2.将构建完成的博客部署到Github上<br>3.利用域名解析将自己的域名解析到Github上<br>4.解决百度爬虫无法爬取Github<br>5.本地博客的备份问题</p>
<h1 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h1><p>Hexo是一个简单轻便的博客系统。Hexo可以将Markdown文件自动转换成静态网页，正是基于这个特性，可以利用Github pages的静态页面托管服务，将Hexo博客系统托管到Github上。<br>对比使用博客园，CSDN这种类型的博客，通过Github+Hexo搭建的博客具有高度的可定制性。Hexo提供了成百上千的主题，也可以学习自己制作主题。而且数据在本地有一份镜像，只要做好备份就可以保证，一切都在掌控之中。<br>Hexo的使用，官方网站介绍的非常详细。其中最常用的几个命令如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hexo new “博客名” <span class="comment">#创建新博客，在source/_posts目录下会产生一个md文件，博客就在这里面写</span></div><div class="line">hexo server      <span class="comment">#运行该命令后，可以在localhost:4000访问本地博客</span></div><div class="line">hexo clean       <span class="comment">#清理本地空间</span></div><div class="line">hexo generation  <span class="comment">#生成静态文件</span></div><div class="line">hexo deploy      <span class="comment">#根据_config.yml文件中的deploy选项，进行自动部署</span></div></pre></td></tr></table></figure></p>
<p>了解了基本使用之后，可以从github上下载喜欢的hexo主题进行配置和修改，甚至可以自己编写主题。So,做一个有个性的人，尽情的折腾吧!</p>
<blockquote>
<p>ps：我觉得好看的几个主题都被用得烂大街了，不开心。比如NexT。</p>
</blockquote>
<h1 id="Github-pages"><a href="#Github-pages" class="headerlink" title="Github pages"></a>Github pages</h1><p>Github pages可以免费托管静态页面，因此可以将Hexo博客部署到Github pages上。部署成功后可以通过username.github.io访问Hexo博客。全程傻瓜式操作。需要注意的是要使用Github pages服务，创建的项目名必须与用户名一致。并且静态页面要托管在Master分支下。</p>
<p>原则上，只需要将Hexo产生的静态页面文件夹下的文件全部上传到Master分支，就可以完成部署。但为了方便起见，可以利用Hexo的deploy命令完成一键部署。</p>
<p>主要步骤如下：<br>1.配置可以利用SSH自动登录Github。虽然不是必须的，但是可以免除每次部署都要输入用户名和密码的麻烦。<br>2.安装hexo-deployer-git插件。命令如下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm install hexo-deployer-git --save</div></pre></td></tr></table></figure></p>
<p>3.编辑站点的配置文件_config.yml </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  type: git</div><div class="line">  repo: &lt;repository url&gt;</div><div class="line">  branch: [branch]</div></pre></td></tr></table></figure>
<h1 id="域名"><a href="#域名" class="headerlink" title="域名"></a>域名</h1><p>如果觉得username.github.io这样的域名不够高大上，可以通过购买自己喜欢的域名，然后与Github pages进行绑定。<br>国内的域名购买主要有百度开放云，万网，新网等。完成域名购买之后进入相应网站的管理控制台，进行域名解析。<br>域名解析方式有两种，A解析将域名直接指向指定的IP。CNAME解析将域名解析到另一个指定的域名。部署在github上的博客的IP可以通过Ping命令得到。即ping username.github.io</p>
<blockquote>
<p>ps.买5年win的域名，花了25块。现在觉得这个域名好傻逼。</p>
</blockquote>
<h1 id="Coding-net"><a href="#Coding-net" class="headerlink" title="Coding.net"></a>Coding.net</h1><p>将域名绑定到Github pages上之后，发现百度爬虫无法爬取博客内容。这会导致无法通过百度搜索到刚刚辛辛苦苦搭建的博客。原因是Github禁止了百度爬虫的爬取。为了解决这个问题，选择的方案是将Hexo博客同时部署到Github和Coding.net上。国内线路访问时，将域名解析到Coding.net上，而国外线路访问时，将域名解析到Github上。Hexo的站点配置文件_cofig.yml要类似这么写。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">- type: git</div><div class="line">  repo: &lt;repository url&gt;  <span class="comment">#github的ssh地址</span></div><div class="line">  branch: [branch]</div><div class="line">- type: git</div><div class="line">  repo: &lt;repository url&gt;  <span class="comment">#coding.net的ssh地址</span></div><div class="line">  branch: [branch]</div></pre></td></tr></table></figure></p>
<p>经过几次实验，这个方案好像是可行的。</p>
<h1 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h1><p>常在河边走，哪有不湿鞋。Hexo博客的编写都是基于本地的。因此备份是为了防止某一天系统莫名其妙崩了或者硬盘莫名其妙挂掉了。其次如果需要在多台机器上进行博客的撰写，备份也可以作为本地文件的同步方案。理论上只要将这个Hexo博客目录进行备份，就可以在必要时进行恢复或同步。选择的方式是直接将Hexo目录备份到Github项目的另一个分支下。需要注意的是备份要通过.gitignore文件忽略.deploy_git文件夹和public文件夹，因为这两个文件夹存放的是生成的静态网页，这部分内容不需要备份。</p>
<h1 id="几个坑"><a href="#几个坑" class="headerlink" title="几个坑"></a>几个坑</h1><ul>
<li>Hexo是基于node.js的。百度到了很多关于Ubuntu下node.js的安装，有基于deb的，有自己编译的，但不知道为什么全都不管用。所以最好还是根据Hexo官网提供的方法进行安装，三条命令搞定。</li>
<li>域名解析时，将域名通过CNAME解析到username.github.io时没有成功。原因未明。通过A解析到IP时成功了。可能是DNS解析服务器的问题。</li>
<li>基于上面的问题，在网上看了一些大神分析，购买域名的网站提供的DNS解析服务比较辣鸡，建议转换到dnspod这种第三方的DNS解析服务商。只需要在购买域名的网站控制台里更改DNS服务器地址为dnspod的地址，即可更换DNS解析服务商。更换之后域名的解析需要登录dnspod完成。</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 折腾日记 </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[第一篇博客]]></title>
      <url>https://zhehangt.github.io/2016/09/18/FirstBlog/</url>
      <content type="html"><![CDATA[<a id="more"></a>
<p>很久之前就想搭建一个独立博客，拖了很久总算在今天完成了这个小小的愿望。依赖的工具主要是GitHub Pages+Hexo。</p>
<p>越长大越会对这个世界充满好奇，因为在这个世界上有趣的事情实在是太多太多。自从本科阴差阳错地掉进了计算机的坑至今，虽然一直有成为大神的美好愿望，但无奈骨骼不够惊奇，也没遇上一言不和就给我输真气灌内力的老师傅，所以很遗憾依旧是一个技术大白。计算机的世界真是有趣的有点太可怕了。</p>
<p>搭建博客这件事情拖了很久，最大的原因是自己还没有能力产出，可以想像我写出来的东西我自己读起来都会觉得可怕。但满世界的鸡汤马汤告诉我，不论死得有多惨，万事总要开头。</p>
<p>所以，这是我的第一篇不知道在说什么的博客。</p>
]]></content>
      
        <categories>
            
            <category> 不知道在说什么 </category>
            
        </categories>
        
        
    </entry>
    
  
  
</search>
